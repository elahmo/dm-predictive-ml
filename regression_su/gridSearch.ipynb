{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "from subprocess import check_output\n",
    "input_folder = \"../../moviedata\"\n",
    "# print(check_output([\"ls\", input_folder]).decode(\"utf8\"))\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "from sklearn import tree\n",
    "# import matplotlib\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge as br\n",
    "from sklearn.linear_model import ElasticNet as en\n",
    "from sklearn.linear_model import Lars as ls\n",
    "from sklearn.linear_model import Lasso as lo\n",
    "from sklearn.linear_model import LassoLars as ll\n",
    "from sklearn.linear_model import HuberRegressor as hr\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, RFE\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "from itertools import  product\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "import math\n",
    "# import statsmodels.api as sm\n",
    "# from patsy import dmatrices\n",
    "from sklearn import metrics\n",
    "\n",
    "# import seaborn as sns # More snazzy plotting library\n",
    "import itertools\n",
    "from itertools import  product\n",
    "import pprint\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "# f = pd.read_csv(input_folder+\"/movie_metadata.csv\")\n",
    "f = pd.read_csv(input_folder+\"/movie_metadata_cleaned_categ_num_only.csv\")\n",
    "dta_clean = f.dropna()\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "from subprocess import check_output\n",
    "input_folder = \"../../moviedata\"\n",
    "# print(check_output([\"ls\", input_folder]).decode(\"utf8\"))\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "from sklearn import tree\n",
    "# import matplotlib\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge as br\n",
    "from sklearn.linear_model import ElasticNet as en\n",
    "from sklearn.linear_model import Lars as ls\n",
    "from sklearn.linear_model import Lasso as lo\n",
    "from sklearn.linear_model import LassoLars as ll\n",
    "from sklearn.linear_model import HuberRegressor as hr\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, RFE\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "from itertools import  product\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "import math\n",
    "# import statsmodels.api as sm\n",
    "# from patsy import dmatrices\n",
    "from sklearn import metrics\n",
    "\n",
    "# import seaborn as sns # More snazzy plotting library\n",
    "import itertools\n",
    "from itertools import  product\n",
    "import pprint\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from multiprocessing import Process, Value, Array\n",
    "from asyncio import Queue\n",
    "from threading import Thread\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define new transformers\n",
    "def dummy(X):\n",
    "    return X\n",
    "\n",
    "def poly(X, pw):\n",
    "    res = X\n",
    "    for power in range(2,pw + 1):\n",
    "        res = np.concatenate((res, np.power(X, power)), axis=1)\n",
    "    return res\n",
    "\n",
    "def log(X):\n",
    "    df_t = pd.DataFrame(X)\n",
    "    X_t = df_t.replace(0, 1/math.e)\n",
    "    return np.concatenate((X, np.log(X_t)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(para = None):\n",
    "    # print (para);\n",
    "    if para == 2:\n",
    "\n",
    "        #########################\n",
    "        ####Data Preprocessor ###\n",
    "        #########################\n",
    "\n",
    "        preprocessors = [DummyTransformer]\n",
    "        preprocessors_cfg = {}\n",
    "        preprocessors_cfg[DummyTransformer.func.__name__] = {}\n",
    "\n",
    "\n",
    "        #########################\n",
    "        ####  Data Transformer ##\n",
    "        #########################\n",
    "        transfomers = [DummyTransformer]\n",
    "        transfomers_cfg = {}\n",
    "        transfomers_cfg[DummyTransformer.func.__name__] = {}\n",
    "\n",
    "        ###########################\n",
    "        ####Dim Reducer, Feat Sel.#\n",
    "        ###########################\n",
    "        reducers = [DummyTransformer]\n",
    "        reducers_cfg = {}\n",
    "        reducers_cfg[DummyTransformer.func.__name__] = {}\n",
    "\n",
    "        models_cfg[br.__name__] = dict(\n",
    "            # model__n_iter = [10, 50, 100, 150, 200, 250, 300, 350],\n",
    "            model__compute_score = [True, False],\n",
    "            # model__fit_intercept = [True, False],\n",
    "            model__normalize = [True, False],\n",
    "            # model__copy_X = [True, False],\n",
    "            # model__verbose = [True, False]\n",
    "            )\n",
    "\n",
    "        models_cfg[en.__name__] = dict(\n",
    "            # model__max_iter = [10, 50, 100, 150, 200, 250, 300, 350],\n",
    "            model__precompute = [True, False, 'auto'],\n",
    "            # model__fit_intercept = [True, False],\n",
    "            # model__normalize = [True, False],\n",
    "            # model__copy_X = [True, False],\n",
    "            # model__warm_start = [True, False],\n",
    "            model__positive = [True, False],\n",
    "            model_selection = ['cyclic', 'random']\n",
    "            )\n",
    "\n",
    "        # models_cfg[ls.__name__] = dict(\n",
    "        #     model__precompute = [True, False, 'auto'],\n",
    "        #     # model__fit_intercept = [True, False],\n",
    "        #     # model__normalize = [True, False],\n",
    "        #     # model__copy_X = [True, False],\n",
    "        #     model__fit_path = [True, False],\n",
    "        #     model__positive = [True, False],\n",
    "        #     # model_verbose = [True, False]\n",
    "        #     )\n",
    "\n",
    "        # models_cfg[lo.__name__] = dict(\n",
    "        #     # model__max_iter = [10, 50, 100, 150, 200, 250, 300, 350],\n",
    "        #     model__precompute = [True, False, 'auto'],\n",
    "        #     # model__fit_intercept = [True, False],\n",
    "        #     # model__normalize = [True, False],\n",
    "        #     # model__copy_X = [True, False],\n",
    "        #     model__warm_start = [True, False],\n",
    "        #     model__positive = [True, False],\n",
    "        #     model_selection = ['cyclic', 'random']\n",
    "        #     )\n",
    "\n",
    "        models_cfg[ll.__name__] = dict(\n",
    "            # model__max_iter = [10, 50, 100, 150, 200, 250, 300, 350],\n",
    "            model__precompute = [True, False, 'auto'],\n",
    "            # model__fit_intercept = [True, False],\n",
    "            # model__normalize = [True, False],\n",
    "            # model__copy_X = [True, False],\n",
    "            model__fit_path = [True, False],\n",
    "            model__positive = [True, False],\n",
    "            # model_verbose = [True, False]\n",
    "            )\n",
    "        # models_cfg[hr.__name__] = dict(\n",
    "        #   )\n",
    "    else:\n",
    "        # models_cfg[br.__name__] = dict(\n",
    "        #     )\n",
    "\n",
    "        # models_cfg[en.__name__] = dict(\n",
    "        #     )\n",
    "\n",
    "        # models_cfg[ls.__name__] = dict(\n",
    "        #     )\n",
    "\n",
    "        # models_cfg[lo.__name__] = dict(\n",
    "        #     )\n",
    "\n",
    "        models_cfg[ll.__name__] = dict(\n",
    "            )\n",
    "        # models_cfg[hr.__name__] = dict(\n",
    "        #   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define helpers\n",
    "def get_powers_list(n_samples, n_features, n):\n",
    "    base_arr = [{\"pw\":2},{\"pw\":3},{\"pw\":4}]\n",
    "    max_pw = math.ceil(3320/n_features)\n",
    "    if max_pw > 7: max_pw = 7\n",
    "    step = math.floor((max_pw-4) / n)\n",
    "    if step < 1 : step = 1\n",
    "    extra_arr = [{\"pw\":power} for power in range(4 + step, max_pw, step)]\n",
    "    if  n_samples/n_features < 2:\n",
    "        res = [{\"pw\":1}]\n",
    "    elif max_pw - 1 == 2:\n",
    "        res = [{\"pw\":2}]\n",
    "    elif max_pw - 1 == 3:\n",
    "        res = [{\"pw\":2}, {\"pw\":3}]\n",
    "    elif max_pw - 1 == 4:\n",
    "        res = [{\"pw\":2},{\"pw\":3},{\"pw\":4}]\n",
    "    else :\n",
    "        res = base_arr + extra_arr\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_components_list(n_features, lst):\n",
    "    lst = lst + [{\"pw\": 0.1},{\"pw\": 0.4},{\"pw\": 0.5},{\"pw\": 0.8}]\n",
    "    lst = sorted(list(map(lambda x: math.floor(x[\"pw\"]*n_features), lst)) + [1, 3, 5], reverse=True)\n",
    "    lst[0] = lst[0]-1\n",
    "    lst_n = [n for n in lst if n < 3321]\n",
    "    if len(lst_n) < len(lst):\n",
    "        lst_n = [3320] + lst_n\n",
    "    return lst_n\n",
    "\n",
    "def run_grid_search(x,y, model, cfg_dict, pipeline_cfg, results, errors, errors_ind):\n",
    "    global itter_current\n",
    "    itter_current += 1\n",
    "    #check if itteration start is set to something different than 0 and then check if current itteration has been reached\n",
    "    if itter_start != 0 and itter_current < itter_start: return\n",
    "    #create pipline and use GridSearch to find the best params for given pipeline\n",
    "    name = type(model).__name__\n",
    "\n",
    "    #Define and save pipe cfg\n",
    "    pipe = Pipeline(steps=[('model', model)])\n",
    "\n",
    "    #create a dict with param grid\n",
    "    param_grid = models_cfg[name]\n",
    "    #create estimator\n",
    "    cv = 4\n",
    "    print('####################################################################################')\n",
    "    print('################# Runing the itteration %d  of the GridSearchCV ####################' %(itter_current))\n",
    "    print('####################################################################################')\n",
    "    print (\"***Starting [\"  + name + \"] estimator run, pipeline: \"+ pipeline_cfg+\" \")\n",
    "    print(\"##param_grid##\")\n",
    "    print(param_grid)\n",
    "    estimator = GridSearchCV(pipe,param_grid,verbose=2, cv=cv, n_jobs=-1)\n",
    "    #run the esmimator, except eceptions, sape errors\n",
    "    try:\n",
    "            estimator.fit(x, y)\n",
    "            print (\"GREP_ME***Results of [\"  + name + \"] estimatorrun are\")\n",
    "            print (estimator.cv_results_)            \n",
    "            print (\"GREP_ME***Best params of [\"  + name + \"] estimator,pipeline:\"+ pipeline_cfg+\"  run are\")\n",
    "            best_param = dict(estimator.best_params_, **cfg_dict)\n",
    "            print (best_param)\n",
    "            print (\"GREP_ME***Best score of [\"  + name + \"] estimator, pipeline:\"+ pipeline_cfg+\" run are\")\n",
    "            print (estimator.best_score_)\n",
    "            if (name not in results) or (estimator.best_score_ > results[name][\"score\"]):\n",
    "                results[name] = {\"score\": estimator.best_score_, \"pipe\":pipeline_cfg, \"best_cfg\": best_param}\n",
    "    except (ValueError, MemoryError) as err:\n",
    "            print (\"GREP_ME***Error caught for  [\"  + name + \"] , pipeline: [\"+ pipeline_cfg+\"] \")\n",
    "            print(err)\n",
    "            pass\n",
    "\n",
    "def launch_pipe_instance(x,y, pipe, cfg_dict, pipeline_cfg, precomp_pipe, errors, errors_ind, ind):\n",
    "    print (\"Starting precomp pipline for \"+ str(cfg_dict))\n",
    "    #run the pipe, except eceptions, save errors\n",
    "    try:\n",
    "            #precomp_pipe.put_nowait({\"pipeline_cfg\": pipeline_cfg, \"cfg_dict\": cfg_dict,\"precomp_transform\": pipe.set_params(**cfg_dict).fit_transform(x,y)})\n",
    "            dump_dict = {\"pipeline_cfg\": pipeline_cfg, \"cfg_dict\": cfg_dict,\"precomp_transform\": pipe.set_params(**cfg_dict).fit_transform(x,y)}\n",
    "            tmp_trg = \"./tmp/\" + str(itter_current) + \"_\" + str(ind)\n",
    "            with open(tmp_trg, 'wb') as handle:\n",
    "                  pickle.dump(dump_dict, handle)\n",
    "            print (\"Finished precomp pipline for \"+ str(cfg_dict))\n",
    "            \n",
    "\n",
    "    except (ValueError, MemoryError) as err:\n",
    "            print (\"GREP_ME***Error caught for  precomp pipeline: [\"+ pipeline_cfg+\"] \")\n",
    "            errors_ind.append({\"cfg\": pipeline_cfg})\n",
    "            errors.append({\"Precomp pipe: \" + pipeline_cfg: {\"error\": err}})\n",
    "            pass\n",
    "\n",
    "def get_pipe_result(x, y, preprocessor, transfomer, reducer, precomp_pipe, errors, errors_ind):\n",
    "    global itter_current\n",
    "    itter_current += 1\n",
    "    #create pipline for the preprocessing\n",
    "    preprocessor_name = type(preprocessor).__name__ if (type(preprocessor).__name__ != \"FunctionTransformer\") else preprocessor.func.__name__\n",
    "    transfomer_name =  type(transfomer).__name__ if (type( transfomer).__name__ != \"FunctionTransformer\") else  transfomer.func.__name__\n",
    "    reducer_name = type(reducer).__name__ if (type(reducer).__name__ != \"FunctionTransformer\") else reducer.func.__name__\n",
    "    \n",
    "    print('####################################################################################')\n",
    "    print('################# Runing the itteration %d  of pipeline precomp      ###############' %(itter_current))\n",
    "    print('####################################################################################')\n",
    "    \n",
    "    #Define and save pipe cfg\n",
    "    pipeline_cfg = \"| preprocessor:\" + preprocessor_name +  \" | transfomer: \" + transfomer_name + \" | reducer: \" + reducer_name\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('transfomer', transfomer), ('reducer', reducer)])\n",
    "    print(pipeline_cfg)\n",
    "    \n",
    "    #itterate over each cfg variation and precompute the result\n",
    "    param_grid = dict(dict(preprocessors_cfg[preprocessor_name], **dict(transfomers_cfg[transfomer_name], **reducers_cfg[reducer_name])))\n",
    "    print(param_grid)\n",
    "    processes = []\n",
    "    local_ind = 0\n",
    "    for _terms  in list(product(*[preprocessors_cfg[preprocessor_name][it] for it in preprocessors_cfg[preprocessor_name]])):\n",
    "        cfg_dict = dict((term, _terms[ind]) for ind, term in enumerate(tuple(it for it in preprocessors_cfg[preprocessor_name])))\n",
    "    \n",
    "        for _terms  in list(product(*[transfomers_cfg[transfomer_name][it] for it in transfomers_cfg[transfomer_name]])):\n",
    "            cfg_dict.update(dict((term, _terms[ind]) for ind, term in enumerate(tuple(it for it in transfomers_cfg[transfomer_name]))))\n",
    "                \n",
    "            for _terms  in list(product(*[reducers_cfg[reducer_name][it] for it in reducers_cfg[reducer_name]])):\n",
    "                cfg_dict.update(dict((term, _terms[ind]) for ind, term in enumerate(tuple(it for it in reducers_cfg[reducer_name]))))\n",
    "                #launch in a parraler manner a pipe dict\n",
    "                #launch_pipe_instance(x,y, pipe, cfg_dict, pipeline_cfg, precomp_pipe, errors, errors_ind, local_ind)\n",
    "                local_ind += 1\n",
    "                final_cfg = cfg_dict\n",
    "                p = Process(target=launch_pipe_instance, args=(x,y, pipe, cfg_dict, pipeline_cfg, precomp_pipe, errors, errors_ind, local_ind))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "\n",
    "    for p in processes: p.join()\n",
    "\n",
    "# def run_solver(x,y,preprocessors, transfomers, reducers, models, results, errors, errors_ind):\n",
    "def run_solver(x,y,preprocessors, transfomers, reducers, models, results, errors, errors_ind, precomp_pipe):\n",
    "    # mix it, so that the sample order is randomized\n",
    "    x, _X_dummy, y, _y_dummy = train_test_split(x, y, test_size=0)\n",
    "    n_samples, n_features = x.shape\n",
    "\n",
    "    #make a dir for preprocessor temp files\n",
    "    try:\n",
    "        os.mkdir(\"./tmp\")\n",
    "    except FileExistsError:\n",
    "        #rm temp dir and make new one\n",
    "        shutil.rmtree(\"./tmp\")\n",
    "        os.mkdir(\"./tmp\")\n",
    "\n",
    "    # for preprocessor, transfomer, reducer, model in product(preprocessors, transfomers, reducers, models):\n",
    "    for preprocessor, transfomer, reducer in product(preprocessors, transfomers, reducers):\n",
    "        ##run gridesearch with new amout of features, depending of preprocessor and hence pass the right amount of maximum components to the reducers\n",
    "        if preprocessor.func.__name__ == LogarithmicTransformer.func.__name__ :\n",
    "            n_components = get_components_list(n_features, [{\"pw\":2}, {\"pw\":1}])\n",
    "            # reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n",
    "            # reducers_cfg[GenericUnivariateSelect.__name__][\"reducer__param\"] = n_components\n",
    "            reducers_cfg[RFE.__name__][\"reducer__n_features_to_select\"] = n_components\n",
    "            # run_grid_search(x,y,preprocessor, transfomer, reducer, model, results, errors, errors_ind)\n",
    "            get_pipe_result(x, y,preprocessor, transfomer, reducer, precomp_pipe, errors, errors_ind)\n",
    "        elif preprocessor.func.__name__ == PolynomialTransformer.func.__name__:\n",
    "            kw_arg_powers = get_powers_list(n_samples, n_features, 3)\n",
    "            pw_lst = []\n",
    "            for pw in kw_arg_powers:\n",
    "                pw_lst = pw_lst + [pw]\n",
    "                preprocessors_cfg[PolynomialTransformer.func.__name__][\"preprocessor__kw_args\"] = [pw]\n",
    "                n_components = get_components_list(n_features, pw_lst)\n",
    "                reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n",
    "                reducers_cfg[GenericUnivariateSelect.__name__][\"reducer__param\"] = n_components\n",
    "                reducers_cfg[RFE.__name__][\"reducer__n_features_to_select\"] = n_components\n",
    "                # run_grid_search(x,y,preprocessor, transfomer, reducer, model, results, errors, errors_ind)\n",
    "                get_pipe_result(x, y,preprocessor, transfomer, reducer, precomp_pipe, errors, errors_ind)\n",
    "        else:\n",
    "            n_components = get_components_list(n_features, [{\"pw\":1}])\n",
    "            reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n",
    "            reducers_cfg[GenericUnivariateSelect.__name__][\"reducer__param\"] = n_components\n",
    "            reducers_cfg[RFE.__name__][\"reducer__n_features_to_select\"] = n_components\n",
    "            # run_grid_search(x,y,preprocessor, transfomer, reducer, model, results, errors, errors_ind)\n",
    "            get_pipe_result(x, y,preprocessor, transfomer, reducer, precomp_pipe, errors, errors_ind)\n",
    "\n",
    "    #for each physically saved pickle run grid search for each model\n",
    "    for filename in os.listdir(\"./tmp\"):\n",
    "        pipe_dict = pickle.loads(open(\"./tmp/\" + filename, 'rb').read())\n",
    "        for model in models: \n",
    "            run_grid_search( pipe_dict['precomp_transform'],y, model,  pipe_dict['cfg_dict'],  pipe_dict['pipeline_cfg'], results, errors, errors_ind)\n",
    "\n",
    "# def run_for_many(cl_n,label_fn):\n",
    "def run_for_many(X, y, sam):\n",
    "    results = {}\n",
    "    errors = []\n",
    "    errors_ind = []\n",
    "    precomp_pipe = []\n",
    "\n",
    "    print (\"#########################################\")\n",
    "    # print (\"###Starting all estimators for cl: \"+ str(cl_n))\n",
    "    print (\"###Starting all estimators for cl: \" + sam)\n",
    "    print (\"#########################################\")\n",
    "    # run_solver(X,y, preprocessors, transfomers, reducers, models, results, errors, errors_ind)\n",
    "    run_solver(X,y, preprocessors, transfomers, reducers, models, results, errors, errors_ind, precomp_pipe)\n",
    "    print (\"#########################################\")\n",
    "    # print (\"###Finished all estimators for cl: \"+ str(cl_n))\n",
    "    print (\"###Finished all estimators for cl: \" + sam)\n",
    "    print (\"#########################################\")\n",
    "\n",
    "    print (\"#########################################\")\n",
    "    # print (\"######Printing all errors for cl: \"+ str(cl_n))\n",
    "    print (\"######Printing all errors for cl: \" + sam)\n",
    "    print (\"#########################################\")\n",
    "    print(errors)\n",
    "    print (\"#########################################\")\n",
    "    # print (\"######Printing errors summary for cl: \"+ str(cl_n))\n",
    "    print (\"######Printing errors summary for cl: \" + sam)\n",
    "    print (\"#########################################\")\n",
    "    print(errors_ind)\n",
    "    print (\"#########################################\")\n",
    "    # print (\"#######Printing results for cl: \"+ str(cl_n))\n",
    "    print (\"#######Printing results for cl: \" + sam)\n",
    "    print (\"#########################################\")\n",
    "    print(results)\n",
    "    print(\"priting simply sorted numbers, grep them to find the best cfg or cl: \" + sam)\n",
    "    scores = [results[model][\"score\"] for model in results]\n",
    "    print(sorted(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "f = pd.read_csv(input_folder+\"/movie_metadata_cleaned_categ_num_only.csv\")\n",
    "dta_clean = f.dropna()\n",
    "\n",
    "X_a = dta_clean.drop('worldwide_gross', axis=1)\n",
    "y_a = dta_clean['worldwide_gross']\n",
    "\n",
    "df_1_1 = dta_clean[dta_clean[\"worldwide_gross\"] < 200000000]\n",
    "X_1_1 = df_1_1.drop('worldwide_gross', axis=1)\n",
    "y_1_1 = df_1_1['worldwide_gross']\n",
    "\n",
    "df_1_2 = dta_clean[dta_clean[\"worldwide_gross\"] >= 200000000]\n",
    "X_1_2 = df_1_2.drop('worldwide_gross', axis=1)\n",
    "y_1_2 = df_1_2['worldwide_gross']\n",
    "\n",
    "df_1 = dta_clean[dta_clean[\"worldwide_gross\"] < 10000000]\n",
    "X_1 = df_1.drop('worldwide_gross', axis=1)\n",
    "y_1 = df_1['worldwide_gross']\n",
    "\n",
    "df_2 = dta_clean[dta_clean[\"worldwide_gross\"] >= 10000000]\n",
    "df_2 = df_2[df_2[\"worldwide_gross\"] < 300000000]\n",
    "X_2 = df_2.drop('worldwide_gross', axis=1)\n",
    "y_2 = df_2['worldwide_gross']\n",
    "\n",
    "df_3 = dta_clean[dta_clean[\"worldwide_gross\"] >= 300000000]\n",
    "X_3 = df_3.drop('worldwide_gross', axis=1)\n",
    "y_3 = df_3['worldwide_gross']\n",
    "\n",
    "#shuffle the whole dataset\n",
    "X_a, X_d, y_a, y_d = train_test_split(X_a, y_a, test_size=0, random_state=0)\n",
    "#shuffle the whole dataset\n",
    "X_1, X_d, y_1, y_d = train_test_split(X_1, y_1, test_size=0, random_state=0)\n",
    "#shuffle the whole dataset\n",
    "X_2, X_d, y_2, y_d = train_test_split(X_2, y_2, test_size=0, random_state=0)\n",
    "#shuffle the whole dataset\n",
    "X_3, X_d, y_3, y_d = train_test_split(X_3, y_3, test_size=0, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "###define a global itteration var###########\n",
    "itter_start   = 0\n",
    "itter_current = 0\n",
    "\n",
    "DummyTransformer = FunctionTransformer(dummy)\n",
    "LogarithmicTransformer = FunctionTransformer(log)\n",
    "PolynomialTransformer = FunctionTransformer(poly)\n",
    "\n",
    "###define new config###########\n",
    "\n",
    "#########################\n",
    "####Data Preprocessor ###\n",
    "#########################\n",
    "# 0.984846011963 (LassoLars):[] | log : [] dummy : [] RFE : [\"n_features_to_select': 139\", \"step': 0.1\"]\n",
    "\n",
    "# preprocessors = [DummyTransformer, LogarithmicTransformer, PolynomialTransformer]\n",
    "# preprocessors_cfg = {}\n",
    "# preprocessors_cfg[DummyTransformer.func.__name__] = {}\n",
    "# preprocessors_cfg[LogarithmicTransformer.func.__name__] = {}\n",
    "# preprocessors_cfg[PolynomialTransformer.func.__name__] = dict(\n",
    "#         preprocessor__kw_args = []\n",
    "#         )\n",
    "preprocessors = [LogarithmicTransformer]\n",
    "preprocessors_cfg = {}\n",
    "preprocessors_cfg[LogarithmicTransformer.func.__name__] = {}\n",
    "\n",
    "#########################\n",
    "####  Data Transformer ##\n",
    "#########################\n",
    "# transfomers = [DummyTransformer, Normalizer(), StandardScaler()]\n",
    "# transfomers_cfg = {}\n",
    "# transfomers_cfg[DummyTransformer.func.__name__] = {}\n",
    "# transfomers_cfg[Normalizer.__name__] = dict(\n",
    "#         transfomer__norm = ['l1', 'l2', 'max']\n",
    "#         )\n",
    "# transfomers_cfg[StandardScaler.__name__] = {}\n",
    "transfomers = [DummyTransformer]\n",
    "transfomers_cfg = {}\n",
    "transfomers_cfg[DummyTransformer.func.__name__] = {}\n",
    "\n",
    "###########################\n",
    "####Dim Reducer, Feat Sel.#\n",
    "###########################\n",
    "# reducers = [DummyTransformer, PCA(), GenericUnivariateSelect(), RFE(ExtraTreesRegressor())]\n",
    "# reducers_cfg = {}\n",
    "# reducers_cfg[DummyTransformer.func.__name__] = {}\n",
    "# reducers_cfg[PCA.__name__] = dict(\n",
    "#         reducer__n_components = [],\n",
    "#         # reducer__whiten = [True, False],\n",
    "#         reducer__svd_solver = ['auto']\n",
    "#         )\n",
    "# reducers_cfg[GenericUnivariateSelect.__name__] = dict(\n",
    "#         reducer__score_func = [f_regression],\n",
    "#         reducer__mode = ['k_best'],\n",
    "#         reducer__param = []\n",
    "#         )\n",
    "# reducers_cfg[RFE.__name__] = dict(\n",
    "#         reducer__n_features_to_select = [],\n",
    "#         reducer__step = [0.1]\n",
    "#         )\n",
    "\n",
    "reducers = [RFE(ExtraTreesRegressor())]\n",
    "reducers_cfg = {}\n",
    "reducers_cfg[RFE.__name__] = dict(\n",
    "        reducer__n_features_to_select = [139],\n",
    "        reducer__step = [0.1]\n",
    "        )\n",
    "#########################\n",
    "####### Models ##########\n",
    "#########################\n",
    "\n",
    "# models = [br(), en(), ls(), lo(), ll()]\n",
    "#models = [br(), en(), ll()]\n",
    "models = [ll()]\n",
    "\n",
    "models_cfg = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ignore warnigs\n",
    "\n",
    "\n",
    "tuples_of_data = [(X_a,y_a, \"LassoLars_all_samples\")]\n",
    "\n",
    "# labels = [label_gross_3, label_gross_2, label_gross_4, label_gross_5]\n",
    "#save orig datetime and save orign stdout\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "# for ind, cb in enumerate(labels):\n",
    "for x in range(1,2):\n",
    "\n",
    "    for item in tuples_of_data:\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            time = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            # trg = \"classifyRes_\" + time + \"_\" + cb.__name__ + \".log\"\n",
    "            trg = \"classifyRes_\" + time + \"_\" + item[2] + '_' + str(x) + \".log\"\n",
    "            new_file = open(trg,\"w\")\n",
    "            sys.stdout = new_file\n",
    "            init(x)\n",
    "            # print (trg + str(x))\n",
    "            # break\n",
    "            run_for_many(item[0], item[1], item[2])\n",
    "            #return stdout for some reason\n",
    "sys.stdout = orig_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
