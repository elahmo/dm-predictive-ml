{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import all helpers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, RFE\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "import seaborn as sns # More snazzy plotting library\n",
    "import itertools\n",
    "from itertools import  product\n",
    "\n",
    "#import regressors\n",
    "#-----Ensemble---------------------\n",
    "from sklearn.ensemble import       AdaBoostRegressor\n",
    "from sklearn.ensemble import       BaggingRegressor\n",
    "from sklearn.ensemble import       ExtraTreesRegressor\n",
    "from sklearn.ensemble import       GradientBoostingRegressor\n",
    "from sklearn.ensemble import       RandomForestRegressor\n",
    "\n",
    "#----Generalized Linear models-----\n",
    "from sklearn.linear_model import   ARDRegression\n",
    "from sklearn.linear_model import   BayesianRidge\n",
    "from sklearn.linear_model import   ElasticNet\n",
    "from sklearn.linear_model import   HuberRegressor\n",
    "from sklearn.linear_model import   Lars\n",
    "from sklearn.linear_model import   Lasso\n",
    "from sklearn.linear_model import   LassoLars\n",
    "from sklearn.linear_model import   LinearRegression\n",
    "from sklearn.linear_model import   PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import   Ridge\n",
    "from sklearn.linear_model import   SGDRegressor\n",
    "from sklearn.linear_model import   OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import   RANSACRegressor\n",
    "from sklearn.linear_model import   TheilSenRegressor\n",
    "\n",
    "#---Nearest Neighbors----\n",
    "from sklearn.neighbors import      KNeighborsRegressor\n",
    "from sklearn.neighbors import      RadiusNeighborsRegressor\n",
    "\n",
    "\n",
    "#----Neural Networks--------------- \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#-----Support Vector Machines------\n",
    "from sklearn.svm import            SVR\n",
    "from sklearn.svm import            LinearSVR\n",
    "from sklearn.svm import            NuSVR\n",
    "\n",
    "#-----Decission Trees--------------\n",
    "from sklearn.tree import           DecisionTreeRegressor\n",
    "from sklearn.tree import           ExtraTreeRegressor\n",
    "\n",
    "#----extras\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.isotonic import         IsotonicRegression\n",
    "from sklearn.kernel_ridge import     KernelRidge\n",
    "\n",
    "\n",
    "\n",
    "#file_path =  \"../dataset/movie_metadata_cleaned_tfidf_num_only_min.csv\"\n",
    "file_path =  \"../dataset/movie_metadata_cleaned_categ_num_only.csv\"\n",
    "#file_path = \"../dataset/movie_metadata_cleaned_no_vector_num_only.csv\"\n",
    "\n",
    "dta = pd.read_csv(file_path)\n",
    "dta_clean = dta\n",
    "#remove the null values, that is fill NaN with there - FIXME: Rihards, naive implementation\n",
    "dta_clean = dta_clean.fillna(value=0, axis=1)\n",
    "dta_clean = dta_clean.dropna()\n",
    "dta_clean = dta_clean.drop('Unnamed: 0', axis=1)\n",
    "dta_clean.describe()\n",
    "\n",
    "##Divide test and target data and mix it, so that the sample order is randomized\n",
    "y = dta_clean['worldwide_gross']\n",
    "X = dta_clean.drop('worldwide_gross', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#models = [AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor,BayesianRidge,ElasticNet,HuberRegressor,Lars,Lasso,LassoLars,LinearRegression,PassiveAggressiveRegressor,Ridge,SGDRegressor,OrthogonalMatchingPursuit,RANSACRegressor,TheilSenRegressor,KNeighborsRegressor,RadiusNeighborsRegressor,MLPRegressor,SVR,LinearSVR,NuSVR,DecisionTreeRegressor,ExtraTreeRegressor]\n",
    "##defines results and errors dict\n",
    "results = {}\n",
    "errors = {}\n",
    "errors_ind = {}\n",
    "n_samples, n_features = X.shape\n",
    "##define helpers \n",
    "def get_powers_list(n_samples, n_features, n):\n",
    "    base_arr = [{\"pw\":2},{\"pw\":3},{\"pw\":4}]\n",
    "    max_pw = math.ceil(n_samples/n_features)\n",
    "    step = math.floor((max_pw-4) / n)\n",
    "    extra_arr = [{\"pw\":power} for power in range(4 + step, max_pw, step)]\n",
    "    if  n_samples/n_features < 2:\n",
    "        res = [{\"pw\":1}]\n",
    "    elif max_pw - 1 == 2:\n",
    "        res = [{\"pw\":2}]\n",
    "    elif max_pw - 1 == 3:\n",
    "        res = [{\"pw\":2}, {\"pw\":3}]\n",
    "    elif max_pw - 1 == 4:\n",
    "        res = [{\"pw\":2},{\"pw\":3},{\"pw\":4}]\n",
    "    else :\n",
    "        res = base_arr + extra_arr\n",
    "    return res\n",
    "\n",
    "def get_components_list(n_features, lst):\n",
    "    lst = lst + [{\"pw\": 0.1},{\"pw\": 0.4},{\"pw\": 0.5},{\"pw\": 0.8}]\n",
    "    lst = sorted(list(map(lambda x: math.floor(x[\"pw\"]*n_features), lst)) + [1, 3, 5], reverse=True)\n",
    "    lst[0] = lst[0]-1\n",
    "    return lst\n",
    "\n",
    "##define new transformers\n",
    "def dummy(X):  \n",
    "    return X\n",
    "\n",
    "def poly(X, pw):\n",
    "    res = X\n",
    "    for power in range(2,pw + 1):\n",
    "        res = np.concatenate((res, np.power(X, power)), axis=1)\n",
    "    return res\n",
    "\n",
    "def log(X):\n",
    "    df_t = pd.DataFrame(X)\n",
    "    X_t = df_t.replace(0, 1/math.e) \n",
    "    return np.concatenate((X, np.log(X_t)), axis=1)\n",
    "\n",
    "\n",
    "DummyTransformer = FunctionTransformer(dummy)\n",
    "LogarithmicTransformer = FunctionTransformer(log)\n",
    "PolynomialTransformer = FunctionTransformer(poly)\n",
    "\n",
    "###define new config###########\n",
    "\n",
    "#########################\n",
    "####Data Preprocessor ###\n",
    "#########################\n",
    "#preprocessors = [DummyTransformer, LogarithmicTransformer, PolynomialTransformer]\n",
    "preprocessors = [PolynomialTransformer]\n",
    "preprocessors_cfg = {}\n",
    "preprocessors_cfg[DummyTransformer.func.__name__] = {}\n",
    "preprocessors_cfg[LogarithmicTransformer.func.__name__] = {}\n",
    "preprocessors_cfg[PolynomialTransformer.func.__name__] = dict(\n",
    "        preprocessor__kw_args = get_powers_list(n_samples, n_features, 3)\n",
    "        )\n",
    "#########################\n",
    "####  Data Transformer ##\n",
    "#########################\n",
    "#transfomers = [DummyTransformer, Normalizer(), StandardScaler()]\n",
    "transfomers = [DummyTransformer]\n",
    "transfomers_cfg = {}\n",
    "transfomers_cfg[DummyTransformer.func.__name__] = {}\n",
    "transfomers_cfg[Normalizer.__name__] = dict(\n",
    "        transfomer__norm = ['l1', 'l2', 'max']\n",
    "        )\n",
    "transfomers_cfg[StandardScaler.__name__] = {}\n",
    "###########################\n",
    "####Dim Reducer, Feat Sel.#\n",
    "###########################\n",
    "#reducers = [DummyTransformer, FactorAnalysis(), PCA(), GenericUnivariateSelect(), RFE(ExtraTreesRegressor())]\n",
    "reducers = [FactorAnalysis()]\n",
    "reducers_cfg = {}\n",
    "reducers_cfg[DummyTransformer.func.__name__] = {}\n",
    "reducers_cfg[FactorAnalysis.__name__] = dict(\n",
    "        reducer__n_components = [],\n",
    "        reducer__svd_method = ['randomized']\n",
    "        )\n",
    "reducers_cfg[PCA.__name__] = dict(\n",
    "        reducer__n_components = [],\n",
    "        reducer__whiten = [True, False],\n",
    "        reducer__svd_solver = ['auto']\n",
    "        )\n",
    "reducers_cfg[GenericUnivariateSelect.__name__] = dict(\n",
    "        reducer__score_func = [f_regression],\n",
    "        reducer__mode = ['k_best'],\n",
    "        reducer__param = []\n",
    "        )\n",
    "reducers_cfg[RFE.__name__] = dict(\n",
    "        reducer__n_features_to_select = [],\n",
    "        reducer__step = [0.1]\n",
    "        )\n",
    "#########################\n",
    "####### Models ##########\n",
    "#########################\n",
    "models = [LinearRegression()]\n",
    "models_cfg = {}\n",
    "models_cfg[LinearRegression.__name__] = dict(\n",
    "    #model__normalize = [True,False],\n",
    "    model__fit_intercept = [True]\n",
    ")\n",
    "\n",
    "def run_grid_search(x,y,preprocessor, transfomer, reducer, model, res_dict, error, errors_ind):\n",
    "    \n",
    "    #create pipline and use GridSearch to find the best params for given pipeline\n",
    "    name = type(model).__name__\n",
    "    preprocessor_name = type(preprocessor).__name__ if (type(preprocessor).__name__ != \"FunctionTransformer\") else preprocessor.func.__name__\n",
    "    transfomer_name =  type(transfomer).__name__ if (type( transfomer).__name__ != \"FunctionTransformer\") else  transfomer.func.__name__\n",
    "    reducer_name = type(reducer).__name__ if (type(reducer).__name__ != \"FunctionTransformer\") else reducer.func.__name__\n",
    "    \n",
    "    #Define and save pipe cfg\n",
    "    pipeline_cfg = \"| preprocessor:\" + preprocessor_name +  \" | transfomer: \" + transfomer_name + \" | reducer: \" + reducer_name\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('transfomer', transfomer), ('reducer', reducer),('model', model)])\n",
    "    \n",
    "    #create a dict with param grid\n",
    "    param_grid = dict(models_cfg[name], **dict(reducers_cfg[reducer_name], **dict(transfomers_cfg[transfomer_name], **preprocessors_cfg[preprocessor_name])))\n",
    "    param_grid = {'model__fit_intercept': [True], 'preprocessor__kw_args': [{'pw': 24}], \n",
    "                  'reducer__n_components': [3311, 1932, 552, 414, 276, 110, 69, 55, 13, 5, 3, 1], 'reducer__svd_method': ['randomized']}\n",
    "    #create estimator\n",
    "    cv = 4\n",
    "    print('####################################################################################')\n",
    "    print()\n",
    "    print('####################################################################################')\n",
    "    print (\"***Starting [\"  + name + \"] estimator run, pipeline: \"+ pipeline_cfg+\" \")\n",
    "    print(\"##param_grid##\")\n",
    "    print(param_grid)\n",
    "    estimator = GridSearchCV(pipe,param_grid,verbose=2, cv=cv, n_jobs=4)\n",
    "    #run the esmimator, except eceptions, sape errors\n",
    "    try:\n",
    "            estimator.fit(x, y)\n",
    "            print (\"GREP_ME***Results of [\"  + name + \"] estimatorrun are\")\n",
    "            print (estimator.cv_results_)\n",
    "            print (\"GREP_ME***Best params of [\"  + name + \"] estimator,pipeline:\"+ pipeline_cfg+\"  run are\")\n",
    "            print (estimator.best_params_)\n",
    "            print (\"GREP_ME***Best score of [\"  + name + \"] estimator, pipeline:\"+ pipeline_cfg+\" run are\")\n",
    "            print (estimator.best_score_)\n",
    "            if (name not in results) or (estimator.best_score_ > results[name][\"score\"]):\n",
    "                results[name] = {\"score\": estimator.best_score_, \"pipe\":pipeline_cfg, \"best_cfg\": estimator.best_params_}\n",
    "    #except (ValueError, MemoryError) as err:\n",
    "    except IOError as err:\n",
    "            print (\"GREP_ME***Error caught for  [\"  + name + \"] , pipeline: [\"+ pipeline_cfg+\"] \")\n",
    "            errors_ind.update({\"cfg\": \"Model[\"+ name +\"] pipe: \" + pipeline_cfg})\n",
    "            errors.update({\"Model[\"+ name +\"] pipe: \" + pipeline_cfg: {\"error\": err}})\n",
    "            pass\n",
    "            \n",
    "def run_solver(x,y,preprocessors, transfomers, reducers, models, res_dict, error, errors_ind):\n",
    "    # mix it, so that the sample order is randomized\n",
    "    x, _X_dummy, y, _y_dummy = train_test_split(x, y, test_size=0)\n",
    "    for preprocessor, transfomer, reducer, model in product(preprocessors, transfomers, reducers, models):\n",
    "        ##run gridesearch with new amout of features, depending of preprocessor and hence pass the right amount of maximum components to the reducers\n",
    "        if preprocessor.func.__name__ == LogarithmicTransformer.func.__name__ :\n",
    "            n_components = get_components_list(n_features, [{\"pw\":2}, {\"pw\":1}])\n",
    "            reducers_cfg[FactorAnalysis.__name__][\"reducer__n_components\"] = n_components\n",
    "            reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n",
    "            reducers_cfg[GenericUnivariateSelect.__name__][\"reducer__param\"] = n_components\n",
    "            reducers_cfg[RFE.__name__][\"reducer__n_features_to_select\"] = n_components\n",
    "            run_grid_search(x,y,preprocessor, transfomer, reducer, model, res_dict, error, errors_ind)\n",
    "        elif preprocessor.func.__name__ == PolynomialTransformer.func.__name__:\n",
    "            kw_arg_powers = get_powers_list(n_samples, n_features, 3)\n",
    "            pw_lst = []\n",
    "            for pw in kw_arg_powers:\n",
    "                pw_lst = pw_lst + [pw]\n",
    "                preprocessors_cfg[PolynomialTransformer.func.__name__][\"preprocessor__kw_args\"] = [pw]\n",
    "                n_components = get_components_list(n_features, pw_lst)\n",
    "                reducers_cfg[FactorAnalysis.__name__][\"reducer__n_components\"] = n_components\n",
    "                reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n",
    "                reducers_cfg[GenericUnivariateSelect.__name__][\"reducer__param\"] = n_components\n",
    "                reducers_cfg[RFE.__name__][\"reducer__n_features_to_select\"] = n_components\n",
    "                run_grid_search(x,y,preprocessor, transfomer, reducer, model, res_dict, error, errors_ind)\n",
    "        else:\n",
    "            n_components = get_components_list(n_features, [{\"pw\":1}])\n",
    "            reducers_cfg[FactorAnalysis.__name__][\"reducer__n_components\"] = n_components\n",
    "            reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n",
    "            reducers_cfg[GenericUnivariateSelect.__name__][\"reducer__param\"] = n_components\n",
    "            reducers_cfg[RFE.__name__][\"reducer__n_features_to_select\"] = n_components\n",
    "            run_grid_search(x,y,preprocessor, transfomer, reducer, model, res_dict, error, errors_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################\n",
      "\n",
      "####################################################################################\n",
      "***Starting [LinearRegression] estimator run, pipeline: | preprocessor:poly | transfomer: dummy | reducer: FactorAnalysis \n",
      "##param_grid##\n",
      "{'reducer__svd_method': ['randomized'], 'preprocessor__kw_args': [{'pw': 24}], 'reducer__n_components': [3311, 1932, 552, 414, 276, 110, 69, 55, 13, 5, 3, 1], 'model__fit_intercept': [True]}\n",
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "[CV] model__fit_intercept=True, preprocessor__kw_args={'pw': 24}, reducer__svd_method=randomized, reducer__n_components=3311 \n",
      "[CV] model__fit_intercept=True, preprocessor__kw_args={'pw': 24}, reducer__svd_method=randomized, reducer__n_components=3311 \n",
      "[CV] model__fit_intercept=True, preprocessor__kw_args={'pw': 24}, reducer__svd_method=randomized, reducer__n_components=3311 \n",
      "[CV] model__fit_intercept=True, preprocessor__kw_args={'pw': 24}, reducer__svd_method=randomized, reducer__n_components=3311 \n",
      "[CV] model__fit_intercept=True, preprocessor__kw_args={'pw': 24}, reducer__svd_method=randomized, reducer__n_components=1932 \n",
      "[CV] model__fit_intercept=True, preprocessor__kw_args={'pw': 24}, reducer__svd_method=randomized, reducer__n_components=1932 \n",
      "[CV] model__fit_intercept=True, preprocessor__kw_args={'pw': 24}, reducer__svd_method=randomized, reducer__n_components=1932 \n",
      "[CV] model__fit_intercept=True, preprocessor__kw_args={'pw': 24}, reducer__svd_method=randomized, reducer__n_components=1932 \n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    165         sys.exit(msg)\n    166     main_globals = sys.modules[\"__main__\"].__dict__\n    167     if alter_argv:\n    168         sys.argv[0] = mod_spec.origin\n    169     return _run_code(code, main_globals, None,\n--> 170                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py')\n    171 \n    172 def run_module(mod_name, init_globals=None,\n    173                run_name=None, alter_sys=False):\n    174     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_code(code=<code object <module> at 0x7f559ea0e300, file \"/...3.4/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f559ea0e300, file \"/...3.4/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 5\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 5), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 5)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=5)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': '            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 4, 7, 11, 8, 12, 81734, tzinfo=datetime.timezone.utc), 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'session': '32E9374792424E3182B658E9BE4E49FC', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'32E9374792424E3182B658E9BE4E49FC']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': '            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 4, 7, 11, 8, 12, 81734, tzinfo=datetime.timezone.utc), 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'session': '32E9374792424E3182B658E9BE4E49FC', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'32E9374792424E3182B658E9BE4E49FC'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': '            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 4, 7, 11, 8, 12, 81734, tzinfo=datetime.timezone.utc), 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'session': '32E9374792424E3182B658E9BE4E49FC', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = '            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-5-20e93172e809>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f556f0f8128, executi..._before_exec=None error_in_exec=None result=None>)\n   2816 \n   2817         try:\n   2818             for i, node in enumerate(to_run_exec):\n   2819                 mod = ast.Module([node])\n   2820                 code = compiler(mod, cell_name, \"exec\")\n-> 2821                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f5561c8bd20, file \"<ipython-input-5-20e93172e809>\", line 3>\n        result = <ExecutionResult object at 7f556f0f8128, executi..._before_exec=None error_in_exec=None result=None>\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f5561c8bd20, file \"<ipython-input-5-20e93172e809>\", line 3>, result=<ExecutionResult object at 7f556f0f8128, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f5561c8bd20, file \"<ipython-input-5-20e93172e809>\", line 3>\n        self.user_global_ns = {'ARDRegression': <class 'sklearn.linear_model.bayes.ARDRegression'>, 'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'BayesianRidge': <class 'sklearn.linear_model.bayes.BayesianRidge'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'DummyTransformer': FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True), 'ElasticNet': <class 'sklearn.linear_model.coordinate_descent.ElasticNet'>, 'ExtraTreeRegressor': <class 'sklearn.tree.tree.ExtraTreeRegressor'>, 'ExtraTreesRegressor': <class 'sklearn.ensemble.forest.ExtraTreesRegressor'>, 'FactorAnalysis': <class 'sklearn.decomposition.factor_analysis.FactorAnalysis'>, ...}\n        self.user_ns = {'ARDRegression': <class 'sklearn.linear_model.bayes.ARDRegression'>, 'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'BayesianRidge': <class 'sklearn.linear_model.bayes.BayesianRidge'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'DummyTransformer': FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True), 'ElasticNet': <class 'sklearn.linear_model.coordinate_descent.ElasticNet'>, 'ExtraTreeRegressor': <class 'sklearn.tree.tree.ExtraTreeRegressor'>, 'ExtraTreesRegressor': <class 'sklearn.ensemble.forest.ExtraTreesRegressor'>, 'FactorAnalysis': <class 'sklearn.decomposition.factor_analysis.FactorAnalysis'>, ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/user/data_mining/logistic_regression/<ipython-input-5-20e93172e809> in <module>()\n      1 \n      2 \n----> 3 \n      4 \n      5 ##run solver can be wrapped in a loop for different sets of classes (different sets of X and y), so that for a best model is found for each class. In this case an independets results array has to be passed for each class, otherwise eahcn new function call will rewrite the results from the previous simnulation.\n      6 run_solver(X,y, preprocessors, transfomers, reducers, models, results, errors, errors_ind)\n      7 print (\"##############################\")\n      8 print (\"###Finished all estimators####\")\n      9 print (\"##############################\")\n     10 \n     11 print (\"##############################\")\n     12 print (\"######Printing all errors#####\")\n     13 print (\"##############################\")\n\n...........................................................................\n/home/user/data_mining/logistic_regression/<ipython-input-4-58aade27efb4> in run_solver(x=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, preprocessors=[FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True)], transfomers=[FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True)], reducers=[FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01)], models=[LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)], res_dict={}, error={}, errors_ind={})\n    171                 n_components = get_components_list(n_features, pw_lst)\n    172                 reducers_cfg[FactorAnalysis.__name__][\"reducer__n_components\"] = n_components\n    173                 reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n    174                 reducers_cfg[GenericUnivariateSelect.__name__][\"reducer__param\"] = n_components\n    175                 reducers_cfg[RFE.__name__][\"reducer__n_features_to_select\"] = n_components\n--> 176                 run_grid_search(x,y,preprocessor, transfomer, reducer, model, res_dict, error, errors_ind)\n    177         else:\n    178             n_components = get_components_list(n_features, [{\"pw\":1}])\n    179             reducers_cfg[FactorAnalysis.__name__][\"reducer__n_components\"] = n_components\n    180             reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n\n...........................................................................\n/home/user/data_mining/logistic_regression/<ipython-input-4-58aade27efb4> in run_grid_search(x=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, preprocessor=FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True), transfomer=FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True), reducer=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), res_dict={}, error={}, errors_ind={})\n    132     print(\"##param_grid##\")\n    133     print(param_grid)\n    134     estimator = GridSearchCV(pipe,param_grid,verbose=2, cv=cv, n_jobs=4)\n    135     #run the esmimator, except eceptions, sape errors\n    136     try:\n--> 137             estimator.fit(x, y)\n    138             print (\"GREP_ME***Results of [\"  + name + \"] estimatorrun are\")\n    139             print (estimator.cv_results_)\n    140             print (\"GREP_ME***Best params of [\"  + name + \"] estimator,pipeline:\"+ pipeline_cfg+\"  run are\")\n    141             print (estimator.best_params_)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=4, error_score='raise',\n       e...train_score=True,\n       scoring=None, verbose=2), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,\n       scoring=None, verbose=2)>\n        X =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns]\n        y = 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64\n        groups = None\n        self.param_grid = {'model__fit_intercept': [True], 'preprocessor__kw_args': [{'pw': 24}], 'reducer__n_components': [3311, 1932, 552, 414, 276, 110, 69, 55, 13, 5, 3, 1], 'reducer__svd_method': ['randomized']}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=4, error_score='raise',\n       e...train_score=True,\n       scoring=None, verbose=2), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=4), iterable=<generator object <genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=4)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Apr  7 12:10:36 2017\nPID: 29529                                   Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1203, 1204, 1205, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1200, 1201, 1202]), 2, {'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1203, 1204, 1205, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1200, 1201, 1202]), 2, {'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, scorer=<function _passthrough_scorer>, train=array([1203, 1204, 1205, ..., 4809, 4810, 4811]), test=array([   0,    1,    2, ..., 1200, 1201, 1202]), verbose=2, parameters={'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...it_intercept=True, n_jobs=1, normalize=False))])>\n        X_train =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns]\n        y_train = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns], y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...it_intercept=True, n_jobs=1, normalize=False))])>\n        X =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns]\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns], y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]])\n        transform.fit_transform = <bound method FactorAnalysis.fit_transform of Fa...te=0, svd_method='randomized',\n        tol=0.01)>\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params_steps = {'model': {}, 'preprocessor': {}, 'reducer': {}, 'transfomer': {}}\n        name = 'reducer'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in fit_transform(self=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), X=array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]]), y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    492         if y is None:\n    493             # fit method of arity 1 (unsupervised transformation)\n    494             return self.fit(X, **fit_params).transform(X)\n    495         else:\n    496             # fit method of arity 2 (supervised transformation)\n--> 497             return self.fit(X, y, **fit_params).transform(X)\n        self.fit = <bound method FactorAnalysis.fit of FactorAnalys...te=0, svd_method='randomized',\n        tol=0.01)>\n        X = array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]])\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params.transform = undefined\n    498 \n    499 \n    500 class DensityMixin(object):\n    501     \"\"\"Mixin class for all density estimators in scikit-learn.\"\"\"\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py in fit(self=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), X=array([[  1.40658867e+04,  -7.67417567e+02,  -6....085065e-04,  -1.02521474e-02,  -5.54170130e-04]]), y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64)\n    198                              ' the documentation' % self.svd_method)\n    199 \n    200         for i in xrange(self.max_iter):\n    201             # SMALL helps numerics\n    202             sqrt_psi = np.sqrt(psi) + SMALL\n--> 203             s, V, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n        s = array([ inf,  inf,  inf, ...,  inf,  inf,  inf])\n        V = undefined\n        unexp_var = nan\n        my_svd = <function FactorAnalysis.fit.<locals>.my_svd>\n        X = array([[  1.40658867e+04,  -7.67417567e+02,  -6....085065e-04,  -1.02521474e-02,  -5.54170130e-04]])\n        sqrt_psi = array([             nan,              nan,      ...0000100e-06,   1.00000100e-06,   1.00000100e-06])\n        nsqrt = 60.07495318350236\n    204             s **= 2\n    205             # Use 'maximum' here to avoid sqrt problems.\n    206             W = np.sqrt(np.maximum(s - 1., 0.))[:, np.newaxis] * V\n    207             del V\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py in my_svd(X=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]))\n    189             random_state = check_random_state(self.random_state)\n    190 \n    191             def my_svd(X):\n    192                 _, s, V = randomized_svd(X, n_components,\n    193                                          random_state=random_state,\n--> 194                                          n_iter=self.iterated_power)\n    195                 return s, V, squared_norm(X) - squared_norm(s)\n    196         else:\n    197             raise ValueError('SVD method %s is not supported. Please consider'\n    198                              ' the documentation' % self.svd_method)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py in randomized_svd(M=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]), n_components=3311, n_oversamples=10, n_iter=3, power_iteration_normalizer='auto', transpose=False, flip_sign=True, random_state=<mtrand.RandomState object>)\n    359     if transpose:\n    360         # this implementation is a bit faster with smaller shape[1]\n    361         M = M.T\n    362 \n    363     Q = randomized_range_finder(M, n_random, n_iter,\n--> 364                                 power_iteration_normalizer, random_state)\n        power_iteration_normalizer = 'auto'\n        random_state = <mtrand.RandomState object>\n    365 \n    366     # project M to the (k + p) dimensional space using the basis vectors\n    367     B = safe_sparse_dot(Q.T, M)\n    368 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py in randomized_range_finder(A=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]), size=3321, n_iter=3, power_iteration_normalizer='LU', random_state=<mtrand.RandomState object>)\n    253     for i in range(n_iter):\n    254         if power_iteration_normalizer == 'none':\n    255             Q = safe_sparse_dot(A, Q)\n    256             Q = safe_sparse_dot(A.T, Q)\n    257         elif power_iteration_normalizer == 'LU':\n--> 258             Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n        Q = array([[ 0.35356032,  0.48941691,  0.30663461, .... -0.42312625,\n         0.33898944, -0.36818682]])\n        _ = undefined\n        A = array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]])\n    259             Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)\n    260         elif power_iteration_normalizer == 'QR':\n    261             Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n    262             Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/scipy/linalg/decomp_lu.py in lu(a=array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]]), permute_l=True, overwrite_a=False, check_finite=True)\n    173     -----\n    174     This is a LU factorization routine written for Scipy.\n    175 \n    176     \"\"\"\n    177     if check_finite:\n--> 178         a1 = asarray_chkfinite(a)\n        a1 = undefined\n        a = array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]])\n    179     else:\n    180         a1 = asarray(a)\n    181     if len(a1.shape) != 2:\n    182         raise ValueError('expected matrix')\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/numpy/lib/function_base.py in asarray_chkfinite(a=array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]]), dtype=None, order=None)\n   1210 \n   1211     \"\"\"\n   1212     a = asarray(a, dtype=dtype, order=order)\n   1213     if a.dtype.char in typecodes['AllFloat'] and not np.isfinite(a).all():\n   1214         raise ValueError(\n-> 1215             \"array must not contain infs or NaNs\")\n   1216     return a\n   1217 \n   1218 \n   1219 def piecewise(x, condlist, funclist, *args, **kw):\n\nValueError: array must not contain infs or NaNs\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py\", line 238, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\", line 268, in fit\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\", line 234, in _fit\n    Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/base.py\", line 497, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py\", line 203, in fit\n    s, V, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py\", line 194, in my_svd\n    n_iter=self.iterated_power)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py\", line 364, in randomized_svd\n    power_iteration_normalizer, random_state)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py\", line 258, in randomized_range_finder\n    Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n  File \"/usr/local/lib/python3.4/dist-packages/scipy/linalg/decomp_lu.py\", line 178, in lu\n    a1 = asarray_chkfinite(a)\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/lib/function_base.py\", line 1215, in asarray_chkfinite\n    \"array must not contain infs or NaNs\")\nValueError: array must not contain infs or NaNs\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 353, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Apr  7 12:10:36 2017\nPID: 29529                                   Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1203, 1204, 1205, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1200, 1201, 1202]), 2, {'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1203, 1204, 1205, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1200, 1201, 1202]), 2, {'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, scorer=<function _passthrough_scorer>, train=array([1203, 1204, 1205, ..., 4809, 4810, 4811]), test=array([   0,    1,    2, ..., 1200, 1201, 1202]), verbose=2, parameters={'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...it_intercept=True, n_jobs=1, normalize=False))])>\n        X_train =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns]\n        y_train = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns], y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...it_intercept=True, n_jobs=1, normalize=False))])>\n        X =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns]\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns], y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]])\n        transform.fit_transform = <bound method FactorAnalysis.fit_transform of Fa...te=0, svd_method='randomized',\n        tol=0.01)>\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params_steps = {'model': {}, 'preprocessor': {}, 'reducer': {}, 'transfomer': {}}\n        name = 'reducer'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in fit_transform(self=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), X=array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]]), y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    492         if y is None:\n    493             # fit method of arity 1 (unsupervised transformation)\n    494             return self.fit(X, **fit_params).transform(X)\n    495         else:\n    496             # fit method of arity 2 (supervised transformation)\n--> 497             return self.fit(X, y, **fit_params).transform(X)\n        self.fit = <bound method FactorAnalysis.fit of FactorAnalys...te=0, svd_method='randomized',\n        tol=0.01)>\n        X = array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]])\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params.transform = undefined\n    498 \n    499 \n    500 class DensityMixin(object):\n    501     \"\"\"Mixin class for all density estimators in scikit-learn.\"\"\"\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py in fit(self=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), X=array([[  1.40658867e+04,  -7.67417567e+02,  -6....085065e-04,  -1.02521474e-02,  -5.54170130e-04]]), y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64)\n    198                              ' the documentation' % self.svd_method)\n    199 \n    200         for i in xrange(self.max_iter):\n    201             # SMALL helps numerics\n    202             sqrt_psi = np.sqrt(psi) + SMALL\n--> 203             s, V, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n        s = array([ inf,  inf,  inf, ...,  inf,  inf,  inf])\n        V = undefined\n        unexp_var = nan\n        my_svd = <function FactorAnalysis.fit.<locals>.my_svd>\n        X = array([[  1.40658867e+04,  -7.67417567e+02,  -6....085065e-04,  -1.02521474e-02,  -5.54170130e-04]])\n        sqrt_psi = array([             nan,              nan,      ...0000100e-06,   1.00000100e-06,   1.00000100e-06])\n        nsqrt = 60.07495318350236\n    204             s **= 2\n    205             # Use 'maximum' here to avoid sqrt problems.\n    206             W = np.sqrt(np.maximum(s - 1., 0.))[:, np.newaxis] * V\n    207             del V\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py in my_svd(X=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]))\n    189             random_state = check_random_state(self.random_state)\n    190 \n    191             def my_svd(X):\n    192                 _, s, V = randomized_svd(X, n_components,\n    193                                          random_state=random_state,\n--> 194                                          n_iter=self.iterated_power)\n    195                 return s, V, squared_norm(X) - squared_norm(s)\n    196         else:\n    197             raise ValueError('SVD method %s is not supported. Please consider'\n    198                              ' the documentation' % self.svd_method)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py in randomized_svd(M=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]), n_components=3311, n_oversamples=10, n_iter=3, power_iteration_normalizer='auto', transpose=False, flip_sign=True, random_state=<mtrand.RandomState object>)\n    359     if transpose:\n    360         # this implementation is a bit faster with smaller shape[1]\n    361         M = M.T\n    362 \n    363     Q = randomized_range_finder(M, n_random, n_iter,\n--> 364                                 power_iteration_normalizer, random_state)\n        power_iteration_normalizer = 'auto'\n        random_state = <mtrand.RandomState object>\n    365 \n    366     # project M to the (k + p) dimensional space using the basis vectors\n    367     B = safe_sparse_dot(Q.T, M)\n    368 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py in randomized_range_finder(A=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]), size=3321, n_iter=3, power_iteration_normalizer='LU', random_state=<mtrand.RandomState object>)\n    253     for i in range(n_iter):\n    254         if power_iteration_normalizer == 'none':\n    255             Q = safe_sparse_dot(A, Q)\n    256             Q = safe_sparse_dot(A.T, Q)\n    257         elif power_iteration_normalizer == 'LU':\n--> 258             Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n        Q = array([[ 0.35356032,  0.48941691,  0.30663461, .... -0.42312625,\n         0.33898944, -0.36818682]])\n        _ = undefined\n        A = array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]])\n    259             Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)\n    260         elif power_iteration_normalizer == 'QR':\n    261             Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n    262             Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/scipy/linalg/decomp_lu.py in lu(a=array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]]), permute_l=True, overwrite_a=False, check_finite=True)\n    173     -----\n    174     This is a LU factorization routine written for Scipy.\n    175 \n    176     \"\"\"\n    177     if check_finite:\n--> 178         a1 = asarray_chkfinite(a)\n        a1 = undefined\n        a = array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]])\n    179     else:\n    180         a1 = asarray(a)\n    181     if len(a1.shape) != 2:\n    182         raise ValueError('expected matrix')\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/numpy/lib/function_base.py in asarray_chkfinite(a=array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]]), dtype=None, order=None)\n   1210 \n   1211     \"\"\"\n   1212     a = asarray(a, dtype=dtype, order=order)\n   1213     if a.dtype.char in typecodes['AllFloat'] and not np.isfinite(a).all():\n   1214         raise ValueError(\n-> 1215             \"array must not contain infs or NaNs\")\n   1216     return a\n   1217 \n   1218 \n   1219 def piecewise(x, condlist, funclist, *args, **kw):\n\nValueError: array must not contain infs or NaNs\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Apr  7 12:10:36 2017\nPID: 29529                                   Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1203, 1204, 1205, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1200, 1201, 1202]), 2, {'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1203, 1204, 1205, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1200, 1201, 1202]), 2, {'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, scorer=<function _passthrough_scorer>, train=array([1203, 1204, 1205, ..., 4809, 4810, 4811]), test=array([   0,    1,    2, ..., 1200, 1201, 1202]), verbose=2, parameters={'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...it_intercept=True, n_jobs=1, normalize=False))])>\n        X_train =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns]\n        y_train = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns], y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...it_intercept=True, n_jobs=1, normalize=False))])>\n        X =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns]\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns], y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]])\n        transform.fit_transform = <bound method FactorAnalysis.fit_transform of Fa...te=0, svd_method='randomized',\n        tol=0.01)>\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params_steps = {'model': {}, 'preprocessor': {}, 'reducer': {}, 'transfomer': {}}\n        name = 'reducer'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in fit_transform(self=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), X=array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]]), y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    492         if y is None:\n    493             # fit method of arity 1 (unsupervised transformation)\n    494             return self.fit(X, **fit_params).transform(X)\n    495         else:\n    496             # fit method of arity 2 (supervised transformation)\n--> 497             return self.fit(X, y, **fit_params).transform(X)\n        self.fit = <bound method FactorAnalysis.fit of FactorAnalys...te=0, svd_method='randomized',\n        tol=0.01)>\n        X = array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]])\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params.transform = undefined\n    498 \n    499 \n    500 class DensityMixin(object):\n    501     \"\"\"Mixin class for all density estimators in scikit-learn.\"\"\"\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py in fit(self=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), X=array([[  1.40658867e+04,  -7.67417567e+02,  -6....085065e-04,  -1.02521474e-02,  -5.54170130e-04]]), y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64)\n    198                              ' the documentation' % self.svd_method)\n    199 \n    200         for i in xrange(self.max_iter):\n    201             # SMALL helps numerics\n    202             sqrt_psi = np.sqrt(psi) + SMALL\n--> 203             s, V, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n        s = array([ inf,  inf,  inf, ...,  inf,  inf,  inf])\n        V = undefined\n        unexp_var = nan\n        my_svd = <function FactorAnalysis.fit.<locals>.my_svd>\n        X = array([[  1.40658867e+04,  -7.67417567e+02,  -6....085065e-04,  -1.02521474e-02,  -5.54170130e-04]])\n        sqrt_psi = array([             nan,              nan,      ...0000100e-06,   1.00000100e-06,   1.00000100e-06])\n        nsqrt = 60.07495318350236\n    204             s **= 2\n    205             # Use 'maximum' here to avoid sqrt problems.\n    206             W = np.sqrt(np.maximum(s - 1., 0.))[:, np.newaxis] * V\n    207             del V\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py in my_svd(X=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]))\n    189             random_state = check_random_state(self.random_state)\n    190 \n    191             def my_svd(X):\n    192                 _, s, V = randomized_svd(X, n_components,\n    193                                          random_state=random_state,\n--> 194                                          n_iter=self.iterated_power)\n    195                 return s, V, squared_norm(X) - squared_norm(s)\n    196         else:\n    197             raise ValueError('SVD method %s is not supported. Please consider'\n    198                              ' the documentation' % self.svd_method)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py in randomized_svd(M=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]), n_components=3311, n_oversamples=10, n_iter=3, power_iteration_normalizer='auto', transpose=False, flip_sign=True, random_state=<mtrand.RandomState object>)\n    359     if transpose:\n    360         # this implementation is a bit faster with smaller shape[1]\n    361         M = M.T\n    362 \n    363     Q = randomized_range_finder(M, n_random, n_iter,\n--> 364                                 power_iteration_normalizer, random_state)\n        power_iteration_normalizer = 'auto'\n        random_state = <mtrand.RandomState object>\n    365 \n    366     # project M to the (k + p) dimensional space using the basis vectors\n    367     B = safe_sparse_dot(Q.T, M)\n    368 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py in randomized_range_finder(A=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]), size=3321, n_iter=3, power_iteration_normalizer='LU', random_state=<mtrand.RandomState object>)\n    253     for i in range(n_iter):\n    254         if power_iteration_normalizer == 'none':\n    255             Q = safe_sparse_dot(A, Q)\n    256             Q = safe_sparse_dot(A.T, Q)\n    257         elif power_iteration_normalizer == 'LU':\n--> 258             Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n        Q = array([[ 0.35356032,  0.48941691,  0.30663461, .... -0.42312625,\n         0.33898944, -0.36818682]])\n        _ = undefined\n        A = array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]])\n    259             Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)\n    260         elif power_iteration_normalizer == 'QR':\n    261             Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n    262             Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/scipy/linalg/decomp_lu.py in lu(a=array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]]), permute_l=True, overwrite_a=False, check_finite=True)\n    173     -----\n    174     This is a LU factorization routine written for Scipy.\n    175 \n    176     \"\"\"\n    177     if check_finite:\n--> 178         a1 = asarray_chkfinite(a)\n        a1 = undefined\n        a = array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]])\n    179     else:\n    180         a1 = asarray(a)\n    181     if len(a1.shape) != 2:\n    182         raise ValueError('expected matrix')\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/numpy/lib/function_base.py in asarray_chkfinite(a=array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]]), dtype=None, order=None)\n   1210 \n   1211     \"\"\"\n   1212     a = asarray(a, dtype=dtype, order=order)\n   1213     if a.dtype.char in typecodes['AllFloat'] and not np.isfinite(a).all():\n   1214         raise ValueError(\n-> 1215             \"array must not contain infs or NaNs\")\n   1216     return a\n   1217 \n   1218 \n   1219 def piecewise(x, condlist, funclist, *args, **kw):\n\nValueError: array must not contain infs or NaNs\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-20e93172e809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m##run solver can be wrapped in a loop for different sets of classes (different sets of X and y), so that for a best model is found for each class. In this case an independets results array has to be passed for each class, otherwise eahcn new function call will rewrite the results from the previous simnulation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfomers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"##############################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"###Finished all estimators####\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-58aade27efb4>\u001b[0m in \u001b[0;36mrun_solver\u001b[0;34m(x, y, preprocessors, transfomers, reducers, models, res_dict, error, errors_ind)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mreducers_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGenericUnivariateSelect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reducer__param\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mreducers_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRFE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reducer__n_features_to_select\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mrun_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfomer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_components_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"pw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-58aade27efb4>\u001b[0m in \u001b[0;36mrun_grid_search\u001b[0;34m(x, y, preprocessor, transfomer, reducer, model, res_dict, error, errors_ind)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m#run the esmimator, except eceptions, sape errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"GREP_ME***Results of [\"\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"] estimatorrun are\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    165         sys.exit(msg)\n    166     main_globals = sys.modules[\"__main__\"].__dict__\n    167     if alter_argv:\n    168         sys.argv[0] = mod_spec.origin\n    169     return _run_code(code, main_globals, None,\n--> 170                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py')\n    171 \n    172 def run_module(mod_name, init_globals=None,\n    173                run_name=None, alter_sys=False):\n    174     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_code(code=<code object <module> at 0x7f559ea0e300, file \"/...3.4/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f559ea0e300, file \"/...3.4/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 5\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 5), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 5)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=5)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': '            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 4, 7, 11, 8, 12, 81734, tzinfo=datetime.timezone.utc), 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'session': '32E9374792424E3182B658E9BE4E49FC', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'32E9374792424E3182B658E9BE4E49FC']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': '            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 4, 7, 11, 8, 12, 81734, tzinfo=datetime.timezone.utc), 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'session': '32E9374792424E3182B658E9BE4E49FC', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'32E9374792424E3182B658E9BE4E49FC'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': '            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 4, 7, 11, 8, 12, 81734, tzinfo=datetime.timezone.utc), 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'session': '32E9374792424E3182B658E9BE4E49FC', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4E0FE23245594B3D89F5DFF8FD8E3880', 'msg_type': 'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = '            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='            \\n##run solver can be wrapped in a lo...\"##############################\")\\nprint(results)\\n', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-5-20e93172e809>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f556f0f8128, executi..._before_exec=None error_in_exec=None result=None>)\n   2816 \n   2817         try:\n   2818             for i, node in enumerate(to_run_exec):\n   2819                 mod = ast.Module([node])\n   2820                 code = compiler(mod, cell_name, \"exec\")\n-> 2821                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f5561c8bd20, file \"<ipython-input-5-20e93172e809>\", line 3>\n        result = <ExecutionResult object at 7f556f0f8128, executi..._before_exec=None error_in_exec=None result=None>\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f5561c8bd20, file \"<ipython-input-5-20e93172e809>\", line 3>, result=<ExecutionResult object at 7f556f0f8128, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f5561c8bd20, file \"<ipython-input-5-20e93172e809>\", line 3>\n        self.user_global_ns = {'ARDRegression': <class 'sklearn.linear_model.bayes.ARDRegression'>, 'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'BayesianRidge': <class 'sklearn.linear_model.bayes.BayesianRidge'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'DummyTransformer': FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True), 'ElasticNet': <class 'sklearn.linear_model.coordinate_descent.ElasticNet'>, 'ExtraTreeRegressor': <class 'sklearn.tree.tree.ExtraTreeRegressor'>, 'ExtraTreesRegressor': <class 'sklearn.ensemble.forest.ExtraTreesRegressor'>, 'FactorAnalysis': <class 'sklearn.decomposition.factor_analysis.FactorAnalysis'>, ...}\n        self.user_ns = {'ARDRegression': <class 'sklearn.linear_model.bayes.ARDRegression'>, 'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'BayesianRidge': <class 'sklearn.linear_model.bayes.BayesianRidge'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'DummyTransformer': FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True), 'ElasticNet': <class 'sklearn.linear_model.coordinate_descent.ElasticNet'>, 'ExtraTreeRegressor': <class 'sklearn.tree.tree.ExtraTreeRegressor'>, 'ExtraTreesRegressor': <class 'sklearn.ensemble.forest.ExtraTreesRegressor'>, 'FactorAnalysis': <class 'sklearn.decomposition.factor_analysis.FactorAnalysis'>, ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/user/data_mining/logistic_regression/<ipython-input-5-20e93172e809> in <module>()\n      1 \n      2 \n----> 3 \n      4 \n      5 ##run solver can be wrapped in a loop for different sets of classes (different sets of X and y), so that for a best model is found for each class. In this case an independets results array has to be passed for each class, otherwise eahcn new function call will rewrite the results from the previous simnulation.\n      6 run_solver(X,y, preprocessors, transfomers, reducers, models, results, errors, errors_ind)\n      7 print (\"##############################\")\n      8 print (\"###Finished all estimators####\")\n      9 print (\"##############################\")\n     10 \n     11 print (\"##############################\")\n     12 print (\"######Printing all errors#####\")\n     13 print (\"##############################\")\n\n...........................................................................\n/home/user/data_mining/logistic_regression/<ipython-input-4-58aade27efb4> in run_solver(x=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, preprocessors=[FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True)], transfomers=[FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True)], reducers=[FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01)], models=[LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)], res_dict={}, error={}, errors_ind={})\n    171                 n_components = get_components_list(n_features, pw_lst)\n    172                 reducers_cfg[FactorAnalysis.__name__][\"reducer__n_components\"] = n_components\n    173                 reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n    174                 reducers_cfg[GenericUnivariateSelect.__name__][\"reducer__param\"] = n_components\n    175                 reducers_cfg[RFE.__name__][\"reducer__n_features_to_select\"] = n_components\n--> 176                 run_grid_search(x,y,preprocessor, transfomer, reducer, model, res_dict, error, errors_ind)\n    177         else:\n    178             n_components = get_components_list(n_features, [{\"pw\":1}])\n    179             reducers_cfg[FactorAnalysis.__name__][\"reducer__n_components\"] = n_components\n    180             reducers_cfg[PCA.__name__][\"reducer__n_components\"] = n_components\n\n...........................................................................\n/home/user/data_mining/logistic_regression/<ipython-input-4-58aade27efb4> in run_grid_search(x=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, preprocessor=FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True), transfomer=FunctionTransformer(accept_sparse=False,\n       ...=None, kw_args=None, pass_y=False, validate=True), reducer=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), model=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False), res_dict={}, error={}, errors_ind={})\n    132     print(\"##param_grid##\")\n    133     print(param_grid)\n    134     estimator = GridSearchCV(pipe,param_grid,verbose=2, cv=cv, n_jobs=4)\n    135     #run the esmimator, except eceptions, sape errors\n    136     try:\n--> 137             estimator.fit(x, y)\n    138             print (\"GREP_ME***Results of [\"  + name + \"] estimatorrun are\")\n    139             print (estimator.cv_results_)\n    140             print (\"GREP_ME***Best params of [\"  + name + \"] estimator,pipeline:\"+ pipeline_cfg+\"  run are\")\n    141             print (estimator.best_params_)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=4, error_score='raise',\n       e...train_score=True,\n       scoring=None, verbose=2), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,\n       scoring=None, verbose=2)>\n        X =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns]\n        y = 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64\n        groups = None\n        self.param_grid = {'model__fit_intercept': [True], 'preprocessor__kw_args': [{'pw': 24}], 'reducer__n_components': [3311, 1932, 552, 414, 276, 110, 69, 55, 13, 5, 3, 1], 'reducer__svd_method': ['randomized']}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=4, error_score='raise',\n       e...train_score=True,\n       scoring=None, verbose=2), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=4), iterable=<generator object <genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=4)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Apr  7 12:10:36 2017\nPID: 29529                                   Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1203, 1204, 1205, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1200, 1201, 1202]), 2, {'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], 4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1203, 1204, 1205, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1200, 1201, 1202]), 2, {'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[4812 rows x 138 columns], y=4006       881950\n1122    104407366\n675     1787...     12600000\nName: worldwide_gross, dtype: int64, scorer=<function _passthrough_scorer>, train=array([1203, 1204, 1205, ..., 4809, 4810, 4811]), test=array([   0,    1,    2, ..., 1200, 1201, 1202]), verbose=2, parameters={'model__fit_intercept': True, 'preprocessor__kw_args': {'pw': 24}, 'reducer__n_components': 3311, 'reducer__svd_method': 'randomized'}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...it_intercept=True, n_jobs=1, normalize=False))])>\n        X_train =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns]\n        y_train = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns], y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...it_intercept=True, n_jobs=1, normalize=False))])>\n        X =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns]\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...fit_intercept=True, n_jobs=1, normalize=False))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  \n\n[3609 rows x 138 columns], y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]])\n        transform.fit_transform = <bound method FactorAnalysis.fit_transform of Fa...te=0, svd_method='randomized',\n        tol=0.01)>\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params_steps = {'model': {}, 'preprocessor': {}, 'reducer': {}, 'transfomer': {}}\n        name = 'reducer'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in fit_transform(self=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), X=array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]]), y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64, **fit_params={})\n    492         if y is None:\n    493             # fit method of arity 1 (unsupervised transformation)\n    494             return self.fit(X, **fit_params).transform(X)\n    495         else:\n    496             # fit method of arity 2 (supervised transformation)\n--> 497             return self.fit(X, y, **fit_params).transform(X)\n        self.fit = <bound method FactorAnalysis.fit of FactorAnalys...te=0, svd_method='randomized',\n        tol=0.01)>\n        X = array([[ 21000.,   1000.,    616., ...,      0.,... 160.,    132., ...,      0.,      0.,      0.]])\n        y = 2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64\n        fit_params.transform = undefined\n    498 \n    499 \n    500 class DensityMixin(object):\n    501     \"\"\"Mixin class for all density estimators in scikit-learn.\"\"\"\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py in fit(self=FactorAnalysis(copy=True, iterated_power=3, max_...ate=0, svd_method='randomized',\n        tol=0.01), X=array([[  1.40658867e+04,  -7.67417567e+02,  -6....085065e-04,  -1.02521474e-02,  -5.54170130e-04]]), y=2861     16829464\n1847     50445860\n4196       2...     12600000\nName: worldwide_gross, dtype: int64)\n    198                              ' the documentation' % self.svd_method)\n    199 \n    200         for i in xrange(self.max_iter):\n    201             # SMALL helps numerics\n    202             sqrt_psi = np.sqrt(psi) + SMALL\n--> 203             s, V, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n        s = array([ inf,  inf,  inf, ...,  inf,  inf,  inf])\n        V = undefined\n        unexp_var = nan\n        my_svd = <function FactorAnalysis.fit.<locals>.my_svd>\n        X = array([[  1.40658867e+04,  -7.67417567e+02,  -6....085065e-04,  -1.02521474e-02,  -5.54170130e-04]])\n        sqrt_psi = array([             nan,              nan,      ...0000100e-06,   1.00000100e-06,   1.00000100e-06])\n        nsqrt = 60.07495318350236\n    204             s **= 2\n    205             # Use 'maximum' here to avoid sqrt problems.\n    206             W = np.sqrt(np.maximum(s - 1., 0.))[:, np.newaxis] * V\n    207             del V\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/decomposition/factor_analysis.py in my_svd(X=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]))\n    189             random_state = check_random_state(self.random_state)\n    190 \n    191             def my_svd(X):\n    192                 _, s, V = randomized_svd(X, n_components,\n    193                                          random_state=random_state,\n--> 194                                          n_iter=self.iterated_power)\n    195                 return s, V, squared_norm(X) - squared_norm(s)\n    196         else:\n    197             raise ValueError('SVD method %s is not supported. Please consider'\n    198                              ' the documentation' % self.svd_method)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py in randomized_svd(M=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]), n_components=3311, n_oversamples=10, n_iter=3, power_iteration_normalizer='auto', transpose=False, flip_sign=True, random_state=<mtrand.RandomState object>)\n    359     if transpose:\n    360         # this implementation is a bit faster with smaller shape[1]\n    361         M = M.T\n    362 \n    363     Q = randomized_range_finder(M, n_random, n_iter,\n--> 364                                 power_iteration_normalizer, random_state)\n        power_iteration_normalizer = 'auto'\n        random_state = <mtrand.RandomState object>\n    365 \n    366     # project M to the (k + p) dimensional space using the basis vectors\n    367     B = safe_sparse_dot(Q.T, M)\n    368 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/utils/extmath.py in randomized_range_finder(A=array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]]), size=3321, n_iter=3, power_iteration_normalizer='LU', random_state=<mtrand.RandomState object>)\n    253     for i in range(n_iter):\n    254         if power_iteration_normalizer == 'none':\n    255             Q = safe_sparse_dot(A, Q)\n    256             Q = safe_sparse_dot(A.T, Q)\n    257         elif power_iteration_normalizer == 'LU':\n--> 258             Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n        Q = array([[ 0.35356032,  0.48941691,  0.30663461, .... -0.42312625,\n         0.33898944, -0.36818682]])\n        _ = undefined\n        A = array([[          nan,           nan,           ...612318  ,\n        -170.65576607,   -9.224636  ]])\n    259             Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)\n    260         elif power_iteration_normalizer == 'QR':\n    261             Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n    262             Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/scipy/linalg/decomp_lu.py in lu(a=array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]]), permute_l=True, overwrite_a=False, check_finite=True)\n    173     -----\n    174     This is a LU factorization routine written for Scipy.\n    175 \n    176     \"\"\"\n    177     if check_finite:\n--> 178         a1 = asarray_chkfinite(a)\n        a1 = undefined\n        a = array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]])\n    179     else:\n    180         a1 = asarray(a)\n    181     if len(a1.shape) != 2:\n    182         raise ValueError('expected matrix')\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/numpy/lib/function_base.py in asarray_chkfinite(a=array([[ nan,  nan,  nan, ...,  nan,  nan,  nan]...      [ nan,  nan,  nan, ...,  nan,  nan,  nan]]), dtype=None, order=None)\n   1210 \n   1211     \"\"\"\n   1212     a = asarray(a, dtype=dtype, order=order)\n   1213     if a.dtype.char in typecodes['AllFloat'] and not np.isfinite(a).all():\n   1214         raise ValueError(\n-> 1215             \"array must not contain infs or NaNs\")\n   1216     return a\n   1217 \n   1218 \n   1219 def piecewise(x, condlist, funclist, *args, **kw):\n\nValueError: array must not contain infs or NaNs\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "            \n",
    "##run solver can be wrapped in a loop for different sets of classes (different sets of X and y), so that for a best model is found for each class. In this case an independets results array has to be passed for each class, otherwise eahcn new function call will rewrite the results from the previous simnulation.\n",
    "run_solver(X,y, preprocessors, transfomers, reducers, models, results, errors, errors_ind)\n",
    "print (\"##############################\")\n",
    "print (\"###Finished all estimators####\")\n",
    "print (\"##############################\")\n",
    "\n",
    "print (\"##############################\")\n",
    "print (\"######Printing all errors#####\")\n",
    "print (\"##############################\")\n",
    "print(errors)\n",
    "print (\"##############################\")\n",
    "print (\"######Printing errors summary#\")\n",
    "print (\"##############################\")\n",
    "print(errors_ind)\n",
    "print (\"##############################\")\n",
    "print (\"#######Printing results#######\")\n",
    "print (\"##############################\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sorted(results.items(), key=lambda x:x[1])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "type(AdaBoostRegressor()).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
