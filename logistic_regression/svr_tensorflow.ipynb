{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>blockbuster_month</th>\n",
       "      <th>cast_total_facebook_likes</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>dump_month</th>\n",
       "      <th>duration</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "      <th>...</th>\n",
       "      <th>raiting_pg</th>\n",
       "      <th>raiting_pg13</th>\n",
       "      <th>raiting_r</th>\n",
       "      <th>raiting_tv14</th>\n",
       "      <th>raiting_tvg</th>\n",
       "      <th>raiting_tvma</th>\n",
       "      <th>raiting_tvpg</th>\n",
       "      <th>raiting_tvy7</th>\n",
       "      <th>raiting_unrated</th>\n",
       "      <th>raiting_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "      <td>4812.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6801.628221</td>\n",
       "      <td>1720.181629</td>\n",
       "      <td>662.147548</td>\n",
       "      <td>0.368869</td>\n",
       "      <td>10068.043225</td>\n",
       "      <td>742.647756</td>\n",
       "      <td>0.219659</td>\n",
       "      <td>106.889443</td>\n",
       "      <td>6.417249</td>\n",
       "      <td>8067.969659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141521</td>\n",
       "      <td>0.297589</td>\n",
       "      <td>0.427265</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.011222</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15654.706536</td>\n",
       "      <td>4105.115266</td>\n",
       "      <td>1665.732147</td>\n",
       "      <td>0.482549</td>\n",
       "      <td>18756.607185</td>\n",
       "      <td>2934.558160</td>\n",
       "      <td>0.414059</td>\n",
       "      <td>21.994601</td>\n",
       "      <td>1.105504</td>\n",
       "      <td>20009.801260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348595</td>\n",
       "      <td>0.457245</td>\n",
       "      <td>0.494733</td>\n",
       "      <td>0.061053</td>\n",
       "      <td>0.038117</td>\n",
       "      <td>0.051912</td>\n",
       "      <td>0.043211</td>\n",
       "      <td>0.014416</td>\n",
       "      <td>0.105349</td>\n",
       "      <td>0.020385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>644.750000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1563.250000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>628.000000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3258.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12000.000000</td>\n",
       "      <td>942.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14618.750000</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>7000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>713000.000000</td>\n",
       "      <td>137000.000000</td>\n",
       "      <td>23000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>729779.000000</td>\n",
       "      <td>33000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>353000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actor_1_facebook_likes  actor_2_facebook_likes  actor_3_facebook_likes  \\\n",
       "count             4812.000000             4812.000000             4812.000000   \n",
       "mean              6801.628221             1720.181629              662.147548   \n",
       "std              15654.706536             4105.115266             1665.732147   \n",
       "min                  0.000000                0.000000                0.000000   \n",
       "25%                644.750000              310.000000              154.000000   \n",
       "50%               1000.000000              628.000000              389.000000   \n",
       "75%              12000.000000              942.000000              650.000000   \n",
       "max             713000.000000           137000.000000            23000.000000   \n",
       "\n",
       "       blockbuster_month  cast_total_facebook_likes  director_facebook_likes  \\\n",
       "count        4812.000000                4812.000000              4812.000000   \n",
       "mean            0.368869               10068.043225               742.647756   \n",
       "std             0.482549               18756.607185              2934.558160   \n",
       "min             0.000000                   0.000000                 0.000000   \n",
       "25%             0.000000                1563.250000                 8.000000   \n",
       "50%             0.000000                3258.000000                53.000000   \n",
       "75%             1.000000               14618.750000               212.000000   \n",
       "max             1.000000              729779.000000             33000.000000   \n",
       "\n",
       "        dump_month     duration   imdb_score  movie_facebook_likes  \\\n",
       "count  4812.000000  4812.000000  4812.000000           4812.000000   \n",
       "mean      0.219659   106.889443     6.417249           8067.969659   \n",
       "std       0.414059    21.994601     1.105504          20009.801260   \n",
       "min       0.000000     7.000000     1.600000              0.000000   \n",
       "25%       0.000000    93.000000     5.800000              0.000000   \n",
       "50%       0.000000   103.000000     6.500000            192.000000   \n",
       "75%       0.000000   118.000000     7.200000           7000.000000   \n",
       "max       1.000000   227.000000     9.600000         353000.000000   \n",
       "\n",
       "          ...        raiting_pg  raiting_pg13    raiting_r  raiting_tv14  \\\n",
       "count     ...       4812.000000   4812.000000  4812.000000   4812.000000   \n",
       "mean      ...          0.141521      0.297589     0.427265      0.003741   \n",
       "std       ...          0.348595      0.457245     0.494733      0.061053   \n",
       "min       ...          0.000000      0.000000     0.000000      0.000000   \n",
       "25%       ...          0.000000      0.000000     0.000000      0.000000   \n",
       "50%       ...          0.000000      0.000000     0.000000      0.000000   \n",
       "75%       ...          0.000000      1.000000     1.000000      0.000000   \n",
       "max       ...          1.000000      1.000000     1.000000      1.000000   \n",
       "\n",
       "       raiting_tvg  raiting_tvma  raiting_tvpg  raiting_tvy7  raiting_unrated  \\\n",
       "count  4812.000000   4812.000000   4812.000000   4812.000000      4812.000000   \n",
       "mean      0.001455      0.002702      0.001870      0.000208         0.011222   \n",
       "std       0.038117      0.051912      0.043211      0.014416         0.105349   \n",
       "min       0.000000      0.000000      0.000000      0.000000         0.000000   \n",
       "25%       0.000000      0.000000      0.000000      0.000000         0.000000   \n",
       "50%       0.000000      0.000000      0.000000      0.000000         0.000000   \n",
       "75%       0.000000      0.000000      0.000000      0.000000         0.000000   \n",
       "max       1.000000      1.000000      1.000000      1.000000         1.000000   \n",
       "\n",
       "         raiting_x  \n",
       "count  4812.000000  \n",
       "mean      0.000416  \n",
       "std       0.020385  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 139 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import all helpers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, RFE\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns # More snazzy plotting library\n",
    "import itertools\n",
    "from itertools import  product\n",
    "\n",
    "#import regressors\n",
    "#-----Ensemble---------------------\n",
    "from sklearn.ensemble import       AdaBoostRegressor\n",
    "from sklearn.ensemble import       BaggingRegressor\n",
    "from sklearn.ensemble import       ExtraTreesRegressor\n",
    "from sklearn.ensemble import       GradientBoostingRegressor\n",
    "from sklearn.ensemble import       RandomForestRegressor\n",
    "\n",
    "#----Generalized Linear models-----\n",
    "from sklearn.linear_model import   ARDRegression\n",
    "from sklearn.linear_model import   BayesianRidge\n",
    "from sklearn.linear_model import   ElasticNet\n",
    "from sklearn.linear_model import   HuberRegressor\n",
    "from sklearn.linear_model import   Lars\n",
    "from sklearn.linear_model import   Lasso\n",
    "from sklearn.linear_model import   LassoLars\n",
    "from sklearn.linear_model import   LinearRegression\n",
    "from sklearn.linear_model import   PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import   Ridge\n",
    "from sklearn.linear_model import   SGDRegressor\n",
    "from sklearn.linear_model import   OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import   RANSACRegressor\n",
    "from sklearn.linear_model import   TheilSenRegressor\n",
    "\n",
    "#---Nearest Neighbors----\n",
    "from sklearn.neighbors import      KNeighborsRegressor\n",
    "from sklearn.neighbors import      RadiusNeighborsRegressor\n",
    "\n",
    "\n",
    "#----Neural Networks--------------- \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#-----Support Vector Machines------\n",
    "from sklearn.svm import            SVR\n",
    "from sklearn.svm import            LinearSVR\n",
    "from sklearn.svm import            NuSVR\n",
    "\n",
    "#-----Decission Trees--------------\n",
    "from sklearn.tree import           DecisionTreeRegressor\n",
    "from sklearn.tree import           ExtraTreeRegressor\n",
    "\n",
    "#----extras\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.isotonic import         IsotonicRegression\n",
    "from sklearn.kernel_ridge import     KernelRidge\n",
    "\n",
    "\n",
    "\n",
    "#file_path =  \"../dataset/movie_metadata_cleaned_tfidf_num_only_min.csv\"\n",
    "file_path =  \"../dataset/movie_metadata_cleaned_categ_num_only.csv\"\n",
    "#file_path = \"../dataset/movie_metadata_cleaned_no_vector_num_only.csv\"\n",
    "\n",
    "dta = pd.read_csv(file_path)\n",
    "dta_clean = dta\n",
    "#remove the null values, that is fill NaN with there - FIXME: Rihards, naive implementation\n",
    "dta_clean = dta_clean.fillna(value=0, axis=1)\n",
    "dta_clean = dta_clean.dropna()\n",
    "dta_clean = dta_clean.drop('Unnamed: 0', axis=1)\n",
    "dta_clean.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clasify the data for the logistic regression\n",
    "def label_gross (gross):\n",
    "    if (gross < 1000000) : return 1\n",
    "    elif ((gross >= 1000000) & (gross < 10000000)) : return 2\n",
    "    elif ((gross >= 10000000) & (gross < 50000000)) : return 3\n",
    "    elif ((gross >= 50000000) & (gross < 200000000)) : return 4\n",
    "    elif (gross >= 200000000) : return 5\n",
    "    \n",
    "#y = dta_clean.worldwide_gross.apply (lambda gross: label_gross (gross))\n",
    "y = dta_clean['worldwide_gross']\n",
    "X = dta_clean.drop('worldwide_gross', axis=1)\n",
    "\n",
    "X_col = X.columns\n",
    "if False:\n",
    "    nomalizer_scaler = preprocessing.Normalizer().fit(X)\n",
    "    X = nomalizer_scaler.transform(X)\n",
    "if False:\n",
    "    standard_scaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = standard_scaler.transform(X)\n",
    "        \n",
    "#pd.DataFrame(X, columns=X_col).describe()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "#model = SVR(kernel='rbf', verbose=2)\n",
    "\n",
    "#model = LinearRegression()\n",
    "#model = LogisticRegression()\n",
    "\n",
    "#model = DecisionTreeRegressor()\n",
    "#model = DecisionTreeClassifier()\n",
    "\n",
    "#model = BayesianRidge()\n",
    "#model = RidgeClassifier()\n",
    "\n",
    "#model = KNeighborsRegressor()\n",
    "#model = KNeighborsClassifier()\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "#from sklearn.linear_model import   ARDRegression\n",
    "#model = GradientBoostingClassifier()\n",
    "\n",
    "#model = AdaBoostRegressor()\n",
    "#model = AdaBoostClassifier()\n",
    "\n",
    "#model = LinearSVR()\n",
    "#model = LinearSVC()\n",
    "\n",
    "#model = SVR(kernel='rbf')\n",
    "#model = SVC(kernel='rbf')\n",
    "\n",
    "#model = MLPRegressor(solver='lbfgs',  max_iter=1000)\n",
    "#model = MLPClassifier(solver='lbfgs',  max_iter=1000)\n",
    "#model = ExtraTreesRegressor()\n",
    "#model.fit(X_train, y_train)\n",
    "# check the accuracy on the training set\n",
    "#model.predict(X_test)\n",
    "#LR_1.score(X_1, y_1)\n",
    "#model.score(X_test, y_test)\n",
    "#score_re = {}\n",
    "#scorers = ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\n",
    "#for scoring in scorers:\n",
    "#    try:\n",
    "#        print(scoring)\n",
    "#        score_re[scoring] = cross_val_score(model, X, y, scoring=scoring)\n",
    "#    except (ValueError, AttributeError): \n",
    "#        continue\n",
    "#model = GradientBoostingRegressor()\n",
    "#X.describe()\n",
    "#X, _X_dummy, y, _y_dummy = train_test_split(X, y, test_size=0)\n",
    "#model.coef_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] second__degree=2 ................................................\n",
      "[CV] second__degree=2 ................................................\n",
      "[CV] second__degree=2 ................................................\n",
      "[CV] second__degree=2 ................................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-01bf4b7def8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#pd.DataFrame(pipe.fit_transform(X,y)).describe()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond__degree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer,PolynomialFeatures\n",
    "def dummy(X):   return X\n",
    "DummyTransformer = FunctionTransformer(dummy)\n",
    "LogarithmicFeatures = FunctionTransformer(np.log1p)\n",
    "#X = DummyTransformer.fit_transform(X,y)\n",
    "#X = preprocessing.Normalizer().fit_transform(X)\n",
    "first  = DummyTransformer\n",
    "second = PolynomialFeatures()\n",
    "#first  = DummyTransformer\n",
    "#second = DummyTransformer\n",
    "model = GradientBoostingRegressor()\n",
    "pipe = Pipeline(steps=[('first', first), ('second', second), ('model', model)])\n",
    "#pd.DataFrame(pipe.fit_transform(X,y)).describe()\n",
    "est = GridSearchCV(pipe, dict(second__degree = [2]), verbose=2, cv=5, n_jobs=4)\n",
    "est.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#est.best_score_\n",
    "transform = dict(transform__das = [40,2],\n",
    "                 transform__fa = [\"aa\", \"asb\"]\n",
    "                )\n",
    "model = dict(model__degree = [1,2],\n",
    "             model__sumthin = [\"a\", \"b\"]\n",
    "            )\n",
    "print (n_samples)\n",
    "print (n_features)\n",
    "\n",
    "def poly(X, pw):\n",
    "    res = X\n",
    "    for power in range(2,pw + 1):\n",
    "        res = np.concatenate((res, np.power(X, power)), axis=1)\n",
    "    return res\n",
    "\n",
    "def log(X):\n",
    "    df_t = pd.DataFrame(X)\n",
    "    X_t = df_t.replace(0, 1/math.e) \n",
    "    return np.concatenate((X, np.log(X_t)), axis=1)\n",
    "\n",
    "PolyFeatures = FunctionTransformer(poly, kw_args = {\"pw\": 24})\n",
    "frame = pd.DataFrame(PolyFeatures.fit_transform(X))\n",
    "frame \n",
    "print(*np.isfinite(frame).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f654a44bb70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAFcCAYAAAA9LkIhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtcVHX+x/HXAGHeQDGZ0TRbL1201LaLoKYJDd5CkMDN\nba38lZa6mVtZpmWW5m5lZuVve6y528V+eReoLG9jiv6EfDxy1S0v3XSVhEFRQahEhvP7w5+zInNg\nEGYG8P18PNxtvucyn3My3nzP95zvsRiGYSAiIuJBUKALEBGRukshISIiphQSIiJiSiEhIiKmFBIi\nImJKISEiIqZ8GhIxMTHEx8eTkJBAUlISACdPnmT06NHExcUxevRoCgoKADAMg1mzZmG324mPj+eb\nb75x7yc1NZW4uDji4uJITU2t9DtLS0vJzs6mtLTUdwcmInKJ8HlP4v333yc9PZ1Vq1YBsGDBAqKj\no1m3bh3R0dEsWLAAgIyMDA4ePMi6deuYOXMmM2bMAM6Gyvz581m2bBnLly9n/vz57mDxJDc3l9jY\nWHJzc319aCIiDZ7fLzc5HA4SExMBSExMZMOGDeXaLRYLPXv2pLCwkLy8PLZu3UqfPn1o0aIF4eHh\n9OnThy1btvi7bBGRS5LPQ+LBBx8kKSmJpUuXApCfn09kZCQArVu3Jj8/HwCn04nNZnNvZ7PZcDqd\nFdqtVitOp9PXZYuICBDiy50vXrwYq9VKfn4+o0ePpmPHjuWWWywWLBaLL0sQEZEa8GlPwmq1AtCq\nVSvsdju7d++mVatW5OXlAZCXl0dERIR73fPHEXJzc7FarRXanU6ne78iIuJbPguJn3/+maKiIvc/\n/+///i9dunQhJiaGtLQ0ANLS0oiNjQVwtxuGwc6dO2nevDmRkZH07duXrVu3UlBQQEFBAVu3bqVv\n376+KltERM7js8tN+fn5TJgwAQCXy8Vdd91Fv379uPHGG5k0aRIrVqygbdu2zJs3D4D+/fuzefNm\n7HY7jRs3Zvbs2QC0aNGC8ePHk5ycDMCECRNo0aKFr8oWEZHzWBraVOHZ2dnExsbicDho165doMsR\nEanX9MS1iIiYUkiIiIgpn94CWx89NncTP/70nye6O14ZzhuP3xG4gkREAkg9ifNcGBAAP/5UwGNz\nNwWmIBGRAFNInOfCgKiqXUSkoVNIiIiIKYWEiIiYUkiIiIgphYSIiJhSSIiIiCmFhIiImFJIiIiI\nKYWEiIiYUkiIiIgphYSIiJhSSIiIiCmFhIiImFJIiIiIKYWEiIiYUkiIiIgphYSIiJhSSIiIiCmF\nhIiImFJIiIiIKYWEiIiYUkiIiIgphYSIiJhSSIiIiCmFhIiImFJIiIiIKYWEiIiYUkiIiIgphYSI\niJhSSIiIiCmFhIiImFJIiIiIKYWEiIiYUkiIiIgphYSIiJhSSIiIiCmFhIiImFJIiIiIKYWEiIiY\nUkiIiIgphYSIiJhSSIiIiCmFhIiImPJ5SLhcLhITE3n44YcBOHz4MCkpKdjtdiZNmkRJSQkAJSUl\nTJo0CbvdTkpKCtnZ2e59/O1vf8NutzNw4EC2bNni65JFROT/+TwkPvjgAzp16uT+PGfOHB544AHW\nr19PWFgYK1asAGD58uWEhYWxfv16HnjgAebMmQPA999/z+rVq1m9ejULFy7khRdewOVy+bpsERHB\nxyGRm5vLpk2bSE5OBsAwDLKyshg4cCAAw4cPx+FwALBx40aGDx8OwMCBA8nMzMQwDBwOB0OHDiU0\nNJT27dvToUMHdu/e7cuyRUTk//k0JGbPns3kyZMJCjr7NSdOnCAsLIyQkBAAbDYbTqcTAKfTSZs2\nbQAICQmhefPmnDhxAqfTic1mc+/TarW6txEREd/yWUh88cUXREREcMMNN/jqK0RExMdCfLXjHTt2\nsHHjRjIyMjh9+jRFRUW89NJLFBYWUlpaSkhICLm5uVitVuBsDyEnJwebzUZpaSmnTp2iZcuWWK1W\ncnNz3ft1Op3ubURExLd81pN44oknyMjIYOPGjcydO5eoqChee+01evXqxdq1awFITU0lJiYGgJiY\nGFJTUwFYu3YtUVFRWCwWYmJiWL16NSUlJRw+fJiDBw/SvXt3X5UtIiLn8ftzEpMnT+bdd9/Fbrdz\n8uRJUlJSAEhOTubkyZPY7XbeffddnnzySQC6dOnC4MGDGTJkCA899BDTp08nODjY32WLiFySLIZh\nGIEuojZlZ2cTGxuLw+GgXbt21do2/ol002WfvJZQ09JEROodPXEtIiKmFBIiImJKISEiIqYUEiIi\nYkohISIiphQSIiJiSiEhIiKmFBIiImJKISEiIqYUEiIiYkohISIiphQSIiJiSiEhIiKmFBIiImJK\nISEiIqYUEiIiYkohISIiphQSIiJiSiEhIiKmFBIiImJKISEiIqYUEiIiYkohISIiphQSIiJiSiEh\nIiKmFBIiImJKISEiIqYUEiIiYkohISIiphQSIiJiSiHhpWWObwNdgoiI3ykkvLTos72BLkFExO8U\nEiIiYkohISIiphQSIiJiSiFxnk9eSwh0CSIidYpCQkRETCkkRETElEJCRERMKSRERMSUQkJEREwp\nJERExJRCQkRETCkkRETElEJCRERMKSRERMSUQkJEREwpJERExJTPQuL06dMkJyczbNgwhg4dyptv\nvgnA4cOHSUlJwW63M2nSJEpKSgAoKSlh0qRJ2O12UlJSyM7Odu/rb3/7G3a7nYEDB7JlyxZflSwi\nIhfwWUiEhoby/vvv8/HHH5OWlsaWLVvYuXMnc+bM4YEHHmD9+vWEhYWxYsUKAJYvX05YWBjr16/n\ngQceYM6cOQB8//33rF69mtWrV7Nw4UJeeOEFXC6Xr8oWEZHz+CwkLBYLTZs2BaC0tJTS0lIsFgtZ\nWVkMHDgQgOHDh+NwOADYuHEjw4cPB2DgwIFkZmZiGAYOh4OhQ4cSGhpK+/bt6dChA7t37/ZV2SIi\nch6fjkm4XC4SEhLo3bs3vXv3pn379oSFhRESEgKAzWbD6XQC4HQ6adOmDQAhISE0b96cEydO4HQ6\nsdls7n1arVb3NiIi4ls+DYng4GDS09PZvHkzu3fv5scff/Tl14mISC3zy91NYWFh9OrVi507d1JY\nWEhpaSkAubm5WK1W4GwPIScnBzh7eerUqVO0bNkSq9VKbm6ue19Op9O9jYiI+JbPQuL48eMUFhYC\n8Ouvv7Jt2zY6depEr169WLt2LQCpqanExMQAEBMTQ2pqKgBr164lKioKi8VCTEwMq1evpqSkhMOH\nD3Pw4EG6d+/uq7JFROQ8IZUtTE9P5/PPP3f/ht+mTRsGDRpEQkICFoul0h3n5eUxZcoUXC4XhmEw\naNAgBgwYQOfOnfnTn/7EvHnzuP7660lJSQEgOTmZyZMnY7fbCQ8P5/XXXwegS5cuDB48mCFDhhAc\nHMz06dMJDg6ujWMXEZEqWAzDMDwtmDVrFgcOHGDEiBHuAeWcnByWLVtGhw4dmD59ul8L9VZ2djax\nsbE4HA7atWtX7e3jn0g3XfbJawk1KU1EpN4x7UlkZGSwbt26cm3du3fHbrczaNAgnxcmIiKBV+mY\nxMmTJyu0FRQUYNL5EBGRBsa0J/HQQw+RkJCA3W6nbdu2APz00084HA7Gjx/vtwJFRCRwTENixIgR\n9O7dm7Vr13LkyBEA2rZty6JFi2jfvr3fChQRkcCp9O6mdu3a8eCDD/qrFhERqWM0VXg17NifF+gS\nRET8SiFRDc8vyAx0CSIifqWQEBERUwoJEREx5VVIPPzww5V+FhGRhsmrkHj00Ucr/dyQaOoNEZH/\n8Cokbrjhhko/i4hIw2T6nMQrr7xS6YZPPfVUrRcjIiJ1i2lPokmTJjRp0oRjx47x+eefu99TvWbN\nGvLz8/1Zo4iIBIhpT+KPf/wjAPfddx+rVq2iZcuWAIwbN47HHnvMP9WJiEhAVTkmcezYMXdAALRs\n2ZJjx475tCgREakbKp27CaBz585MmzaN5ORkAFatWkXnzp19XpiIiARelT2J2bNn07x5c2bOnMnM\nmTNp1qwZs2fP9kdtIiISYFX2JJo1a8aUKVP8UYuIiNQxVfYk8vPzefLJJ7n33nsB2LdvH4sXL/Z5\nYSIiEnhVhsSzzz7LzTffTGFhIQAdO3bko48+8nlhIiISeFWGhNPpZOTIkQQHBwMQGhpKUJDmBRQR\nuRRU+dM+JKT8sEVhYSGGYfisIBERqTuqHLi22+1Mnz6d4uJiVq1axUcffcTdd9/tj9pERCTAqgyJ\nMWPG8PHHH1NYWMjmzZsZNWoUCQmaKVVE5FJQZUgADBs2jGHDhvm6FhERqWOqDIn8/Hw+/PBDDh06\nRGlpqbv9jTfe8GlhIiKXin79+pGRkVGtbTZs2MC1115L+/btfVTVWVWGxPjx4+natSvR0dHuO5xE\nRCSwNmzYQJMmTbwOCZfLdVE/w6sMiV9++YXnn3++2jtuqOKfSNfb60SkRt588022bt1Ko0aNGDJk\niLv9rbfewmazkZKSwr///W+effZZFi1axKJFi0hPT6dx48Z069aN4cOHs2XLFvbs2UN4eDiLFi1i\n/fr1vPvuuwQFBdGxY0dmzJjBkSNHmDBhAl27diU/P5+xY8fy8ssvc/nllxMUFMT7779fZa1VhkSP\nHj3Yv38/1157bc3OioiIkJGRwd69e1myZAlBQUG4XC7efvvtSrdZuXIl7777Li1btqSsrIygoCBu\nv/12hg0bRu/evSkoKOCvf/0rS5YsoVGjRsyePZv169fTrVs3fvrpJ9577z1atmzJrFmzePjhh7nz\nzjspKyvzqt4qQ+Kee+7hD3/4AzabjUaNGrnbV6xY4dUXiIjIf3z77bf07t3b/VDy+ZeALBaL+5/P\nfx7t+eef59VXX+X06dMMHjyYO++8s9w+//3vf+N0OnnooYcAKC4u5sorr6Rbt2506tTJ/bqHMWPG\n8M477/D5559z7bXXMmbMmHLf6UmVITF58mQeeeQRunbtqjEJEZEa6tKlC8uWLePee+919yTOCQ8P\n58iRIwB888037vbrrruO2bNn8+uvvzJgwADuvPNOLrvsMvfNRFdddRVXXnklf//73wkNDQWgpKSE\nvLy8cjNkhIWF8eyzzwJnXygXFRVF9+7dK623ypBo1KgRDz74oLfH3yB88loC8U+kB7oMEWmA+vfv\nz44dOxgxYgRNmjRh8ODB7mWDBw/mkUce4V//+hfXXHONu/3pp5/m+PHjlJSUuCdbjYmJ4e2332bF\nihW8+eabPPLII/zXf/0XFouFoKAgJk+eTIsWLcp993vvvcfWrVsBiIiI8GoYwWJUMcfG3LlzueWW\nW+jXr5/3ZyGAsrOziY2NxeFw0K5du4veT2UhoYFrEblUVNmTWLZsGQsWLKBp06aEhoZiGAYWi4XM\nzEx/1CciIgFUZUisXLnSH3WIiEgdVGVIXHnllf6oQ0RE6qAqQyInJ4dXX32Vffv2cfr0aXe7w+Hw\naWEiIhJ4Vb5PYurUqURHR2MYBnPmzOHmm29m+PDh/qhNREQCrMqQOHHiBCkpKYSEhHDTTTfxl7/8\nhc2bN/ujNhERCbAqQ+Kyyy4DoEmTJhw5coTS0lKOHz/u88JERKR2PPPMM0RHR3PXXXdVe9sqQ+KW\nW27h5MmTjBw5kqSkJO68805iYmIuqlAREfG/pKQkFi5ceFHbVjlw/fTTTwOQmJjIbbfdRlFRUbkn\nAUVEpPbs2J/Hhu2HyM0vxtaqKXfedhW/vTayRvu89dZbyc7Ovqhtq+xJwNnpwg8cOMDPP/9MUFAQ\n33///UV9mYiImNuxP49Fn+0h51gRhmGQc6yIRZ/tYcf+vIDVVGVP4n/+53+YM2cOLVq0cM8WaLFY\ndAusiEgt27D9kMd2x/ZDNe5NXKwqQ+If//gHn376qR6qO0/KM5+y/M/VHwASEalMbn6x5/bjntv9\nocrLTa1bt1ZAXODXElfVK4mIVJOtVVPP7RGe2/2hypDo3bs3r7zyCt988w3ff/+9+09VcnJyGDVq\nFEOGDGHo0KHu1+SdPHmS0aNHExcXx+jRoykoKADOvmBj1qxZ2O124uPjy82lnpqaSlxcHHFxcaSm\npl7ssYqI1Gl33naVx/ZYk3ZvPf7449xzzz0cOHCAfv36sXz5cq+3rfJyU1paGgBr1qxxt3kzJhEc\nHMyUKVPo1q0bRUVF3H333fTp04dVq1YRHR3N2LFjWbBgAQsWLGDy5MlkZGRw8OBB1q1bx65du5gx\nYwbLly/n5MmTzJ8/n5UrV2KxWEhKSiImJobw8HCvD1JEpD44N+7g2H6I3OPF2CKaElsLdzfNnTv3\noretMiQ2btx4UTuOjIwkMvLsgTVr1oyOHTvidDpxOBwsWrQIOHtb7ahRo5g8eTIOh4PExEQsFgs9\ne/aksLCQvLw8tm/fTp8+fdwvz+jTpw9btmy5qIdCquPy0GBdVhIRv/vttZEBG6T2pFq3wFbnctP5\nsrOz2bt3Lz169CA/P98dHq1btyY/Px8Ap9OJzWZzb2Oz2XA6nRXarVYrTqezWt9/MTQwLSJSjVtg\nw8PD3e9Krc4tsMXFxUycOJGpU6fSrFmzcsssFkuVL+EWEZHA8ektsGfOnGHixInEx8cTFxcHQKtW\nrcjLyyMyMpK8vDwiIiKAsz2E3Nxc97a5ublYrVasVivbt293tzudTm677bZq1yIiItXns1tgDcNg\n2rRpdOzYkdGjR7vbY2Ji3IPhaWlpxMbGlms3DIOdO3fSvHlzIiMj6du3L1u3bqWgoICCggK2bt1K\n3759q12PiIhUX5U9iXO3wA4dOpRGjRq52zt37lzpdl999RXp6elcc801JCQkAGdvwxo7diyTJk1i\nxYoVtG3blnnz5gHQv39/Nm/ejN1up3HjxsyePRuAFi1aMH78eJKTkwGYMGGCexBbRER8y2IYhlHZ\nCp5mfK3L03JkZ2cTGxuLw+GgXbt2NdpX/BPppss+eS2hRvsWEfGXnJwcnnrqKfLz87FYLIwYMYL7\n77/fq219dgusiIjUDWbPrVV1RQgqCYkjR46U+2yxWIiIiCh3yUlERGrXrtw9fPHjNpzFx7A2vYIB\nHXvTw9a1Rvs0e26tRiGRlJSExWLh/KtRRUVF9OzZk1deeYW2bdvWqOj6bszs9bwz1R7oMkSkAdmV\nu4fFu/9zmTu36Kj7c02D4pzzn1vzhmlIZGVlVWhzuVwsWbKEmTNn8vbbb198lQ1Abv7PgS5BRBqY\nL37c5rn9QGathERlz62Z8eqJ63OCg4O59957yz3PICIitcNZfMxje16R5/bq8PTcmjeqFRLnuFya\n00hEpLZZm17hsT2ymed2b5k9t+YN05D45ZdfKvzJycnh9ddfp0uXLjUquL4IDtKUISLiPwM69vbc\n/pvoGu333HNrWVlZJCQkkJCQwObNm73a1nRM4qabbio3cH3u7qbevXszbdq0GhVcX6S9OqzSZyVE\nRGrTuXGHLw5kkld0jMhmVzDgN9E1Ho+45ZZb2L9//0VtaxoS+/btu+iCRETk4vSwda21O5lqw0WN\nSYiIyKVBISEiIqYUEiIiYkohISIiphQSNfDY3E2BLkFExKcUEjXw408FgS5BRMSnFBIiImJKISEi\nIqYUElXQzBwicilTSFQhfY5eUyoily6FhIiImFJIiIiIKYWEiIiYUkjU0MhnPwt0CSIiPqOQqKGi\nX84EugQREZ9RSIiIiCmFhIiImFJIeOHy0OBAlyAiEhAKCS8s//NdgS5BRCQgFBIiImJKISEiIqYU\nErUg5ZlPA12CiIhPKCRqwa8lrkCXICLiEwoJERExpZAQERFTCgkvNWt8WaBLEBHxO4WElxbPGhLo\nEkRE/E4hUUsSJ38c6BJERGqdQqKWuMqMQJcgIlLrFBIiImJKISEiIqYUEtVga9Uk0CWIiPiVQqIa\n3plqD3QJIiJ+pZAQERFTColaFP9EeqBLEBGpVQoJERExpZAQERFTCgkRETHls5B45plniI6O5q67\n/vN+6JMnTzJ69Gji4uIYPXo0BQUFABiGwaxZs7Db7cTHx/PNN9+4t0lNTSUuLo64uDhSU1N9Va7X\nRg25vtLlwzQuISINiM9CIikpiYULF5ZrW7BgAdHR0axbt47o6GgWLFgAQEZGBgcPHmTdunXMnDmT\nGTNmAGdDZf78+Sxbtozly5czf/58d7AEyojYaypdrsk5RKQh8VlI3HrrrYSHh5drczgcJCYmApCY\nmMiGDRvKtVssFnr27ElhYSF5eXls3bqVPn360KJFC8LDw+nTpw9btmzxVckiInIBv45J5OfnExkZ\nCUDr1q3Jz88HwOl0YrPZ3OvZbDacTmeFdqvVitPp9GfJIiKXtIANXFssFiwWS6C+vkY6Xhle6XKN\nS4hIQ+HXkGjVqhV5eXkA5OXlERERAZztIeTm5rrXy83NxWq1Vmh3Op1YrVZ/luzRG4/fUelyjUuI\nSEPh15CIiYkhLS0NgLS0NGJjY8u1G4bBzp07ad68OZGRkfTt25etW7dSUFBAQUEBW7dupW/fvv4s\nWUTkkhbiqx0//vjjbN++nRMnTtCvXz8effRRxo4dy6RJk1ixYgVt27Zl3rx5APTv35/Nmzdjt9tp\n3Lgxs2fPBqBFixaMHz+e5ORkACZMmECLFi18VXKtWub4tso7oURE6jqLYRgN6upIdnY2sbGxOBwO\n2rVr57PveWzuJn78qfLbcT95LcFn3y8i4g964voiVTUuISLSECgkfGiZ49tAlyAiUiMKCR9a9Nne\nQJcgIlIjCoka0JiDiDR0CgkRETGlkPAxjUuISH2mkKihqi45aVxCROozhYSIiJhSSIiIiCmFhB/c\nN2NNoEsQEbkoColaUNW4xIlTp/1UiYhI7VJIiIiIKYWEnyQ8qRcRiUj9o5CoJZeFVH4qyxrUXLsi\ncqnw2fskLjWrXo4n/rzXlobevIag83KjrAyGPQEfayoPEalH1JPwgQsDAiAoCC67WXc5iUj9opCo\nRR2vDAeoEBDnBAXBiKXj/FiRiEjNKCRqkV5EJCINjUKilp3rTVTmr19+4IdKRERqTiFRy954/A4w\nuZPJ8v//v+lgpr/KERGpEYWED/Roc707EM6xuP/nLPUmRKQ+UEj4wLN3TAQLWM77c2FqqDchIvWB\nQiKAdKeTiNR1CgkfWfa7t71a73dLx/u4EhGRi6eQCDADg7FpUwJdhoiIRwoJH/K2N3HydIGPKxER\nuTgKCR+b1v9Rr9bT+ISI1EUKCR/rYeuKpcINsZ4pKESkrlFI+MHS3/3V63XvWTqhQtuu3D3M27aQ\nZ9b/hXnbFrIrd09tliciYkoh4Sfejk+UUcajn053f96Vu4fFu9PJLTqKYRjkFh1l8e50BYWI+IVC\nwo+6W6/3aj1n8VH3E9lf/LjN4zpfHNDDeCLie3rpkB89e8dEfrd0PIbZ5E7n2XQwk3+fzCbIZN7x\nvKJjtV2eiEgF6kn4WXXGJw6cPMwPx//tcVlksytqqyQREVMKiQDwdnzinB9PHKrQNuA30bVVjoiI\nKYVEgFxMUARZgrA1j2Rk9wR62Lr6qDIRkf/QmEQt2bE/jw3bD5GbX4ytVVPuvO0qfnttZKXbLPvd\n29V6NuLAicPMtj9d01JFRLymnkQt2LE/j0Wf7SHnWBGGYZBzrIhFn+1hx/68KretTo/CZbgYsXQc\no1c9UZNyRUS8ppCoBRu2VxwzAHCYtF+oupeeis/8zIil4xixdFy5ZypERGqbQqIW5OYXe24/7rnd\nk+oGxTnO4qOMWDqOe5aO1wN2IlLrFBK1wNaqqef2CM/tZi42KADKMHhp81uMWDpOU4+LSK3RwHUt\nuPO2q1j0WcXf4mNvu6ra+6ruYLYnJ08XlNtHk8sa86feD+mOKBGpNothGFU//luPZGdnExsbi8Ph\noF27dn773h3783BsP0Tu8WJsEU2J9eLupsr4ckbYEEswKTfcxfCug3z2HSLSMCgk6rBZm95kt3Ov\nz7/HgoV7bhym0BCRChQS9cToVU9QfOZnv31fEEFc17oTw7sO0mUqkUuYxiTqiXeTXmPetoXkFh3l\n4Ilsyijz6feVUcaeo9+xZ/N3HpffcXU043vd59MaRCTwFBL1iLP47MyvV7c820M6XJDDmbIzAall\n08FMNh30PF15kCWIdmE2RvW8W70QkXpOIVGPWJteQW7RUffn9uFtALg8pBHHfj5OXnF+oEorp8wo\n41DBEV7a/BZwdswjvFEYBacLy02THhIUTN+rblOPRKQOU0jUIwM69mbx7vQK7Z7GDVL3rGHJvz72\n6t0VvmZgcPJ0QYX20jJXpT0SAAtwWfBlBFuCOFPmwlXmwmKx0Cg4lBaNw/hNi/YM6NhbPRYRH6k3\nA9cZGRm89NJLlJWVkZKSwtixYz2u11AHrs/ZlbuHLw5kkld0jMhmVzDgN9Fe/YCctelN/uXcVydC\nozZZsGCxWNyXt+Ds2/ycxcewNr2CAR17e2zrYet69lxWo92T6qxbG9vVJQ3hGC7UEI+ppupFSLhc\nLgYOHMi7776L1WolOTmZuXPn0rlz5wrrNvSQqC2pe9aQvm8dP5/5JdCl1Ni5oAhv1JxGIaE0uayx\ne9nPZ34FjHJtAL3a3cSX2f+ssC+zdk/Ts597/7g369bGdnVJQziGCzXEY6oN9eJy0+7du+nQoQPt\n27cHYOjQoTgcDo8hId4Z3nWQx+ciduXuYX7W+xScLgxAVRfPMAwKTxdxuatRuUA4dboIoEJIrP8h\ng7BGzSvsx6z9iwOZFX5QVPb+8cp+qFzsdnVJQziGCzXEY6oN9SIknE4nNpvN/dlqtbJ79+4AVtRw\n9bB15Z3Elz0uS92zhhXfrOZMWamfq6qaxWKhzCij9ILaLvx8zqnTxR7DwKzd0zvFz91t5s26tbFd\nXdIQjuFCDfGYakO9CAmpG8x6H+fsyt3Dop2rOFxwpNzYh9ndTbUpyBKEgUFIUPm/0hd+Pqd5I8+T\nL5q1e3qn+IV3m1W2bm1sV5c0hGO4UEM8ptpQL0LCarWSm5vr/ux0OrFarQGsSDzpYetKj0HeDdou\n2rmK7MIZqmvZAAAM9ElEQVQcyozKHwo8e3dTKMEWy3l3N0HZ/w+lBVmC3H+ahjamUUhoue2bN2oG\nHoLJ3qmfx7EHs3ZP7xQ3u9usqvePX+x2dUlDOIYLNcRjqg31IiRuvPFGDh48yOHDh7FaraxevZrX\nXnst0GXJRfI2TCqzK3cPqXvWcKjgCABXhbd193IuvPvLU1sPW1c6RlxVrXZPx2G276qO/2K2q0sa\nwjFcqCEeU22oF3c3AWzevJnZs2fjcrm4++67GTfO8yypurtJRKT21IueBED//v3p379/oMsQEbmk\n6M10IiJiSiEhIiKmFBIiImJKISEiIqYUEiIiYkohISIiphQSIiJiSiEhIiKmFBIiImJKISEiIqbq\nzbQc3nK5XADlZo0VEQkEm81GSEj9/jFbv6v34OjRs/PB33vvvQGuREQudQ1hotF6Mwust3799Ve+\n/vprWrduTXBwcKDLEZFLWEPoSTS4kBARkdqjgWsRETGlkBAREVMKCRERMaWQEBERU5d0SGRkZDBw\n4EDsdjsLFiyosLykpIRJkyZht9tJSUkhOzs7AFV6VlXtq1atIioqioSEBBISEli+fHkAqizvmWee\nITo6mrvuusvjcsMwmDVrFna7nfj4eL755hs/V+hZVXV/+eWX3Hzzze5zPX/+fD9X6FlOTg6jRo1i\nyJAhDB06lPfff7/COnXxnHtTd1095w2ScYkqLS01YmNjjUOHDhmnT5824uPjje+++67cOh9++KHx\n3HPPGYZhGJ9++qnx2GOPBaLUCrypfeXKlcYLL7wQoAo92759u/H1118bQ4cO9bh806ZNxoMPPmiU\nlZUZ//znP43k5GQ/V+hZVXVnZWUZY8eO9XNVVXM6ncbXX39tGIZhnDp1yoiLi6vw96QunnNv6q6r\n57whumR7Ert376ZDhw60b9+e0NBQhg4disPhKLfOxo0bGT58OAADBw4kMzMTow7cMexN7XXRrbfe\nSnh4uOlyh8NBYmIiFouFnj17UlhYSF5enh8r9KyquuuqyMhIunXrBkCzZs3o2LEjTqez3Dp18Zx7\nU7f4zyUbEk6nE5vN5v5stVor/EV0Op20adMGgJCQEJo3b86JEyf8Wqcn3tQOsG7dOuLj45k4cSI5\nOTn+LPGiXHhcNput3vxw2LlzJ8OGDeOhhx7iu+++C3Q5FWRnZ7N371569OhRrr2un3OzuqHun/OG\n4pINiYZuwIABbNy4kU8++YTevXvz9NNPB7qkBqtbt25s3LiRjz/+mFGjRjFhwoRAl1ROcXExEydO\nZOrUqTRr1izQ5Xitsrrr+jlvSC7ZkLBareUmAXQ6nVit1grrnPsNvLS0lFOnTtGyZUu/1umJN7W3\nbNmS0NBQAFJSUurEgGRVLjyu3NzcCsdVFzVr1oymTZsC0L9/f0pLSzl+/HiAqzrrzJkzTJw4kfj4\neOLi4iosr6vnvKq66/I5b2gu2ZC48cYbOXjwIIcPH6akpITVq1cTExNTbp2YmBhSU1MBWLt2LVFR\nUVgslkCUW443tZ9/XXnjxo106tTJ32VWW0xMDGlpaRiGwc6dO2nevDmRkZGBLqtKR48edY9V7d69\nm7Kysjrxy4RhGEybNo2OHTsyevRoj+vUxXPuTd119Zw3RPV75qkaCAkJYfr06Tz00EO4XC7uvvtu\nunTpwhtvvMENN9xAbGwsycnJTJ48GbvdTnh4OK+//nqgywa8q33RokVs3LiR4OBgwsPD+fOf/xzo\nsnn88cfZvn07J06coF+/fjz66KOUlpYCMHLkSPr378/mzZux2+00btyY2bNnB7jis6qqe+3atSxe\nvJjg4GAuv/xy5s6dWyd+mfjqq69IT0/nmmuuISEhATh7LEeOHAHq7jn3pu66es4bIk3wJyIipi7Z\ny00iIlI1hYSIiJhSSIiIiCmFhIiImFJIiIicp6oJHc/3008/cf/99xMfH8+oUaPKPXPSUCgkRETO\nk5SUxMKFC71a9+WXXyYxMZFPPvmE8ePH89prr/m4Ov9TSEi9M2XKFD788EOPy9566y1efvllr/e1\nePFi3nvvPY/LVq1axcSJEy+mRKnHPE3oeOjQIR588EGSkpL4/e9/zw8//ADADz/8QFRUFABRUVH1\nYqLN6rpkH6aT+snlctXq/kaOHFmr+7sYpaWlhIToP8W67LnnnuOFF17g6quvZteuXbzwwgt88MEH\nXHfddaxbt47777+f9evXU1xczIkTJxrU09/6myk+t2TJEvbv38/zzz/P7t27SUlJYfny5XTv3p0Z\nM2Zw/fXX06ZNG+bOnYvL5SIiIoIXX3yRDh068OWXXzJr1ixuuOEG9uzZw6RJk8rt+9SpU0ybNo1v\nv/2W1q1bY7PZuOKKKwC4/fbbSUtLo1WrVowZMwaLxcKCBQvIz89n+PDhZGRk8NZbb/Hzzz/z9NNP\nU1JSwqxZs8jKyqJly5Zcf/315b5rwYIFrFu3DpfLhdVqZebMmbRu3dr0uNeuXcvrr7/O5ZdfzqBB\ng3j99dfZsWMHTZs25dprr+WPf/wjmzZt4vbbb+fRRx9lzpw5bNmyxV37k08+SXBwMEuXLuW9994j\nNDSUsrIy5s2bx29+8xtefPFFsrKyCA0NpUmTJixZsqSW/80JnJ1o8J///CePPfaYu62kpASAp556\nipkzZ5Kamsott9yC1WolODg4UKX6hEJCfC46Otp9SSczM5ObbrqJrKwsunfvTmZmJsOHD+fhhx/m\nww8/pHPnzixfvpwnn3zS/Ta977//nhdffJGbbroJOPvD95z//u//pmnTpqxZs4bjx4+TlJTE4MGD\nAejVqxdZWVnExcWRnZ2NxWLhzJkzZGZm0qtXrwp1Ll26lOzsbFavXk1paSn33nsv7dq1AyA9PZ3D\nhw+zbNkygoKC+Oijj/jLX/5ieg362LFjTJ8+naVLl3L11Vd7vKTVqFEjVq5cCcBHH33E3r17WbVq\nFQBjxoxh6dKl/P73v+eVV17h888/JzIykpKSElwuF/v27ePLL7/ks88+IygoiIKCgov4NyPeMAyD\nsLAw0tPTKyyzWq3ut+IVFxezbt06wsLC/F2iT2lMQnyuQ4cOnD59mtzcXDIzM/nTn/5EZmYmOTk5\nnDlzhvz8fK677jo6d+4MwN13383evXspKipyb38uIC705ZdfkpycDEBERAR2u929LDo6mm3btrFr\n1y569uxJ9+7d2bVrF9u2bXNfR75wX4mJiVx22WU0btyYYcOGuZdt3LiRbdu2MXz4cBISEvjoo4/4\n6aefTI95165ddO3alauvvtp9TBc690IrwB2WoaGhhIaGkpSURGZmJnD2WveUKVNYtGgRTqeTxo0b\n0759e0pLS5k2bRppaWmmdUjNNWvWjHbt2vH5558DZ0Nj3759ABw/fpyysjLgbE/T07/n+k4hIX4R\nFRXFF198QX5+Pr169eLo0aNs2rTJ42/0F2rSpMlFf2dmZiaZmZlERUURFRVFVlYWWVlZREdHV2tf\nhmEwbtw40tPTSU9P59NPP63x5R1vj2v+/PlMmjSJX375hfvuu4/NmzfTvHlzVq9ezZAhQ9i/fz9D\nhw7l6NGjNapHznr88ce55557OHDgAP369WP58uW8+uqrrFixgmHDhjF06FA2bNgAwPbt2xk0aBAD\nBw7k2LFjjBs3LsDV1z5dbhK/iIqK4o033qBv374A/Pa3v+Wdd95h0qRJ9OzZk6lTp/LDDz/QqVMn\nUlNT6dq1q1cvyImKimLVqlXcfPPNnDhxgg0bNjBo0CAArrzySoKDg0lNTWXJkiUYhsHIkSMJCQmh\nbdu2HveVnp7OkCFDKC0t5dNPP3WvFxMTwwcffOCeEbikpIQff/yR6667zmNdPXr0YOrUqRw6dIir\nrrrKPeW8mejoaNLS0hgyZAgAaWlpxMXFUVpaypEjR+jevTvdu3fn0KFD7N27lxtvvJHg4GBuv/12\nevfuzaZNmzh8+HClYyTinblz53ps//vf/16hbdCgQe6/bw2VQkL8Iioqiqeeesr9G3xUVBRLly4l\nKiqKiIgIXnnlFZ588klKS0uJiIjg1Vdf9Wq/48ePZ+rUqQwaNIjWrVtzyy23lFseHR3NV1995X5H\nwuWXX15hnXNGjBjB/v37GTJkCC1btuTGG28kPz8fgMTERE6ePMkf/vAHAHfgmIXEFVdcwYwZMxgz\nZgyNGzfmjjvucF/G8uR3v/sdhw4dcl+C6tu3LyNGjMDlcjFlyhROnTqFxWKhTZs2PPHEExw5coTn\nnnuO0tJSXC4X/fr1o2fPnl6dM5Hq0FThIj5SVFTk7g2tXLmSFStWsHjx4gBXJVI96kmI+MiiRYtY\ns2YNLpeL8PBwZs2aFeiSRKpNPQmRGpg/fz7r16+v0P6Pf/yDVq1aBaAikdqlkBAREVO6BVZEREwp\nJERExJRCQkRETCkkRETElEJCRERM/R8Jf2VTaR3DlAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f658474d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def label_gross_2 (gross):\n",
    "    if (gross < 200000000) : return 1\n",
    "    elif (gross >= 200000000) : return 2\n",
    "\n",
    "def label_gross_3 (gross):\n",
    "    if (gross < 10000000) : return 1\n",
    "    elif ((gross >= 10000000) & (gross < 300000000)) : return 2\n",
    "    elif (gross >= 300000000) : return 3\n",
    "\n",
    "def label_gross_4 (gross):\n",
    "    if (gross < 5000000) : return 1\n",
    "    elif ((gross >= 5000000) & (gross < 50000000)) : return 2\n",
    "    elif ((gross >= 50000000) & (gross < 350000000)) : return 3\n",
    "    elif (gross >= 350000000) : return 4\n",
    "\n",
    "def label_gross_5 (gross):\n",
    "    if (gross < 1000000) : return 1\n",
    "    elif ((gross >= 1000000) & (gross < 25000000)) : return 2\n",
    "    elif ((gross >= 25000000) & (gross < 100000000)) : return 3\n",
    "    elif ((gross >= 100000000) & (gross < 400000000)) : return 4\n",
    "    elif (gross >= 400000000) : return 5\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "file_plot = \"../dataset/movie_metadata_cleaned_no_vector_num_only.csv\"\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "df = pd.read_csv(file_plot)\n",
    "df = df.dropna()\n",
    "#from sklearn.cluster import AffinityPropagation\n",
    "#af = AffinityPropagation().fit(df.drop('worldwide_gross', axis=1))\n",
    "df[\"clusters\"] = df.worldwide_gross.apply (lambda gross: label_gross_2 (gross))\n",
    "#x_plot = preprocessing.Normalizer().fit_transform(df[\"worldwide_gross\"])\n",
    "#df[\"worldwide_gross\"] = x_plot.reshape((-1, 1))\n",
    "#df.sort(['worldwide_gross'], ascending=[True, False], inplace=True)\n",
    "sns.lmplot(x=\"worldwide_gross\", y=\"Unnamed: 0\", data=df, hue=\"clusters\", fit_reg=False)\n",
    "#sns.pairplot(df, hue=\"clusters\")\n",
    "#pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect, RFE\n",
    "\n",
    "#len(X.columns)\n",
    "##by default polynomical tranform adds new features n_features^pol + comb(pol,n_feeatures^pol-1)\n",
    "#pol = PolynomialFeatures(degree = 3, include_bias = False, interaction_only=True).fit(X)\n",
    "#print(pol.n_input_features_)\n",
    "#print(pol.n_output_features_)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "def get_powers_list(n_samples, n_features, n):\n",
    "    base_arr = [{\"pw\":2},{\"pw\":3},{\"pw\":4}]\n",
    "    max_pw = math.ceil(n_samples/n_features)\n",
    "    step = math.floor((max_pw-4) / n)\n",
    "    extra_arr = [{\"pw\":power} for power in range(4 + step, max_pw, step)]\n",
    "    if  n_samples/n_features < 2:\n",
    "        res = [{\"pw\":1}]\n",
    "    elif max_pw - 1 == 2:\n",
    "        res = [{\"pw\":2}]\n",
    "    elif max_pw - 1 == 3:\n",
    "        res = [{\"pw\":2}, {\"pw\":3}]\n",
    "    elif max_pw - 1 == 4:\n",
    "        res = [{\"pw\":2},{\"pw\":3},{\"pw\":4}]\n",
    "    else :\n",
    "        res = base_arr + extra_arr\n",
    "    return res\n",
    "\n",
    "def get_components_list(n_features, lst):\n",
    "    lst = lst + [{\"pw\": 0.1},{\"pw\": 0.4},{\"pw\": 0.5},{\"pw\": 0.8}]\n",
    "    lst = sorted(list(map(lambda x: math.floor(x[\"pw\"]*n_features), lst)) + [1, 3, 5], reverse=True)\n",
    "    lst[0] = lst[0]-1\n",
    "    lst_n = [n for n in lst if n < 3321]\n",
    "    if len(lst_n) < len(lst):\n",
    "        lst_n = [3320] + lst_n \n",
    "    return lst_n\n",
    "\n",
    "\n",
    "#get_components_list(n_features,get_components_list(n_features, [{\"pw\":2}, {\"pw\":1}])t_powers_list(n_samples, n_features, 3))\n",
    "\n",
    "get_components_list(n_features,get_powers_list(n_samples, n_features, 3))\n",
    "\n",
    "tuples_of_data = [(y,X), (y,X)]\n",
    "\n",
    "for ind, tupl in enumerate(tuples_of_data):\n",
    "    y_crr,x_crr = tupl\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+04,   1.00000000e+00,   1.00000000e-04])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reciprocal(np.logspace(-4, 4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'label_gross_5'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_gross_5 (gross):\n",
    "    if (gross < 1000000) : return 1\n",
    "    elif ((gross >= 1000000) & (gross < 25000000)) : return 2\n",
    "    elif ((gross >= 25000000) & (gross < 100000000)) : return 3\n",
    "    elif ((gross >= 100000000) & (gross < 400000000)) : return 4\n",
    "    elif (gross >= 400000000) : return 5\n",
    "    \n",
    "label_gross_5.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
