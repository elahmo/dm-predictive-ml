{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76781003  0.75989446  0.75725594  0.75629139  0.77748344]\n",
      "[ 0.5965274   0.63904399  0.61242553  0.50016402  0.5246557 ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, RFE\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "import seaborn as sns # More snazzy plotting library\n",
    "import itertools\n",
    "from itertools import  product\n",
    "import pprint\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from multiprocessing import Process, Value, Array\n",
    "from asyncio import Queue\n",
    "from threading import Thread\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "path_full = \"../dataset/no_imdb_names-count_cat-tf_184f.csv\"\n",
    "path_train = \"../dataset/no_imdb_names-count_cat-tf_184f_train.csv\"\n",
    "path_test = \"../dataset/no_imdb_names-count_cat-tf_184f_test.csv\"\n",
    "\n",
    "dta_full = pd.read_csv(path_full)\n",
    "dta_full = dta_full.fillna(value=0, axis=1)\n",
    "dta_full = dta_full.dropna()\n",
    "dta_full = dta_full.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "dta_train = pd.read_csv(path_train)\n",
    "dta_train = dta_train.fillna(value=0, axis=1)\n",
    "dta_train = dta_train.dropna()\n",
    "dta_train = dta_train.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "dta_test = pd.read_csv(path_test)\n",
    "dta_test = dta_test.fillna(value=0, axis=1)\n",
    "dta_test = dta_test.dropna()\n",
    "dta_test = dta_test.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "def label_gross_2 (gross):\n",
    "    if (gross < 200000000) : return 1\n",
    "    elif (gross >= 200000000) : return 2\n",
    "\n",
    "def label_gross_3 (gross):\n",
    "    if (gross < 10000000) : return 1\n",
    "    elif ((gross >= 10000000) & (gross < 300000000)) : return 2\n",
    "    elif (gross >= 300000000) : return 3\n",
    "\n",
    "def label_gross_4 (gross):\n",
    "    if (gross < 5000000) : return 1\n",
    "    elif ((gross >= 5000000) & (gross < 50000000)) : return 2\n",
    "    elif ((gross >= 50000000) & (gross < 350000000)) : return 3\n",
    "    elif (gross >= 350000000) : return 4\n",
    "\n",
    "def label_gross_5 (gross):\n",
    "    if (gross < 1000000) : return 1\n",
    "    elif ((gross >= 1000000) & (gross < 25000000)) : return 2\n",
    "    elif ((gross >= 25000000) & (gross < 100000000)) : return 3\n",
    "    elif ((gross >= 100000000) & (gross < 400000000)) : return 4\n",
    "    elif (gross >= 400000000) : return 5\n",
    "    \n",
    "def label_gross_6 (gross):\n",
    "    if (gross < 1000000) : return 1\n",
    "    elif ((gross >= 1000000) & (gross < 25000000)) : return 2\n",
    "    elif ((gross >= 25000000) & (gross < 50000000)) : return 3\n",
    "    elif ((gross >= 50000000) & (gross < 150000000)) : return 4\n",
    "    elif ((gross >= 150000000) & (gross < 450000000)) : return 5\n",
    "    elif (gross >= 450000000) : return 6\n",
    "\n",
    "def label_gross_7 (gross):\n",
    "    if (gross < 500000) : return 1\n",
    "    elif ((gross >= 500000) & (gross < 5000000)) : return 2\n",
    "    elif ((gross >= 5000000) & (gross < 50000000)) : return 3\n",
    "    elif ((gross >= 50000000) & (gross < 150000000)) : return 4\n",
    "    elif ((gross >= 150000000) & (gross < 200000000)) : return 5\n",
    "    elif ((gross >= 200000000) & (gross < 500000000)) : return 6\n",
    "    elif (gross >= 500000000) : return 7\n",
    "    \n",
    "def label_gross_8 (gross):\n",
    "    if (gross < 500000) : return 1\n",
    "    elif ((gross >= 500000) & (gross < 5000000)) : return 2\n",
    "    elif ((gross >= 5000000) & (gross < 20000000)) : return 3\n",
    "    elif ((gross >= 20000000) & (gross < 50000000)) : return 4\n",
    "    elif ((gross >= 50000000) & (gross < 100000000)) : return 5\n",
    "    elif ((gross >= 100000000) & (gross < 250000000)) : return 6\n",
    "    elif ((gross >= 250000000) & (gross < 550000000)) : return 7\n",
    "    elif (gross >= 550000000) : return 8\n",
    "    \n",
    "pw = 4\n",
    "def log_poly(X):\n",
    "    #hardcoe for this test\n",
    "    global pw\n",
    "    #do log\n",
    "    df_t = pd.DataFrame(X[:,:10])\n",
    "    X_t = df_t.replace(0, 1/math.e)\n",
    "    log_res = np.log(X_t)\n",
    "    \n",
    "    #do poly\n",
    "    vector = X[:,10:]\n",
    "    res    = X[:,:10]\n",
    "    X      = X[:,:10]\n",
    "    for power in range(2,pw + 1):\n",
    "        res = np.concatenate((res, np.power(X, power)), axis=1)\n",
    "    res_poly_log = np.concatenate((res, log_res), axis=1)\n",
    "    \n",
    "    #return conat results\n",
    "    return np.concatenate((res_poly_log, vector), axis=1)\n",
    "\n",
    "LogPolynomialTransformer = FunctionTransformer(log_poly)\n",
    "\n",
    "preprocessor = LogPolynomialTransformer\n",
    "\n",
    "transfomer = StandardScaler()\n",
    "\n",
    "reducer = RFE(ExtraTreesRegressor(), step=0.1, n_features_to_select =round((dta_train.shape[1] + (pw -1) *10)*0.33) )\n",
    "\n",
    "model_class = GradientBoostingClassifier()\n",
    "model_reg = GradientBoostingRegressor()\n",
    "\n",
    "pipe_cl = Pipeline(steps=[('preprocessor', preprocessor), ('transfomer', transfomer), ('reducer', reducer),  ('classifier', model_class)])\n",
    "pipe_reg = Pipeline(steps=[('preprocessor', preprocessor), ('transfomer', transfomer), ('reducer', reducer),  ('classifier', model_reg)])\n",
    "\n",
    "x_train = dta_train.drop('worldwide_gross', axis=1)\n",
    "y_train_cl = dta_train.worldwide_gross.apply (lambda gross: label_gross_3 (gross))\n",
    "y_train_reg = dta_train['worldwide_gross']\n",
    "\n",
    "x_test = dta_test.drop('worldwide_gross', axis=1)\n",
    "y_test_cl = dta_test.worldwide_gross.apply (lambda gross: label_gross_3 (gross))\n",
    "y_test_reg = dta_test['worldwide_gross']\n",
    "\n",
    "scores_cl = cross_val_score(pipe_cl ,x_train, y_train_cl, cv=5)\n",
    "scores_reg = cross_val_score(pipe_reg ,x_train, y_train_reg, cv=5)\n",
    "print(scores_cl)\n",
    "print(scores_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458509513742\n",
      "0.75096597253\n"
     ]
    }
   ],
   "source": [
    "model_test_cl =  pipe_cl.fit(x_train, y_train_cl)\n",
    "model_test_reg =  pipe_reg.fit(x_train, y_train_reg)\n",
    "print(model_test_cl.score(x_train,y_train_cl))\n",
    "print(model_test_reg.score(x_train,y_train_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.421330517423\n",
      "0.640583692201\n"
     ]
    }
   ],
   "source": [
    "print(model_test_cl.score(x_test,y_test_cl))\n",
    "print(model_test_reg.score(x_test,y_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82822410148\n",
      "0.794086589229\n"
     ]
    }
   ],
   "source": [
    "model_test_grtb_cl =  GradientBoostingClassifier().fit(x_train, y_train_cl)\n",
    "print(model_test_grtb_cl.score(x_train,y_train_cl))\n",
    "print(model_test_grtb_cl.score(x_test,y_test_cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746206504992\n",
      "0.648444194927\n"
     ]
    }
   ],
   "source": [
    "model_test_grtb_reg =  GradientBoostingRegressor().fit(x_train, y_train_reg)\n",
    "print(model_test_grtb_reg.score(x_train,y_train_reg))\n",
    "print(model_test_grtb_reg.score(x_test,y_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.451800357303\n",
      "0.32032433973\n",
      "############################\n",
      "0.486610018717\n",
      "0.376987111481\n",
      "############################\n",
      "0.807096287635\n",
      "0.421949603602\n",
      "############################\n"
     ]
    }
   ],
   "source": [
    "df_1_train = dta_train[dta_train[\"worldwide_gross\"] < 10000000]\n",
    "X_1_train = df_1_train.drop('worldwide_gross', axis=1)\n",
    "y_1_train = df_1_train['worldwide_gross']\n",
    "\n",
    "df_1_test = dta_test[dta_test[\"worldwide_gross\"] < 10000000]\n",
    "X_1_test = df_1_test.drop('worldwide_gross', axis=1)\n",
    "y_1_test = df_1_test['worldwide_gross']\n",
    "\n",
    "\n",
    "df_2_train = dta_train[dta_train[\"worldwide_gross\"] >= 10000000]\n",
    "df_2_train = df_2_train[df_2_train[\"worldwide_gross\"] < 300000000]\n",
    "X_2_train = df_2_train.drop('worldwide_gross', axis=1)\n",
    "y_2_train = df_2_train['worldwide_gross']\n",
    "\n",
    "df_2_test = dta_test[dta_train[\"worldwide_gross\"] >= 10000000]\n",
    "df_2_test = df_2_test[df_2_test[\"worldwide_gross\"] < 300000000]\n",
    "X_2_test = df_2_test.drop('worldwide_gross', axis=1)\n",
    "y_2_test = df_2_test['worldwide_gross']\n",
    "\n",
    "df_3_train = dta_train[dta_train[\"worldwide_gross\"] >= 300000000]\n",
    "X_3_train = df_3_train.drop('worldwide_gross', axis=1)\n",
    "y_3_train = df_3_train['worldwide_gross']\n",
    "\n",
    "df_3_test = dta_test[dta_test[\"worldwide_gross\"] >= 300000000]\n",
    "X_3_test = df_3_test.drop('worldwide_gross', axis=1)\n",
    "y_3_test = df_3_test['worldwide_gross']\n",
    "\n",
    "\n",
    "model_1 =  GradientBoostingRegressor().fit(X_1_train, y_1_train)\n",
    "print(model_1.score(X_1_train,y_1_train))\n",
    "print(model_1.score(X_1_test,y_1_test))\n",
    "print('############################')\n",
    "model_2 =  GradientBoostingRegressor().fit(X_2_train, y_2_train)\n",
    "print(model_2.score(X_2_train,y_2_train))\n",
    "print(model_2.score(X_2_test,y_2_test))\n",
    "print('############################')\n",
    "model_3 =  GradientBoostingRegressor().fit(X_3_train, y_3_train)\n",
    "print(model_3.score(X_3_train,y_3_train))\n",
    "print(model_3.score(X_3_test,y_3_test))\n",
    "print('############################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_test_cl = pd.concat([pd.DataFrame(model_test_grtb_cl.predict(x_test)), pd.DataFrame(x_test)], axis=1)\n",
    "x_train_cl = pd.concat([pd.DataFrame(model_test_grtb_cl.predict(x_train)), pd.DataFrame(x_train)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58182443054843747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lst = []\n",
    "row_ggl = None\n",
    "for ind, row in x_test_cl.iterrows():\n",
    "    row_for_pred = row.drop(row.index[0]).reshape(1, -1)\n",
    "    if row[0] == 1:\n",
    "        y_pred = model_1.predict(row_for_pred)\n",
    "    elif row[0] == 2:\n",
    "        y_pred = model_2.predict(row_for_pred)\n",
    "    elif row[0] == 3:\n",
    "        y_pred = model_3.predict(row_for_pred)\n",
    "    else:\n",
    "        print(\"No class ffound\", end=\" \")\n",
    "        print(ind)\n",
    "    y_pred_lst.append(y_pred)\n",
    "r2_score(y_test_reg,y_pred_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772238798525\n",
      "0.618416611594\n"
     ]
    }
   ],
   "source": [
    "model_regressor_with_cl =  GradientBoostingRegressor().fit(x_train_cl, y_train_reg)\n",
    "print(model_regressor_with_cl.score(x_train_cl,y_train_reg))\n",
    "print(model_regressor_with_cl.score(x_test_cl,y_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876623340409\n",
      "0.599733139766\n"
     ]
    }
   ],
   "source": [
    "x_with_given_cl = pd.concat([y_train_cl, x_train], axis=1)\n",
    "model_regressor_with_cl_with_cl_bef =  GradientBoostingRegressor().fit(x_with_given_cl, y_train_reg)\n",
    "print(model_regressor_with_cl_with_cl_bef.score(x_with_given_cl,y_train_reg))\n",
    "print(model_regressor_with_cl_with_cl_bef.score(x_test_cl,y_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
