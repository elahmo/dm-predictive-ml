#########################################
###Starting all estimators for cl: label_gross_5
#########################################
####################################################################################

####################################################################################
***Starting [LinearSVC] estimator run, pipeline: | preprocessor:dummy | transfomer: dummy | reducer: dummy 
##param_grid##
{'model__loss': ['hinge', 'squared_hinge'], 'model__dual': [False], 'model__C': array([  1.00000000e-04,   1.00000000e+00,   1.00000000e+04]), 'model__penalty': ['l1', 'l2']}
Fitting 4 folds for each of 12 candidates, totalling 48 fits
[CV] model__dual=False, model__loss=hinge, model__C=0.0001, model__penalty=l1 
[CV] model__dual=False, model__loss=hinge, model__C=0.0001, model__penalty=l2 
[CV] model__dual=False, model__loss=hinge, model__C=0.0001, model__penalty=l1 
[CV] model__dual=False, model__loss=hinge, model__C=0.0001, model__penalty=l2 
[CV] model__dual=False, model__loss=hinge, model__C=0.0001, model__penalty=l1 
[CV] model__dual=False, model__loss=hinge, model__C=0.0001, model__penalty=l2 
[CV] model__dual=False, model__loss=hinge, model__C=0.0001, model__penalty=l1 
[CV] model__dual=False, model__loss=hinge, model__C=0.0001, model__penalty=l2 
GREP_ME***Error caught for  [LinearSVC] , pipeline: [| preprocessor:dummy | transfomer: dummy | reducer: dummy] 
#########################################
###Finished all estimators for cl: label_gross_5
#########################################
#########################################
######Printing all errors for cl: label_gross_5
#########################################
[{'Model[LinearSVC] pipe: | preprocessor:dummy | transfomer: dummy | reducer: dummy': {'error': JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/home/user/projects/data_mining/predictive_data_mining/logistic_regression/classifier_solver.py in <module>()
    416     with warnings.catch_warnings():
    417         warnings.simplefilter("ignore")
    418         trg = "classifyRes_" + time + "_" + cb.__name__ + ".log"
    419         new_file = open(trg,"w")
    420         sys.stdout = new_file
--> 421         run_for_many(cb.__name__,cb)
    422         #return stdout for some reason
    423 sys.stdout = orig_stdout
    424 
    425 

...........................................................................
/home/user/projects/data_mining/predictive_data_mining/logistic_regression/classifier_solver.py in run_for_many(cl_n='label_gross_5', label_fn=<function label_gross_5>)
    381     X = dta_clean.drop('worldwide_gross', axis=1)
    382     y = dta_clean.worldwide_gross.apply (lambda gross: label_fn (gross))
    383     print ("#########################################")
    384     print ("###Starting all estimators for cl: "+ str(cl_n))
    385     print ("#########################################")
--> 386     run_solver(X,y, preprocessors, transfomers, reducers, models, results, errors, errors_ind)
        X =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns]
        y = 0       5
1       5
2       5
3       5
4       ...  1
4811    1
Name: worldwide_gross, dtype: int64
        errors = []
        errors_ind = []
    387     print ("#########################################")
    388     print ("###Finished all estimators for cl: "+ str(cl_n))
    389     print ("#########################################")
    390 

...........................................................................
/home/user/projects/data_mining/predictive_data_mining/logistic_regression/classifier_solver.py in run_solver(x=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns], y=3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64, preprocessors=[FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True)], transfomers=[FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True)], reducers=[FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True)], models=[LinearSVC(C=1.0, class_weight=None, dual=True, f...', random_state=None, tol=0.0001,
     verbose=0)], results={}, errors=[], errors_ind=[])
    346         else:
    347             n_components = get_components_list(n_features, [{"pw":1}])
    348             reducers_cfg[PCA.__name__]["reducer__n_components"] = n_components
    349             reducers_cfg[GenericUnivariateSelect.__name__]["reducer__param"] = n_components
    350             reducers_cfg[RFE.__name__]["reducer__n_features_to_select"] = n_components
--> 351             run_grid_search(x,y,preprocessor, transfomer, reducer, model, results, errors, errors_ind)
        x =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns]
        y = 3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64
        preprocessor = FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True)
        transfomer = FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True)
        reducer = FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True)
        model = LinearSVC(C=1.0, class_weight=None, dual=True, f...', random_state=None, tol=0.0001,
     verbose=0)
        results = {}
        errors = []
        errors_ind = []
    352 
    353 ##run calssifiers for two 4 cases - 2 classes, 3 clasees, 4 classes, 5 clasess
    354 
    355 def label_gross_2 (gross):

...........................................................................
/home/user/projects/data_mining/predictive_data_mining/logistic_regression/classifier_solver.py in run_grid_search(x=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns], y=3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64, preprocessor=FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True), transfomer=FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True), reducer=FunctionTransformer(accept_sparse=False,
       ...=None, kw_args=None, pass_y=False, validate=True), model=LinearSVC(C=1.0, class_weight=None, dual=True, f...', random_state=None, tol=0.0001,
     verbose=0), results={}, errors=[], errors_ind=[])
    303     print("##param_grid##")
    304     print(param_grid)
    305     estimator = GridSearchCV(pipe,param_grid,verbose=2, cv=cv, n_jobs=4)
    306     #run the esmimator, except eceptions, sape errors
    307     try:
--> 308             estimator.fit(x, y)
        estimator.fit = <bound method GridSearchCV.fit of GridSearchCV(c...rain_score=True,
       scoring=None, verbose=2)>
        x =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns]
        y = 3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64
    309             print ("GREP_ME***Results of ["  + name + "] estimatorrun are")
    310             print (estimator.cv_results_)
    311             print ("GREP_ME***Best params of ["  + name + "] estimator,pipeline:"+ pipeline_cfg+"  run are")
    312             print (estimator.best_params_)

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=4, error_score='raise',
       e...train_score=True,
       scoring=None, verbose=2), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns], y=3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64, groups=None)
    940 
    941         groups : array-like, with shape (n_samples,), optional
    942             Group labels for the samples used while splitting the dataset into
    943             train/test set.
    944         """
--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))
        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,
       scoring=None, verbose=2)>
        X =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns]
        y = 3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64
        groups = None
        self.param_grid = {'model__C': array([  1.00000000e-04,   1.00000000e+00,   1.00000000e+04]), 'model__dual': [False], 'model__loss': ['hinge', 'squared_hinge'], 'model__penalty': ['l1', 'l2']}
    946 
    947 
    948 class RandomizedSearchCV(BaseSearchCV):
    949     """Randomized search on hyper parameters.

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=4, error_score='raise',
       e...train_score=True,
       scoring=None, verbose=2), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns], y=3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64, groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)
    559                                   fit_params=self.fit_params,
    560                                   return_train_score=self.return_train_score,
    561                                   return_n_test_samples=True,
    562                                   return_times=True, return_parameters=True,
    563                                   error_score=self.error_score)
--> 564           for parameters in parameter_iterable
        parameters = undefined
        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>
    565           for train, test in cv_iter)
    566 
    567         # if one choose to see train score, "out" will contain train score info
    568         if self.return_train_score:

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=4), iterable=<generator object <genexpr>>)
    763             if pre_dispatch == "all" or n_jobs == 1:
    764                 # The iterable was consumed all at once by the above for loop.
    765                 # No need to wait for async callbacks to trigger to
    766                 # consumption.
    767                 self._iterating = False
--> 768             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=4)>
    769             # Make sure that we get a last message telling us we are done
    770             elapsed_time = time.time() - self._start_time
    771             self._print('Done %3i out of %3i | elapsed: %s finished',
    772                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Sat Apr  8 18:38:39 2017
PID: 1226                                    Python 3.4.3: /usr/bin/python3
...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('preprocessor', FunctionTransfo...l1', random_state=None, tol=0.0001, verbose=0))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns], 3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1142, 1152, 1154, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1269, 1276, 1348]), 2, {'model__C': 0.0001, 'model__dual': False, 'model__loss': 'hinge', 'model__penalty': 'l1'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(steps=[('preprocessor', FunctionTransfo...l1', random_state=None, tol=0.0001, verbose=0))]),       actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns], 3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64, <function _passthrough_scorer>, array([1142, 1152, 1154, ..., 4809, 4810, 4811]), array([   0,    1,    2, ..., 1269, 1276, 1348]), 2, {'model__C': 0.0001, 'model__dual': False, 'model__loss': 'hinge', 'model__penalty': 'l1'})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('preprocessor', FunctionTransfo...l1', random_state=None, tol=0.0001, verbose=0))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[4812 rows x 138 columns], y=3229    2
3472    2
4515    1
1238    3
3131    ...  4
4278    1
Name: worldwide_gross, dtype: int64, scorer=<function _passthrough_scorer>, train=array([1142, 1152, 1154, ..., 4809, 4810, 4811]), test=array([   0,    1,    2, ..., 1269, 1276, 1348]), verbose=2, parameters={'model__C': 0.0001, 'model__dual': False, 'model__loss': 'hinge', 'model__penalty': 'l1'}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')
    233 
    234     try:
    235         if y_train is None:
    236             estimator.fit(X_train, **fit_params)
    237         else:
--> 238             estimator.fit(X_train, y_train, **fit_params)
        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...1', random_state=None, tol=0.0001, verbose=0))])>
        X_train =       actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[3606 rows x 138 columns]
        y_train = 817     4
1059    4
1042    4
349     4
928     ...  4
4278    1
Name: worldwide_gross, dtype: int64
        fit_params = {}
    239 
    240     except Exception as e:
    241         # Note fit time as time until error
    242         fit_time = time.time() - start_time

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('preprocessor', FunctionTransfo...l1', random_state=None, tol=0.0001, verbose=0))]), X=      actor_1_facebook_likes  actor_2_facebook_l...        0          0  

[3606 rows x 138 columns], y=817     4
1059    4
1042    4
349     4
928     ...  4
4278    1
Name: worldwide_gross, dtype: int64, **fit_params={})
    265         self : Pipeline
    266             This estimator
    267         """
    268         Xt, fit_params = self._fit(X, y, **fit_params)
    269         if self._final_estimator is not None:
--> 270             self._final_estimator.fit(Xt, y, **fit_params)
        self._final_estimator.fit = <bound method LinearSVC.fit of LinearSVC(C=0.000...='l1', random_state=None, tol=0.0001, verbose=0)>
        Xt = array([[  6.53000000e+02,   6.00000000e+02,   4....000000e+00,   0.00000000e+00,   0.00000000e+00]])
        y = 817     4
1059    4
1042    4
349     4
928     ...  4
4278    1
Name: worldwide_gross, dtype: int64
        fit_params = {}
    271         return self
    272 
    273     def fit_transform(self, X, y=None, **fit_params):
    274         """Fit the model and transform with the final estimator

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/svm/classes.py in fit(self=LinearSVC(C=0.0001, class_weight=None, dual=Fals...y='l1', random_state=None, tol=0.0001, verbose=0), X=array([[  6.53000000e+02,   6.00000000e+02,   4....000000e+00,   0.00000000e+00,   0.00000000e+00]]), y=array([4, 4, 4, ..., 3, 4, 1]), sample_weight=None)
    210 
    211         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(
    212             X, y, self.C, self.fit_intercept, self.intercept_scaling,
    213             self.class_weight, self.penalty, self.dual, self.verbose,
    214             self.max_iter, self.tol, self.random_state, self.multi_class,
--> 215             self.loss, sample_weight=sample_weight)
        self.loss = 'hinge'
        sample_weight = None
    216 
    217         if self.multi_class == "crammer_singer" and len(self.classes_) == 2:
    218             self.coef_ = (self.coef_[1] - self.coef_[0]).reshape(1, -1)
    219             if self.fit_intercept:

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/svm/base.py in _fit_liblinear(X=array([[  6.53000000e+02,   6.00000000e+02,   4....000000e+00,   0.00000000e+00,   0.00000000e+00]]), y=array([4, 4, 4, ..., 3, 4, 1]), C=0.0001, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=False, verbose=0, max_iter=1000, tol=0.0001, random_state=None, multi_class='ovr', loss='hinge', epsilon=0.1, sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.]))
    903         sample_weight = np.ones(X.shape[0])
    904     else:
    905         sample_weight = np.array(sample_weight, dtype=np.float64, order='C')
    906         check_consistent_length(sample_weight, X)
    907 
--> 908     solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
        solver_type = undefined
        multi_class = 'ovr'
        penalty = 'l1'
        loss = 'hinge'
        dual = False
    909     raw_coef_, n_iter_ = liblinear.train_wrap(
    910         X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C,
    911         class_weight_, max_iter, rnd.randint(np.iinfo('i').max),
    912         epsilon, sample_weight)

...........................................................................
/usr/local/lib/python3.4/dist-packages/sklearn/svm/base.py in _get_liblinear_solver_type(multi_class='ovr', penalty='l1', loss='hinge', dual=False)
    767                                 % (penalty, loss, dual))
    768             else:
    769                 return solver_num
    770     raise ValueError('Unsupported set of arguments: %s, '
    771                      'Parameters: penalty=%r, loss=%r, dual=%r'
--> 772                      % (error_string, penalty, loss, dual))
        error_string = "The combination of penalty='l1' and loss='hinge' is not supported"
        penalty = 'l1'
        loss = 'hinge'
        dual = False
    773 
    774 
    775 def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,
    776                    penalty, dual, verbose, max_iter, tol,

ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False
___________________________________________________________________________}}]
#########################################
######Printing errors summary for cl: label_gross_5
#########################################
[{'cfg': 'Model[LinearSVC] pipe: | preprocessor:dummy | transfomer: dummy | reducer: dummy'}]
#########################################
#######Printing results for cl: label_gross_5
#########################################
{}
priting simply sorted numbers, grep them to find the best cfg or cl: label_gross_5
[]
