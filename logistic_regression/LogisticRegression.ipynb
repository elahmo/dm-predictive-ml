{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['actor_1_facebook_likes',\n",
       " 'actor_2_facebook_likes',\n",
       " 'actor_3_facebook_likes',\n",
       " 'aspect_ratio',\n",
       " 'budget',\n",
       " 'cast_total_facebook_likes',\n",
       " 'director_facebook_likes',\n",
       " 'duration',\n",
       " 'facenumber_in_poster',\n",
       " 'gross',\n",
       " 'imdb_score',\n",
       " 'movie_facebook_likes',\n",
       " 'num_critic_for_reviews',\n",
       " 'num_user_for_reviews',\n",
       " 'num_voted_users',\n",
       " 'title_year',\n",
       " 'gross_class']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore warnings for clear printing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns # More snazzy plotting library\n",
    "import itertools\n",
    "\n",
    "\n",
    "##test results \n",
    "#  1.looks the absolute year number gives a good colleration for prediction. \n",
    "#  2. standarlistaion improves prediction by 3%\n",
    "#  3. removing aspect ratio improves prediction by 2%\n",
    "\n",
    "\n",
    "#open the file\n",
    "file_path = \"/home/user/projects/data_mining/predictive_data_mining/logistic_regression/working_dataset.csv\"\n",
    "dta = pd.read_csv(file_path)\n",
    "#clean up data non numeric rows\n",
    "str_list = [] # empty list to contain columns with strings (words)\n",
    "for colname, colvalue in dta.iteritems():\n",
    "    if type(colvalue[1]) == str:\n",
    "        #if colname not in str_list:\n",
    "            str_list.append(colname)\n",
    "# Get to the numeric columns by inversion\n",
    "num_list = dta.columns.difference(str_list)\n",
    "#USe only the numeriv values\n",
    "dta_clean = dta[num_list]\n",
    "#remove the null values, that is fill NaN with there - FIXME: Rihards, naive implementation\n",
    "#dta_clean = dta_clean.fillna(value=0, axis=1)\n",
    "dta_clean = dta_clean.dropna()\n",
    "dta_clean = dta_clean.reindex_axis(sorted(dta_clean.columns), axis=1)\n",
    "#clean up data from zero rows \n",
    "for colname, colvalue in dta_clean.iteritems():\n",
    "    if colname != 'facenumber_in_poster':\n",
    "        dta_clean = dta_clean[dta_clean[colname] != 0]\n",
    "dta_clean.count()\n",
    "#clasify the data for the logistic regression\n",
    "def label_gross (gross):\n",
    "    if (gross < 1000000) : return 1\n",
    "    elif ((gross >= 1000000) & (gross < 10000000)) : return 2\n",
    "    elif ((gross >= 10000000) & (gross < 50000000)) : return 3\n",
    "    elif ((gross >= 50000000) & (gross < 200000000)) : return 4\n",
    "    elif (gross >= 200000000) : return 5\n",
    "\n",
    "dta_clean['gross_class'] = dta_clean.gross.apply (lambda gross: label_gross (gross))\n",
    "dta_clean.count()\n",
    "\n",
    "#define regression function\n",
    "def run_logistic_regression(yIn, xIn, solver, prct_split=0.3, scale=False, stadartlize=False, normalize=False, print_intr=False):\n",
    "        #flatten the y vector\n",
    "        yIn = np.ravel(yIn)\n",
    "        \n",
    "        #do data tranformation\n",
    "        if scale: \n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            for colname, colvalue in xIn.iteritems():\n",
    "                xIn[colname] = min_max_scaler.fit_transform(xIn[colname])\n",
    "        \n",
    "        if stadartlize:             \n",
    "            for colname, colvalue in xIn.iteritems():\n",
    "                standard_scaler = preprocessing.StandardScaler().fit(xIn[colname])\n",
    "                xIn[colname] = standard_scaler.transform(xIn[colname])\n",
    "\n",
    "        if normalize: \n",
    "            for colname, colvalue in xIn.iteritems():\n",
    "                nomalizer_scaler = preprocessing.Normalizer().fit(xIn[colname])\n",
    "                xIn[colname] = nomalizer_scaler.transform(xIn[colname])[0] \n",
    "                \n",
    "        #train the model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(xIn, yIn, test_size=prct_split, random_state=0)\n",
    "        model = LogisticRegression(solver=solver)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # predict class labels for the test set\n",
    "        predicted = model.predict(X_test)\n",
    "        \n",
    "        # find the avarge score 10 iterations\n",
    "        scores = cross_val_score(model, xIn, yIn, scoring='accuracy', cv=10)\n",
    "       \n",
    "        #print findings\n",
    "        if print_intr: \n",
    "            print \"Running with scale: \" + str(scale) + \", stadartlize: \" + str(stadartlize) + \", normalize: \" + str(normalize)\n",
    "            print \"Test data acuracy is: \" + str(metrics.accuracy_score(y_test,predicted))\n",
    "            print  \"Avarage model score: \" + str(scores.mean())\n",
    "        \n",
    "        return metrics.accuracy_score(y_test,predicted), scores.mean()\n",
    "\n",
    "#define data trasormation itterator\n",
    "def run_logc_reg_with_data_trasn(yIn,xIn,prct_split, print_intr):\n",
    "    prev_test_acc = 0\n",
    "    prev_avg_acc = 0 \n",
    "    test_acc_str = \"No Result\"\n",
    "    avg_ac_str = \"No Result\"\n",
    "    for solver in ['newton-cg', 'lbfgs', 'liblinear', 'sag']:\n",
    "        if print_intr:\n",
    "            print \"######################################################\"\n",
    "            print \"Starting regressions sequance with solver: \" + solver\n",
    "        for scale in [False, True]: \n",
    "            for stadartlize in [False, True]:\n",
    "                for normalize in [False, True]:\n",
    "                    test_acc, avg_acc = run_logistic_regression(yIn,xIn,solver, prct_split, scale, stadartlize, normalize, print_intr)\n",
    "                    #save the highest accuracy result and result string\n",
    "                    if test_acc > prev_test_acc: \n",
    "                        prev_test_acc = test_acc\n",
    "                        test_acc_str = \"End highest test acc \" + str(test_acc) + \", achieved with solver: \" + solver + \", scale: \" + str(scale) + \", stadartlize: \" + str(stadartlize) + \", normalize: \" + str(normalize)\n",
    "                    if avg_acc  > prev_avg_acc :\n",
    "                        prev_avg_acc = avg_acc              \n",
    "                        avg_ac_str = \"End highest avg model acc \" + str(avg_acc) + \", achieved with solver: \" + solver + \", scale: \" + str(scale) + \", stadartlize: \" + str(stadartlize) + \", normalize: \" + str(normalize)\n",
    "        if print_intr: print \"######################################################\"\n",
    "    print test_acc_str\n",
    "    print avg_ac_str\n",
    "    return prev_test_acc, prev_avg_acc\n",
    "\n",
    "def run_diff_predictors():\n",
    "    headers = list(dta_clean.columns.values)\n",
    "    headers.remove('gross_class')\n",
    "    headers.remove('gross')\n",
    "    itter = 0\n",
    "    prev_test_acc = 0\n",
    "    prev_avg_acc = 0\n",
    "    best_test_acc_str = \"No results\"\n",
    "    best_avg_acc_str = \"No results\"\n",
    "    iterator = 0\n",
    "    for L in range(0, len(headers)+1):\n",
    "        for subset in itertools.combinations(headers, L):\n",
    "            iterator = iterator + 1\n",
    "    print \"Running total \" + str(iterator) + \" itterators\"\n",
    "    for L in range(0, len(headers)+1):\n",
    "        for subset in itertools.combinations(headers, L):\n",
    "            string = \"gross_class ~ \" + \" + \".join(str(x) for x in subset)            \n",
    "            if itter  > 0:\n",
    "                print itter,\n",
    "                #print(string)\n",
    "                y, X = dmatrices(string,dta_clean, return_type=\"dataframe\")\n",
    "                test_acc, avg_acc = run_logc_reg_with_data_trasn(y,X, 0.3, print_intr=False)            \n",
    "            #save best data\n",
    "                if test_acc > prev_test_acc: \n",
    "                    prev_test_acc = test_acc\n",
    "                    best_test_acc_str = \"End highest test acc \" + str(test_acc) + \", achieved with \" + string\n",
    "                if avg_acc > prev_avg_acc: \n",
    "                    prev_avg_acc = avg_acc              \n",
    "                    best_avg_acc_str = \"End highest avg model acc \" + str(avg_acc) + \", achieved with \" + string\n",
    "            itter = itter + 1\n",
    "            \n",
    "    #dump out the results\n",
    "    print \"Ending the predictors\"\n",
    "    print best_test_acc_str\n",
    "    print best_avg_acc_str\n",
    "    print \"num of tries was \" + str(itter)\n",
    "#extract data frames as predictors and target data sets\n",
    "            \n",
    "\n",
    "#run_logc_reg_with_data_trasn(y,X,0.3, print_intr=False)\n",
    "#run_diff_predictors()\n",
    "list(dta_clean.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny, X = dmatrices(\\'gross_class ~                  actor_1_facebook_likes + actor_2_facebook_likes                 + actor_3_facebook_likes + budget + cast_total_facebook_likes                 + imdb_score + movie_facebook_likes + num_critic_for_reviews                 + num_voted_users + title_year\\',\\n                 dta_clean, return_type=\"dataframe\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "y, X = dmatrices('gross_class ~ \\\n",
    "                 actor_1_facebook_likes + actor_2_facebook_likes\\\n",
    "                 + actor_3_facebook_likes + budget + cast_total_facebook_likes\\\n",
    "                 + imdb_score + movie_facebook_likes + num_critic_for_reviews\\\n",
    "                 + num_voted_users + title_year',\n",
    "                 dta_clean, return_type=\"dataframe\")\n",
    "'''\n",
    "#with warnings.catch_warnings():\n",
    "#    warnings.simplefilter(\"ignore\")\n",
    "#    run_logc_reg_with_data_trasn(y,X, 0.3, print_intr=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   1.44189674e+08   2.40495586e+07   8.46367044e+06\n",
      "    5.38236614e+02   6.12153263e+10   1.81980468e+08   6.52369507e+06\n",
      "    4.22500531e+04   1.85356522e+03   2.13029925e+03   7.53286579e+07\n",
      "    4.79767003e+05   9.43744905e+05   5.49929081e+08   2.20556742e+04]\n",
      " [  0.00000000e+00   2.40495586e+07   1.83250849e+07   6.39658246e+06\n",
      "    1.51947921e+02   4.09761751e+10   5.22262928e+07   3.24249923e+06\n",
      "    1.67868336e+04   1.13838513e+03   8.19892323e+02   3.35596377e+07\n",
      "    2.04715219e+05   4.40418364e+05   2.38849574e+08   7.30532298e+03]\n",
      " [  0.00000000e+00   8.46367044e+06   6.39658246e+06   4.70042025e+06\n",
      "    5.95486040e+01   1.85063084e+10   2.18868848e+07   1.56278201e+06\n",
      "    6.15158928e+03   6.12032037e+02   2.01368117e+02   1.48834843e+07\n",
      "    8.21644741e+04   2.32423551e+05   1.14003925e+08   2.89642442e+03]\n",
      " [  0.00000000e+00   5.38236614e+02   1.51947921e+02   5.95486040e+01\n",
      "    7.31263913e-02   3.71677803e+05   7.93802962e+02   6.40106140e+01\n",
      "    1.32369467e+00   3.64026483e-02   2.47596188e-02   1.67871485e+03\n",
      "    1.19078478e+01   2.00917416e+01   8.15513024e+03   9.61845629e-01]\n",
      " [  0.00000000e+00   6.12153263e+10   4.09761751e+10   1.85063084e+10\n",
      "    3.71677803e+05   9.93964790e+16   1.32782250e+11   2.43865789e+10\n",
      "    3.22599310e+08  -1.35320470e+07   1.21505499e+07   2.98722066e+11\n",
      "    4.07749126e+09   5.75703340e+09   2.35809854e+12   1.16525902e+08]\n",
      " [  0.00000000e+00   1.81980468e+08   5.22262928e+07   2.18868848e+07\n",
      "    7.93802962e+02   1.32782250e+11   2.69693910e+08   1.22113720e+07\n",
      "    6.96350010e+04   3.80398095e+03   3.20115608e+03   1.33172126e+08\n",
      "    8.23870765e+05   1.79226974e+06   9.77740121e+08   3.41339342e+04]\n",
      " [  0.00000000e+00   6.52369507e+06   3.24249923e+06   1.56278201e+06\n",
      "    6.40106140e+01   2.43865789e+10   1.22113720e+07   1.31116852e+07\n",
      "    2.09802213e+04  -4.96757346e+02   1.19718307e+03   2.41956967e+07\n",
      "    1.47650109e+05   5.92960403e+05   2.99363350e+08  -9.26899160e+02]\n",
      " [  0.00000000e+00   4.22500531e+04   1.67868336e+04   6.15158928e+03\n",
      "    1.32369467e+00   3.22599310e+08   6.96350010e+04   2.09802213e+04\n",
      "    4.86687622e+02   1.27685999e+00   9.72854727e+00   1.58269766e+05\n",
      "    8.07071488e+02   2.80960665e+03   1.27063412e+06  -1.50234853e+01]\n",
      " [  0.00000000e+00   1.85356522e+03   1.13838513e+03   6.12032037e+02\n",
      "    3.64026483e-02  -1.35320470e+07   3.80398095e+03  -4.96757346e+02\n",
      "    1.27685999e+00   4.15439152e+00  -9.95704229e-02   8.90081309e+02\n",
      "   -6.30151611e+00  -5.10208160e+01  -1.13968533e+04   1.09326591e+00]\n",
      " [  0.00000000e+00   2.13029925e+03   8.19892323e+02   2.01368117e+02\n",
      "    2.47596188e-02   1.21505499e+07   3.20115608e+03   1.19718307e+03\n",
      "    9.72854727e+00  -9.95704229e-02   1.24445177e+00   1.03552681e+04\n",
      "    6.65323848e+01   1.60604921e+02   9.80052574e+04   1.18598520e-01]\n",
      " [  0.00000000e+00   7.53286579e+07   3.35596377e+07   1.48834843e+07\n",
      "    1.67871485e+03   2.98722066e+11   1.33172126e+08   2.41956967e+07\n",
      "    1.58269766e+05   8.90081309e+02   1.03552681e+04   6.21706683e+08\n",
      "    2.68608770e+06   4.64193671e+06   2.39832316e+09   9.39556349e+04]\n",
      " [  0.00000000e+00   4.79767003e+05   2.04715219e+05   8.21644741e+04\n",
      "    1.19078478e+01   4.07749126e+09   8.23870765e+05   1.47650109e+05\n",
      "    8.07071488e+02  -6.30151611e+00   6.65323848e+01   2.68608770e+06\n",
      "    1.98169044e+04   3.12270385e+04   1.48013062e+07   6.56518648e+02]\n",
      " [  0.00000000e+00   9.43744905e+05   4.40418364e+05   2.32423551e+05\n",
      "    2.00917416e+01   5.75703340e+09   1.79226974e+06   5.92960403e+05\n",
      "    2.80960665e+03  -5.10208160e+01   1.60604921e+02   4.64193671e+06\n",
      "    3.12270385e+04   1.30327491e+05   5.11350885e+07   2.97527782e+02]\n",
      " [  0.00000000e+00   5.49929081e+08   2.38849574e+08   1.14003925e+08\n",
      "    8.15513024e+03   2.35809854e+12   9.77740121e+08   2.99363350e+08\n",
      "    1.27063412e+06  -1.13968533e+04   9.80052574e+04   2.39832316e+09\n",
      "    1.48013062e+07   5.11350885e+07   2.76955757e+10   1.48789394e+05]\n",
      " [  0.00000000e+00   2.20556742e+04   7.30532298e+03   2.89642442e+03\n",
      "    9.61845629e-01   1.16525902e+08   3.41339342e+04  -9.26899160e+02\n",
      "   -1.50234853e+01   1.09326591e+00   1.18598520e-01   9.39556349e+04\n",
      "    6.56518648e+02   2.97527782e+02   1.48789394e+05   9.74957557e+01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f721ffe2890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAADQCAYAAADBGRdTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF5FJREFUeJzt3XmUnFWZx/Fv9ZZOd6ebQBJZh/0RcUdUZBHCIqC4wOAM\nDFsOIot4mGEYRzZRQEHlhMUcUANCQJFBYGRUQKJARJgMnKDggvCwhR0kQLZOutNLzR/3re5Kp5Zb\n1fWmuzq/zzl9Uv1Wve+9Df30ve99730uiIiIiIiIiIiIiIiIiEgNZMa6ApUys/cBPwcudfcrS3xu\nY+AmYLm7fz45djZwQPKRBmBTd39nylUWGReaxroClTCzNmA2cHfEx68E7gV2zR1w94uAi5JrHQtM\nT6GaIuNSXQU70AscApyZO2BmOwNzgCywApjl7suAEwiBvuvIi5hZE3AKsE/6VRYZHxrGugKVcPcB\nd+9Nvs0m/84BTnT3/YHfAKcmn+2m+G3KYcCv864lMuHVW8teyEeAa8wMYBLwcMQ5xwMnplkpkfFm\nIgR7t7vPLPJeduQBM2sHtnT3F9Ktlsj4Ulfd+DwZhrvoj5nZQQBmdoSZ7TvicyO9H3gi5fqJjDt1\n9ejNzHYDrgZmAP3Am8DJwLeBQWAV8C+EgbpHgXZgY+BF4Ax3n29mhwH7ufup6/8nEBEREREREYlW\nV/fss29clD3mAEu9nKlT23j77VWpl6OyVFaty5oxo7NoTNfVaPyy7jXrpZympsb1Uo7KUlnrs6y6\nCvalKzThTaRaCnaRDURdBfuylb1ks+tMihORCHUV7AODWbp7+se6GiJ1qa6CHWD5ehqkE5loFOwi\nG4j6C/ZVCnaRatRfsKtlF6lK/QW7WnaRqtRfsKtlF6lKHQZ731hXQaQu1V+wqxsvUpX6C3Z140Wq\nUpfBrimzIpWrq2BvaW5kTf8gvX0DY10VkbpTV8G+0ZRJgLryItWor2DvaAE0Ii9SjToL9lZg/WWs\nEZlI6ivYk278Cj1+E6lYXQa77tlFKldXwd6V3LMvU8suUrG6CvapyT27WnaRytVVsA/dsyvYRSqW\n6pbNZnYZ8FHC1sn/6u6L8t47FTgKGAAWufvp5a6XC/Zlq/ToTaRSo2rZzezAEu/tDezg7rsDXwC+\nl/deF/AfwJ7uvhews5l9tFx5XR0aoBOp1mi78WeWeG9f4OcA7v4EMNXMOpL3epOvKWbWBLQRtl8u\nqWNyM40NGVb39tPXrymzIpVI8559U2BJ3vdvAJsBuHsP8A3gGWAx8IC7P13ugg0NGaa0NQOwQl15\nkYqkes8+QoZw746ZdQLnAgasAO4xs/e6+5/LXWTjrsksXbmGhpYmpk+fklpl07y2ylJZY1FWmsH+\nCqF1z9kceDV5/S7gWXd/C8DMHgB2BcoGe1tL2Ozu+ZeWslFrOtWfPn0Kb7yxIpVrqyyVNVZlpdmN\nnw8cDmBmuwAvu3t38t5i4F1m1pp8vyvwVMxFO9tzi2E0SCdSidRadndfaGaPmNmDhMdrp5rZccAy\nd7/dzC4B7jOzfuBBd38g5rpDwa5ZdCIVKRvsSet7ArClu59pZrsBjyaDbAeXOtfdzxpx6M95780F\n5lZa4c42LXMVqUZMN/4qYHvCozSAXYB5MDSqvl51qWUXqUpMsO+UzG7rBnD3q4AtUq1VCVPaw6M3\n3bOLVCYm2NfaI9nM2oHWIp9N3XA3XsEuUomYYL/FzO4BtjOzOcBjwE/TrVZx6saLVKfsAJ27zzGz\nh4B9gB5gnrs/knbFiuloayYDrFzVx8DgII0NdbVwT2TMlI0UM9sc+Ji7f9fdvwccZmZjds/e2NBA\n++RmsoSAF5E4Mc3idcBred//JTk2ZnJdeSWeFIkXE+yt7n5z7ht3vwloSa9K5eUm1mgxjEi8mBl0\nWTM7GFhA+ONwMDCYZqXK0ZRZkcrFBPsXgR8APyOsWvtf4MQ0K1VObpmruvEi8WJG458C9lsPdYmm\nx28ilYuZG78vcBqwMWFNOkDW3T+eZsVKyU2sUeJJkXgx3fgfAN8EXsg7NqZ7Jufu2ZU/XiReTLA/\n5+43pF6TCmiATqRyMcF+l5mdSBiNH5on7+7PplWpcjQ/XqRyMcH+b4Rue/7a9CywXSo1itDZPpx0\nMpvNkslkypwhIjGj8duMPGZme6RSm0jNTY1MntTE6t5+unv66ZjcPJbVEakLMaPxXcDRwCbJoVZg\nFiGB5JjpbGtmdW8/y7vXKNhFIsRMl70ZeC9wPDAFOAQ4Nc1KxdAgnUhlYoJ9krufDCx2968Qlroe\nkWqtIijxpEhlohbCmNlGQIOZTUtyvW+TbrXKU8suUpmY0fgbgOOAHwGPm9kSInO8l9nFdSvgJqAZ\n+IO7n1JJxYcev6llF4lStmV39++7+xXufj0hs+xRwOfKnVdqF9fEbOASd/8oMJAEfzS17CKVKdqy\nm9nx7n6tmV1IaJnzH2YfCpxX5tpr7eJqZlPNrMPdV5pZA7Anyb2/u3+50oorf7xIZUq17AN5/w4Q\nZs/lvmL2Sy66iyswnbCh42Vm9nszu6iSSoNWvolUqmjLnnTbAV5y92tqUNbQLq7J6y2Ay4HngTvM\n7JPufmfsxZQ/XqQyMQN0nzOzW919aYXXLrWL6xLgeXd/DiBJVf1uoGyw57awbZ8SUtcvX9XHtGkd\nNZ8yW6/b8qoslVVMTLBPBhab2ZNArhmNWc8+HzgfmDtyF1d37zezZ81sB3d/GvgQkbnoc1vYZrNZ\nWpoaWNM3wEuvLKW1pXZ7VNbztrwqS2UVExMhFxY4VnY9e7ldXAkLbOYlg3V/cvdfVlLxTCZDZ3sL\nS5b1sLx7TU2DXWQiilkIs8DMOgiZaiDMjb8R+HDEuaV2cX0G2Cu+quua0pYL9j5mTB3NlUQmvphN\nIv4TeAlw4BHgj8nXmFP+eJF4MdNlDwfeASx09+nAkcATqdYq0vC6dgW7SDkxwd7t7r0kG0O4+y+A\nT6daq0iaRScSL2ZU661kYO2vZnYd8DdCSz/mprQp8aRIrJiW/Rjgd8DpwNOEyTBHplmpWF1q2UWi\nxbTs1xJWvv3a3b+Vcn0qovzxIvFiWvZfAacAz5vZFWa2a8p1ijacP16LYUTKiVni+hN3/zQhNdUf\nga+Z2V9Sr1kEDdCJxItp2TGzDPBBwkSadwKPplmpWG2tTTQ2ZFjd209ff8xCPJENV8ykmh8CLwJn\nAA8Cu7j70WlXLEZDJjO0o6vWtYuUFjNA9yfgHHdfMvINMzsubynsmOhsb2HpyjUsX7WGTbpax7Iq\nIuNazNz4K0u8PQsY82AH3beLlBN1zz6ead83kTj1H+xKTyUSpf6DXYknRaLUfbAr8aRInNEG+7Ka\n1GIUlHhSJE6pvPFfz/s2P298FsDdL3D3sptFpE0DdCJxSj16ayYE9o7J1/3J5/cG/pB+1eKoGy8S\np1Te+HMBzOyXwEfcfSD5vhn42fqpXnkdbc1kgJWr+hgYHKSxoe6HIURSERMZWxX43NYp1KUqjQ0N\ntE9uJksIeBEpLGa67B2Am9kjwCBhc8fbU61VhbraW1i5uo9l3Wvo6pg01tURGZdipsueY2bXA+8h\nDNJ9w90fj7l4qS2b8z5zMbCbu8+sqOZ5OttbeHlJNyvUsosUFbPqrRX4BOG+/TagMzlW7rxyWzZj\nZjsTcseX3XSiFM2PFykv5p79KmB7INfy7gLMizhvrS2bganJZhP5LgHOZu3toCuWW+aq/PEixcUE\n+07ufjqwCsDdryIknSyn1JbNmNks4F7CLq6josdvIuXFDND1539jZu2ELaAqNbRls5ltDBwNHEgY\n7Y9WaFfLLd7RCcCagWzNdr2s1506VZbKKiYm2G9JtlTezszmAAcDpda455Tasnlm8t4DwCRgezOb\n7e5nlLtooV0tM4ODAPz9re6a7HpZzzt1qiyVVUzMaPwcM3sI2AfoAea5+yMR1y61ZfNtwG0AZrZ1\ncs2ygV6MBuhEyoudbrYGWAT8Fegys33LneDuC4Hcls2Xk2zZbGYj59MPde+rpfnxIuWVbdnN7Dbg\n/YSkk/nuLXduqS2b8z6zmDByX7XhDR77yGazZDKjGtwXmZBi7tm3cfcdUq/JKDQ3NTJ5UhOre/vp\n7umnY3LzWFdJZNyJ6cY/aWbjfg5qZ5vWtYuUEtOyDwKPm9nDDD+Gy7r7selVq3Kd7S28/vZqlnev\nYfNp7WNdHZFxJybYf5t85RvVgFoalHhSpLRSmWo2c/dXgd+zdqYaGM/Brm68SEGlWvZLCfuw38O6\nwZ0FtkurUtUYevymll2koFKZao5M/t1m5HtmtkeKdaqKWnaR0mKes3cR5rFvkhxqJWz7tHl61aqc\n8seLlBbz6O1mwt7sxwNTgEOAU9OsVDW08k2ktJhgn+TuJwOL3f0rhDnyR6Raqyoof7xIaTHB3mpm\nGwENZjbN3d8Ctkm3WpXLnx+fzY67hwUiYy4m2G8AjgN+RJhc8zjwWqq1qkJrSyMtTQ2s6R+kt29g\nrKsjMu7ELHH9fu51sq59OvBompWqRiaTobO9hSXLeljevYbWlpj5QiIbjlKTai4scd6hwHm1r87o\nDAd7HzOmjnVtRMaXUs3fAIVnyo16/XlacvftSjwpsq5Sk2q+kXud5IzbkRDkT7r7mO/eWkhuXbse\nv4msKyZv/OnAU4RsM98DnjGzL6VdsWrkZtGtUMsuso6YUaxZwHa51jxp5e8j5JMfV4a68WrZRdYR\n8+jt1fxue/Kc/Zn0qlQ9zY8XKS6mZX/GzG4nZIttJKSBfsvMjgdw92tTrF9FlHhSpLiYYG8HlgIf\nTr5fTgj6vZLvx0+wD82P12IYkZFigv0cd385/4CZfcDdy06sKbWLq5nNBC4iPOJ7EjjB3bXBo0hK\nYu7ZF5jZUQBm1mhm5wI/LXdSxC6uc4HD3X1Pwmq6gyqqeQFtrU00NmRY3dtPX7+mzIrkiwn2jwH7\nJ/njFxC69btEnFduF9cP5fUY3gA2jq10MQ2ZzNCOrlrXLrK2ssHu7ksIg3M7AxsBd7l7T8S1S+7i\n6u7LIeS6I+z/fmd8tYtT4kmRwmIy1dxNCNTdgS7gajNb7O5frLCsdabZmtkM4BfAKe7+dsxFyu1q\nOW1qGy+8vpJMU+OodsCs1506VZbKKiZmgO56Qhf7q+5+ppmdB3wg4rxSu7hiZp2E1vxsdx+Zqrqo\ncrtatjaFzsqLryxjm+nV5Y+v5506VZbKKibmnn1/QibZ3H5sHwT2jjhvPnA4wMhdXBOzgcvcfX58\ndctTN16ksJiWfSd3393M7gNw96vM7MhyJ7n7QjPL7eI6QLKLK7AMuBs4BtjBzE5ITvmpu19d3Y8x\nTIknRQqLCfb+/G/MrJ2QYbasMru4Rl2jUko8KVJYTDf+liRDzXZmNgd4jIjn7GNFiSdFCotJSzXH\nzB4iZJXtAea5+yNpV6xamh8vUlhUojZ3fxh4OOW61IS68SKFxXTj60pHWzMZYOWqPgYGB8e6OiLj\nxoQL9saGBtonN5MlBLyIBBMu2GG4K6/EkyLDJmSwD+WiU8suMmRCB7tG5EWGTchgzy1zVTdeZNiE\nDHY9fhNZ14QM9tzEGuWPFxk2MYO9XfnjRUaa0MGuATqRYRMz2DU/XmQdEzPYk5VvK1b1kc2Oyw1n\nRda7CRnszU2NTJ7UxMBglu6e/vIniGwAJmSwg+7bRUaauMHepiQWIvkmbrBrYo3IWiZ+sKtlFwEm\ncLB3tallF8kXlZaqWmV2cd0f+BYhzfSd7v7NWpY9RS27yFpSa9kjdnG9AjgM2AP4hJm9q5blK3+8\nyNrS7MYX3cXVzLYD3nL3l5M92e8E9qtl4Vr5JrK2NLvxmwL5KaffSI49nfz7Rt57fwe2r2XhuVl0\nb6/oZfFryys6d1nPAG8v7S7/wRpQWSprfZWV6j37CJkq36vKlKQb//aKXi6Yt6jMp0UmvjSDvdQu\nri+PeG/L5FhJmUym5n8URDYUad6zF93F1d2fBzrNbGszawI+lXxeRFKSaktpZhcDHyfZxRXYBVjm\n7reb2V7Ad5KP3urul6ZZFxERERERERERkfGoLh5llZpjn0JZ3wX2JDyWvNjdf55WWUl5k4G/ABe4\n+/UplnMU8BWgHzjP3e9MqZwO4AZgI2AScL671/RJi5m9jzA781J3v9LMtgJ+THi69CpwjLvXZOpk\nkbKuI/x+9AFHu/vraZSVd/xA4C53H9XTs3G/6i1ijn0ty5oJvDsp6yDg8rTKynMu8CbhD1kqzGwT\n4DzCOoRDgM+mVRYwC3jC3fclPHq9opYXN7M2YDZwN8P/zS4A5rj7xwkzNI9PsawLgbnuvg8hMP89\nhbLyj7cCZxHmrYzKuA92SsyxT8H9wD8lr5cB7WaWWu/HzHYCdgLuIN1e1v7Ab929291fc/eTUizr\ndWCT5PXGrD0tuhZ6CX+w8lvTvYFfJK9/Sfh5a11W7v/PqcBtyeslDP+stSwr39nAHEIvYlTqIdg3\nJfxHzXkD2CyNgtx9IDfxh9CLuCNZqJOWS4DTU7x+ztZAm5n9j5ndb2b7plWQu98CbGVmTwELqFHL\nl3f9AXfvHXG43d1zwVCz349CZSV/MAfMrBH4EnBjWmWZmQE7u/ttRU6rSD0E+0gZUuzyApjZZwld\nwS+nWMaxwP3u/gLpj500EFrZQwnd7OvSKsjMjgZecPcdCSsZryxzSq2lPg6VBPqPgXvc/b4Uisj9\nfs8GzqjVResh2EvNsa+5ZDDkLOAgd1+RVjnAJ4HPm9lCQi/iaym2uK8BC9190N2fBVaY2bSUytqd\nZOqzu/8J2DLNW6HESjOblLzeghrc35ZxHfCku1+YVgFmtjnhFu+/kt+RzcxsVH9Y1ueqt2rNB84H\n5o6cY19rZtZF6Frv6+5L0ygjx92PyCv368Bz7n5vSsXNB+aZ2XcILXyHuy8pc061niY8OflvM9sa\n6E7pVijDcCv+W8Jg4I3APwJ3pVAWMPRUo9fdz69xGfllZdz9FWDHvHKfc/eZo7nwuA92d19oZo+Y\n2YMMz7FPyz8TBlxuCbdLABzr7i+mWGbq3P0VM7sV+L/kUGq3J8APgWvNbAHh9+vEWl7czHYDrgZm\nAP1mdhLhycm85PVioCaPMAuUdTLQCKzOa2Ufd/dR/04W+bn2cfe3ko9oayMRERERERERERERERER\nEamImU02s0PHuh71qh6my4rk7ELYMkyqUBfJK2SYme0DnAm8CLybsPTxIHdfXeTzhxDWsvcADpwE\ntAJzCfn6m4Eb3P0HZjaLMBsNQmD9hJCAYh/C78r+hBle9xC27Hp/8tkjkll6nwK+BqxKvk5Mji8m\n5AY4GNgWONnd7zWzfyAslGkDOoCz3f0eM5tH2EfgvYABPyIs83yUkBRjXlK3ucnP1UZI/pFKQo6J\nQi17fdoNOCtJsjEAHFjoQ0lChKuBg5PEDksICSxOI+y1tzchX8BXzWzb5LQPAccABxD+SNzt7nsQ\n1lsfkHxmW+Da5JoLgDOSjDtXA4cliSvuAnI782aBVe5+YHLstOT494HZ7r4fIaHGNcmKMoBt3f0z\nwCeAc9y9B7gYmO/uZwJfBG5Pyvo0ML3S/4gbGgV7ffpb3kKW5wmLWwrZGXjR3d8EcPcz3f1+4CPA\nb5JjPcAiQkueBRYla8NfJvx+PJBc6yWgM3n9prv/MXn9YFKOAa8nCzgAfgd8OK8uC5J/X8ir70zg\n/GSe+U3AGkLPIZv7fLIEuDNZOZe/+OVW4CQzuxLYNc2UXhPFuF8IIwX1j/i+2O1YlsJ/0LMjzsnP\nEbDWtd19sEA5+ddsSM4duVBjZN6B/hHvQeiCH5q32AOAZBHSQIHr5dfr92b2HsKa+VlmdrS7H4UU\npZZ9YnsC2MLMtgAws8vN7DOE1W8HJsfaCV33RcSP4Uw1sw8kr/cEHiOMB8xIEjJCuL9fWOY6DxBW\nGmJm05LEoqUMEsYYMLMvA1u6+6+AEwjLaqUEBXv9KdSKFlz+mKz7/wJwm5ndD3QBvyIMdk0xs98R\nBtvOT7rLI69d6LpZQhf/WDO7B/gYcFlyO/AF4OakWz6TkEyzVP1PAw5N6nZHUpdCZedePwR83Myu\nIfwhu8nM7k1+pq8W+m8gIlUys23MrK7X92+odM9e55JR8GKPnL7t7ncXeW80lEhBRERERERERERE\nRERERDZU/w+mz+PcNwkITQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f721ff881d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 1.13363171,  1.33126267,  1.57173642,  0.06460166,  0.11447104,\n",
       "         0.07637501,  0.02419416,  0.0372657 ,  0.03928272,  0.07716775,\n",
       "         0.10341573,  0.18675931,  0.86450362,  1.53046759,  1.67597365,\n",
       "         0.06485335,  0.08571259,  0.07520405,  0.02621667,  0.03569531,\n",
       "         0.04265634,  0.09996661,  0.12033168,  0.19553073,  0.9095126 ,\n",
       "         1.26916663,  1.50300701,  0.06849567,  0.09306534,  0.08328168,\n",
       "         0.02967238,  0.04749926,  0.04508734,  0.0914766 ,  0.10709031,\n",
       "         0.173383  ]),\n",
       " 'mean_score_time': array([ 0.00065295,  0.00065804,  0.00075793,  0.00057236,  0.00084503,\n",
       "         0.00074967,  0.00062974,  0.00070103,  0.00081762,  0.00061528,\n",
       "         0.00067321,  0.00119178,  0.00064206,  0.00062672,  0.00074768,\n",
       "         0.00060701,  0.000681  ,  0.00078591,  0.00075237,  0.00068069,\n",
       "         0.00082429,  0.00073163,  0.00067337,  0.00084027,  0.00060169,\n",
       "         0.0006477 ,  0.00075324,  0.00063101,  0.00068474,  0.00078924,\n",
       "         0.00082095,  0.00141637,  0.00080935,  0.0006237 ,  0.00067194,\n",
       "         0.00076826]),\n",
       " 'mean_test_score': array([ 0.36197441,  0.33942718,  0.42474101,  0.35770871,  0.34978672,\n",
       "         0.34673979,  0.34003656,  0.34003656,  0.3345521 ,  0.34491164,\n",
       "         0.34491164,  0.34491164,  0.45947593,  0.40036563,  0.44667885,\n",
       "         0.35770871,  0.34673979,  0.34734918,  0.34003656,  0.33759902,\n",
       "         0.34064595,  0.34491164,  0.34491164,  0.34491164,  0.40097502,\n",
       "         0.43753809,  0.41864717,  0.35770871,  0.34673979,  0.34734918,\n",
       "         0.34003656,  0.33516149,  0.3345521 ,  0.34491164,  0.34491164,\n",
       "         0.34491164]),\n",
       " 'mean_train_score': array([ 0.38535012,  0.33728726,  0.45249332,  0.38052882,  0.38662657,\n",
       "         0.3863216 ,  0.35282937,  0.34673831,  0.34216625,  0.37508305,\n",
       "         0.37508305,  0.37508305,  0.46647854,  0.40193303,  0.47046432,\n",
       "         0.38052882,  0.3863216 ,  0.38601663,  0.35282937,  0.3464339 ,\n",
       "         0.34643557,  0.37508305,  0.37508305,  0.37508305,  0.3991947 ,\n",
       "         0.44241787,  0.46862363,  0.38052882,  0.38662657,  0.38601663,\n",
       "         0.35282937,  0.34399413,  0.34338447,  0.37508305,  0.37508305,\n",
       "         0.37508305]),\n",
       " 'param_logistic__C': masked_array(data = [0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
       "  0.0001 0.0001 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 10000.0\n",
       "  10000.0 10000.0 10000.0 10000.0 10000.0 10000.0 10000.0 10000.0 10000.0\n",
       "  10000.0 10000.0],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_logistic__solver': masked_array(data = ['newton-cg' 'newton-cg' 'newton-cg' 'lbfgs' 'lbfgs' 'lbfgs' 'liblinear'\n",
       "  'liblinear' 'liblinear' 'sag' 'sag' 'sag' 'newton-cg' 'newton-cg'\n",
       "  'newton-cg' 'lbfgs' 'lbfgs' 'lbfgs' 'liblinear' 'liblinear' 'liblinear'\n",
       "  'sag' 'sag' 'sag' 'newton-cg' 'newton-cg' 'newton-cg' 'lbfgs' 'lbfgs'\n",
       "  'lbfgs' 'liblinear' 'liblinear' 'liblinear' 'sag' 'sag' 'sag'],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_pca__n_components': masked_array(data = [4 8 16 4 8 16 4 8 16 4 8 16 4 8 16 4 8 16 4 8 16 4 8 16 4 8 16 4 8 16 4 8\n",
       "  16 4 8 16],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'logistic__C': 0.0001,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 4},\n",
       "  {'logistic__C': 0.0001,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 8},\n",
       "  {'logistic__C': 0.0001,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 16},\n",
       "  {'logistic__C': 0.0001, 'logistic__solver': 'lbfgs', 'pca__n_components': 4},\n",
       "  {'logistic__C': 0.0001, 'logistic__solver': 'lbfgs', 'pca__n_components': 8},\n",
       "  {'logistic__C': 0.0001,\n",
       "   'logistic__solver': 'lbfgs',\n",
       "   'pca__n_components': 16},\n",
       "  {'logistic__C': 0.0001,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 4},\n",
       "  {'logistic__C': 0.0001,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 8},\n",
       "  {'logistic__C': 0.0001,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 16},\n",
       "  {'logistic__C': 0.0001, 'logistic__solver': 'sag', 'pca__n_components': 4},\n",
       "  {'logistic__C': 0.0001, 'logistic__solver': 'sag', 'pca__n_components': 8},\n",
       "  {'logistic__C': 0.0001, 'logistic__solver': 'sag', 'pca__n_components': 16},\n",
       "  {'logistic__C': 1.0,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 4},\n",
       "  {'logistic__C': 1.0,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 8},\n",
       "  {'logistic__C': 1.0,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 16},\n",
       "  {'logistic__C': 1.0, 'logistic__solver': 'lbfgs', 'pca__n_components': 4},\n",
       "  {'logistic__C': 1.0, 'logistic__solver': 'lbfgs', 'pca__n_components': 8},\n",
       "  {'logistic__C': 1.0, 'logistic__solver': 'lbfgs', 'pca__n_components': 16},\n",
       "  {'logistic__C': 1.0,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 4},\n",
       "  {'logistic__C': 1.0,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 8},\n",
       "  {'logistic__C': 1.0,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 16},\n",
       "  {'logistic__C': 1.0, 'logistic__solver': 'sag', 'pca__n_components': 4},\n",
       "  {'logistic__C': 1.0, 'logistic__solver': 'sag', 'pca__n_components': 8},\n",
       "  {'logistic__C': 1.0, 'logistic__solver': 'sag', 'pca__n_components': 16},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 4},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 8},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'newton-cg',\n",
       "   'pca__n_components': 16},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'lbfgs',\n",
       "   'pca__n_components': 4},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'lbfgs',\n",
       "   'pca__n_components': 8},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'lbfgs',\n",
       "   'pca__n_components': 16},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 4},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 8},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'liblinear',\n",
       "   'pca__n_components': 16},\n",
       "  {'logistic__C': 10000.0, 'logistic__solver': 'sag', 'pca__n_components': 4},\n",
       "  {'logistic__C': 10000.0, 'logistic__solver': 'sag', 'pca__n_components': 8},\n",
       "  {'logistic__C': 10000.0,\n",
       "   'logistic__solver': 'sag',\n",
       "   'pca__n_components': 16}),\n",
       " 'rank_test_score': array([ 8, 32,  4,  9, 12, 15, 28, 28, 35, 18, 18, 18,  1,  7,  2,  9, 15,\n",
       "        13, 28, 33, 27, 18, 18, 18,  6,  3,  5,  9, 15, 13, 28, 34, 35, 18,\n",
       "        18, 18], dtype=int32),\n",
       " 'split0_test_score': array([ 0.20620438,  0.27737226,  0.48175182,  0.27007299,  0.25547445,\n",
       "         0.24635036,  0.30109489,  0.29379562,  0.28649635,  0.13686131,\n",
       "         0.13686131,  0.13686131,  0.48905109,  0.46350365,  0.48175182,\n",
       "         0.27007299,  0.24635036,  0.24817518,  0.30109489,  0.29014599,\n",
       "         0.30291971,  0.13686131,  0.13686131,  0.13686131,  0.5       ,\n",
       "         0.46715328,  0.39963504,  0.27007299,  0.24635036,  0.24817518,\n",
       "         0.30109489,  0.28284672,  0.28467153,  0.13686131,  0.13686131,\n",
       "         0.13686131]),\n",
       " 'split0_train_score': array([ 0.19670631,  0.28819762,  0.48673376,  0.33028362,  0.34583715,\n",
       "         0.34492223,  0.33668801,  0.33943275,  0.33211345,  0.36596523,\n",
       "         0.36596523,  0.36596523,  0.45105215,  0.4547118 ,  0.48124428,\n",
       "         0.33028362,  0.34492223,  0.34400732,  0.33668801,  0.33943275,\n",
       "         0.34400732,  0.36596523,  0.36596523,  0.36596523,  0.4547118 ,\n",
       "         0.45196706,  0.45837145,  0.33028362,  0.34583715,  0.34400732,\n",
       "         0.33668801,  0.33211345,  0.33211345,  0.36596523,  0.36596523,\n",
       "         0.36596523]),\n",
       " 'split1_test_score': array([ 0.45521024,  0.42047532,  0.41681901,  0.34734918,  0.34369287,\n",
       "         0.34369287,  0.34003656,  0.33272395,  0.32175503,  0.53016453,\n",
       "         0.53016453,  0.53016453,  0.45155393,  0.44606947,  0.46800731,\n",
       "         0.34734918,  0.34369287,  0.34369287,  0.34003656,  0.32723949,\n",
       "         0.32358318,  0.53016453,  0.53016453,  0.53016453,  0.44606947,\n",
       "         0.44241316,  0.45703839,  0.34734918,  0.34369287,  0.34369287,\n",
       "         0.34003656,  0.32723949,  0.32906764,  0.53016453,  0.53016453,\n",
       "         0.53016453]),\n",
       " 'split1_train_score': array([ 0.48171846,  0.41224863,  0.46709324,  0.37659963,  0.37659963,\n",
       "         0.37659963,  0.37111517,  0.35648995,  0.35191956,  0.41590494,\n",
       "         0.41590494,  0.41590494,  0.47989031,  0.44058501,  0.51279707,\n",
       "         0.37659963,  0.37659963,  0.37659963,  0.37111517,  0.35648995,\n",
       "         0.35283364,  0.41590494,  0.41590494,  0.41590494,  0.44515539,\n",
       "         0.43875686,  0.511883  ,  0.37659963,  0.37659963,  0.37659963,\n",
       "         0.37111517,  0.35648995,  0.35374771,  0.41590494,  0.41590494,\n",
       "         0.41590494]),\n",
       " 'split2_test_score': array([ 0.42490842,  0.32051282,  0.37545788,  0.45604396,  0.45054945,\n",
       "         0.45054945,  0.37912088,  0.39377289,  0.3956044 ,  0.36813187,\n",
       "         0.36813187,  0.36813187,  0.43772894,  0.29120879,  0.39010989,\n",
       "         0.45604396,  0.45054945,  0.45054945,  0.37912088,  0.3956044 ,\n",
       "         0.3956044 ,  0.36813187,  0.36813187,  0.36813187,  0.25641026,\n",
       "         0.4029304 ,  0.3992674 ,  0.45604396,  0.45054945,  0.45054945,\n",
       "         0.37912088,  0.3956044 ,  0.39010989,  0.36813187,  0.36813187,\n",
       "         0.36813187]),\n",
       " 'split2_train_score': array([ 0.47762557,  0.31141553,  0.40365297,  0.4347032 ,  0.43744292,\n",
       "         0.43744292,  0.35068493,  0.34429224,  0.34246575,  0.343379  ,\n",
       "         0.343379  ,  0.343379  ,  0.46849315,  0.31050228,  0.4173516 ,\n",
       "         0.4347032 ,  0.43744292,  0.43744292,  0.35068493,  0.343379  ,\n",
       "         0.34246575,  0.343379  ,  0.343379  ,  0.343379  ,  0.29771689,\n",
       "         0.43652968,  0.43561644,  0.4347032 ,  0.43744292,  0.43744292,\n",
       "         0.35068493,  0.343379  ,  0.34429224,  0.343379  ,  0.343379  ,\n",
       "         0.343379  ]),\n",
       " 'std_fit_time': array([ 0.16441728,  0.12234192,  0.06696524,  0.00456824,  0.04305667,\n",
       "         0.01221549,  0.00131356,  0.00294582,  0.0040212 ,  0.00284398,\n",
       "         0.00381601,  0.02679788,  0.07168915,  0.02594366,  0.15451881,\n",
       "         0.00375347,  0.0240767 ,  0.0114906 ,  0.00132406,  0.00286567,\n",
       "         0.00758779,  0.00600254,  0.01488072,  0.02414513,  0.29983591,\n",
       "         0.08969369,  0.01140591,  0.0060176 ,  0.01454028,  0.01988471,\n",
       "         0.00381276,  0.00055928,  0.00812565,  0.01352996,  0.00502595,\n",
       "         0.02137762]),\n",
       " 'std_score_time': array([  6.32719767e-05,   1.55734364e-05,   2.89447094e-05,\n",
       "          4.58331771e-06,   2.87724857e-04,   2.16985650e-05,\n",
       "          4.73272724e-05,   3.02088874e-05,   7.61324279e-05,\n",
       "          2.20708594e-05,   1.44439600e-05,   3.26682893e-04,\n",
       "          5.38048281e-05,   4.74580085e-06,   4.10190833e-06,\n",
       "          2.41239077e-05,   5.39185724e-05,   2.09386478e-05,\n",
       "          1.63897971e-04,   2.79245057e-05,   1.00255572e-05,\n",
       "          1.34699159e-04,   1.22969966e-05,   3.78939729e-05,\n",
       "          5.96946461e-06,   8.96600391e-06,   8.87893609e-06,\n",
       "          4.39295892e-05,   2.45490107e-05,   4.47318552e-05,\n",
       "          1.13319046e-04,   9.98117493e-04,   6.91642212e-06,\n",
       "          2.34508236e-05,   1.91887051e-05,   2.36000943e-05]),\n",
       " 'std_test_score': array([ 0.11098803,  0.05995485,  0.04375431,  0.07627477,  0.07975539,\n",
       "         0.08339162,  0.03185392,  0.04114172,  0.04545294,  0.16147332,\n",
       "         0.16147332,  0.16147332,  0.02168805,  0.07740795,  0.040338  ,\n",
       "         0.07627477,  0.08339162,  0.08265925,  0.03185392,  0.04367188,\n",
       "         0.03971531,  0.16147332,  0.16147332,  0.16147332,  0.10443203,\n",
       "         0.02644448,  0.02714711,  0.07627477,  0.08339162,  0.08265925,\n",
       "         0.03185392,  0.04637263,  0.0432193 ,  0.16147332,  0.16147332,\n",
       "         0.16147332]),\n",
       " 'std_train_score': array([ 0.13340178,  0.05384653,  0.03545394,  0.04271956,  0.03806406,\n",
       "         0.0383919 ,  0.01413639,  0.00717516,  0.00808858,  0.03030241,\n",
       "         0.03030241,  0.03030241,  0.011859  ,  0.06490803,  0.03970403,\n",
       "         0.04271956,  0.0383919 ,  0.03872177,  0.01413639,  0.00729092,\n",
       "         0.00456768,  0.03030241,  0.03030241,  0.03030241,  0.07186162,\n",
       "         0.00681324,  0.0319685 ,  0.04271956,  0.03806406,  0.03872177,\n",
       "         0.01413639,  0.00996116,  0.00885545,  0.03030241,  0.03030241,\n",
       "         0.03030241])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>budget</th>\n",
       "      <th>cast_total_facebook_likes</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>duration</th>\n",
       "      <th>facenumber_in_poster</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>num_voted_users</th>\n",
       "      <th>title_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.158703e-07</td>\n",
       "      <td>4.122498e-07</td>\n",
       "      <td>1.861868e-07</td>\n",
       "      <td>3.739348e-12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.453466e-07</td>\n",
       "      <td>3.245581e-09</td>\n",
       "      <td>-1.361421e-10</td>\n",
       "      <td>1.222433e-10</td>\n",
       "      <td>3.005359e-06</td>\n",
       "      <td>4.102250e-08</td>\n",
       "      <td>5.791991e-08</td>\n",
       "      <td>2.372417e-05</td>\n",
       "      <td>1.172334e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.014700e-02</td>\n",
       "      <td>8.681576e-03</td>\n",
       "      <td>4.134552e-03</td>\n",
       "      <td>2.973127e-07</td>\n",
       "      <td>-2.394080e-05</td>\n",
       "      <td>0.035679</td>\n",
       "      <td>1.075947e-02</td>\n",
       "      <td>4.567515e-05</td>\n",
       "      <td>-3.856553e-07</td>\n",
       "      <td>3.524143e-06</td>\n",
       "      <td>8.748485e-02</td>\n",
       "      <td>5.344004e-04</td>\n",
       "      <td>1.836914e-03</td>\n",
       "      <td>9.952164e-01</td>\n",
       "      <td>5.563669e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.044229e-01</td>\n",
       "      <td>8.278524e-02</td>\n",
       "      <td>3.203335e-02</td>\n",
       "      <td>2.562708e-06</td>\n",
       "      <td>-1.042500e-06</td>\n",
       "      <td>0.438417</td>\n",
       "      <td>-2.163424e-03</td>\n",
       "      <td>1.223984e-04</td>\n",
       "      <td>9.407970e-06</td>\n",
       "      <td>2.998838e-06</td>\n",
       "      <td>8.354509e-01</td>\n",
       "      <td>3.008434e-03</td>\n",
       "      <td>1.591069e-04</td>\n",
       "      <td>-9.615447e-02</td>\n",
       "      <td>1.936205e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.985078e-01</td>\n",
       "      <td>1.039038e-01</td>\n",
       "      <td>4.008251e-02</td>\n",
       "      <td>2.575750e-08</td>\n",
       "      <td>8.255882e-08</td>\n",
       "      <td>0.667093</td>\n",
       "      <td>7.756734e-03</td>\n",
       "      <td>-7.160618e-07</td>\n",
       "      <td>9.142119e-06</td>\n",
       "      <td>-3.345472e-06</td>\n",
       "      <td>-5.420852e-01</td>\n",
       "      <td>-1.402834e-03</td>\n",
       "      <td>-4.657174e-04</td>\n",
       "      <td>1.248983e-02</td>\n",
       "      <td>-4.435336e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.593528e-01</td>\n",
       "      <td>-5.909469e-01</td>\n",
       "      <td>-2.810301e-01</td>\n",
       "      <td>1.234039e-06</td>\n",
       "      <td>2.138027e-07</td>\n",
       "      <td>-0.365832</td>\n",
       "      <td>-5.231903e-02</td>\n",
       "      <td>-3.496958e-05</td>\n",
       "      <td>-4.277794e-05</td>\n",
       "      <td>1.342436e-05</td>\n",
       "      <td>2.146841e-02</td>\n",
       "      <td>-3.230345e-04</td>\n",
       "      <td>-1.891060e-03</td>\n",
       "      <td>4.772002e-03</td>\n",
       "      <td>-1.150953e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.300228e-02</td>\n",
       "      <td>-3.064034e-02</td>\n",
       "      <td>-1.040459e-02</td>\n",
       "      <td>-2.180755e-06</td>\n",
       "      <td>2.054628e-08</td>\n",
       "      <td>-0.025629</td>\n",
       "      <td>9.985163e-01</td>\n",
       "      <td>7.409864e-04</td>\n",
       "      <td>-4.539018e-05</td>\n",
       "      <td>1.674921e-05</td>\n",
       "      <td>6.205378e-03</td>\n",
       "      <td>-7.913783e-04</td>\n",
       "      <td>3.996753e-03</td>\n",
       "      <td>-1.078638e-02</td>\n",
       "      <td>-2.388906e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.961297e-01</td>\n",
       "      <td>-6.732913e-01</td>\n",
       "      <td>6.807274e-01</td>\n",
       "      <td>-1.138203e-06</td>\n",
       "      <td>1.503561e-09</td>\n",
       "      <td>0.211427</td>\n",
       "      <td>-1.715132e-03</td>\n",
       "      <td>-5.116579e-04</td>\n",
       "      <td>3.766206e-05</td>\n",
       "      <td>-7.932303e-05</td>\n",
       "      <td>1.061799e-03</td>\n",
       "      <td>-3.272745e-03</td>\n",
       "      <td>1.025700e-02</td>\n",
       "      <td>-6.559790e-04</td>\n",
       "      <td>-1.928598e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.283873e-01</td>\n",
       "      <td>-4.220107e-01</td>\n",
       "      <td>-6.739200e-01</td>\n",
       "      <td>1.040549e-05</td>\n",
       "      <td>-3.993162e-09</td>\n",
       "      <td>0.426327</td>\n",
       "      <td>4.937761e-03</td>\n",
       "      <td>9.645712e-04</td>\n",
       "      <td>-2.592849e-04</td>\n",
       "      <td>-1.650957e-04</td>\n",
       "      <td>-2.791878e-05</td>\n",
       "      <td>8.472064e-03</td>\n",
       "      <td>4.869104e-02</td>\n",
       "      <td>-2.760921e-04</td>\n",
       "      <td>1.559211e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.434902e-02</td>\n",
       "      <td>2.646093e-02</td>\n",
       "      <td>2.610746e-02</td>\n",
       "      <td>1.462370e-04</td>\n",
       "      <td>-1.599340e-08</td>\n",
       "      <td>-0.023615</td>\n",
       "      <td>-4.231637e-03</td>\n",
       "      <td>1.165591e-02</td>\n",
       "      <td>-8.107269e-04</td>\n",
       "      <td>-4.626777e-04</td>\n",
       "      <td>-9.108129e-04</td>\n",
       "      <td>1.122760e-01</td>\n",
       "      <td>9.923219e-01</td>\n",
       "      <td>-1.752186e-03</td>\n",
       "      <td>6.985896e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.680369e-04</td>\n",
       "      <td>1.928329e-03</td>\n",
       "      <td>-4.884405e-03</td>\n",
       "      <td>-5.227151e-04</td>\n",
       "      <td>2.318769e-08</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>-1.239935e-03</td>\n",
       "      <td>1.620013e-02</td>\n",
       "      <td>7.727561e-04</td>\n",
       "      <td>-1.456072e-03</td>\n",
       "      <td>3.226398e-03</td>\n",
       "      <td>-9.925635e-01</td>\n",
       "      <td>1.122447e-01</td>\n",
       "      <td>3.469544e-05</td>\n",
       "      <td>-4.373986e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.198878e-06</td>\n",
       "      <td>-2.937556e-04</td>\n",
       "      <td>7.729956e-04</td>\n",
       "      <td>1.871200e-03</td>\n",
       "      <td>-2.225583e-09</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-6.909812e-04</td>\n",
       "      <td>9.965626e-01</td>\n",
       "      <td>4.683693e-03</td>\n",
       "      <td>1.287961e-02</td>\n",
       "      <td>-1.477591e-04</td>\n",
       "      <td>1.815117e-02</td>\n",
       "      <td>-1.371200e-02</td>\n",
       "      <td>-6.863832e-06</td>\n",
       "      <td>-7.844070e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.244571e-05</td>\n",
       "      <td>-7.034178e-05</td>\n",
       "      <td>6.516237e-05</td>\n",
       "      <td>-9.357805e-03</td>\n",
       "      <td>-2.690479e-11</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>-1.335895e-04</td>\n",
       "      <td>-7.925557e-02</td>\n",
       "      <td>-1.297654e-02</td>\n",
       "      <td>1.114316e-02</td>\n",
       "      <td>5.464739e-05</td>\n",
       "      <td>4.221002e-02</td>\n",
       "      <td>-3.146010e-03</td>\n",
       "      <td>-1.343884e-05</td>\n",
       "      <td>-9.957644e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.983098e-05</td>\n",
       "      <td>-9.004264e-05</td>\n",
       "      <td>-1.901127e-04</td>\n",
       "      <td>6.592589e-03</td>\n",
       "      <td>1.239297e-10</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>4.360332e-05</td>\n",
       "      <td>-5.468034e-03</td>\n",
       "      <td>9.996662e-01</td>\n",
       "      <td>-2.066455e-02</td>\n",
       "      <td>-3.557609e-06</td>\n",
       "      <td>1.365169e-03</td>\n",
       "      <td>7.352922e-04</td>\n",
       "      <td>-6.706338e-07</td>\n",
       "      <td>-1.282983e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>8.512175e-05</td>\n",
       "      <td>9.685728e-05</td>\n",
       "      <td>6.298768e-05</td>\n",
       "      <td>1.867714e-02</td>\n",
       "      <td>-1.922524e-11</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>7.860101e-06</td>\n",
       "      <td>1.198988e-02</td>\n",
       "      <td>-2.087347e-02</td>\n",
       "      <td>-9.994618e-01</td>\n",
       "      <td>-5.486233e-07</td>\n",
       "      <td>2.064998e-03</td>\n",
       "      <td>-8.601848e-04</td>\n",
       "      <td>3.696406e-06</td>\n",
       "      <td>-1.195211e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.304235e-06</td>\n",
       "      <td>1.156939e-07</td>\n",
       "      <td>-8.964724e-07</td>\n",
       "      <td>-9.997581e-01</td>\n",
       "      <td>-2.379221e-11</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.770611e-06</td>\n",
       "      <td>2.788234e-03</td>\n",
       "      <td>6.331734e-03</td>\n",
       "      <td>-1.888737e-02</td>\n",
       "      <td>-4.768378e-07</td>\n",
       "      <td>2.219411e-04</td>\n",
       "      <td>7.950985e-05</td>\n",
       "      <td>-1.976209e-08</td>\n",
       "      <td>8.888696e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Intercept  actor_1_facebook_likes  actor_2_facebook_likes  \\\n",
       "0         0.0            6.158703e-07            4.122498e-07   \n",
       "1        -0.0            2.014700e-02            8.681576e-03   \n",
       "2        -0.0            3.044229e-01            8.278524e-02   \n",
       "3         0.0            4.985078e-01            1.039038e-01   \n",
       "4         0.0            6.593528e-01           -5.909469e-01   \n",
       "5         0.0            3.300228e-02           -3.064034e-02   \n",
       "6         0.0           -1.961297e-01           -6.732913e-01   \n",
       "7         0.0           -4.283873e-01           -4.220107e-01   \n",
       "8         0.0            2.434902e-02            2.646093e-02   \n",
       "9         0.0           -2.680369e-04            1.928329e-03   \n",
       "10        0.0           -8.198878e-06           -2.937556e-04   \n",
       "11        0.0           -1.244571e-05           -7.034178e-05   \n",
       "12        0.0           -5.983098e-05           -9.004264e-05   \n",
       "13       -0.0            8.512175e-05            9.685728e-05   \n",
       "14       -0.0            2.304235e-06            1.156939e-07   \n",
       "15        1.0            0.000000e+00            0.000000e+00   \n",
       "\n",
       "    actor_3_facebook_likes  aspect_ratio        budget  \\\n",
       "0             1.861868e-07  3.739348e-12  1.000000e+00   \n",
       "1             4.134552e-03  2.973127e-07 -2.394080e-05   \n",
       "2             3.203335e-02  2.562708e-06 -1.042500e-06   \n",
       "3             4.008251e-02  2.575750e-08  8.255882e-08   \n",
       "4            -2.810301e-01  1.234039e-06  2.138027e-07   \n",
       "5            -1.040459e-02 -2.180755e-06  2.054628e-08   \n",
       "6             6.807274e-01 -1.138203e-06  1.503561e-09   \n",
       "7            -6.739200e-01  1.040549e-05 -3.993162e-09   \n",
       "8             2.610746e-02  1.462370e-04 -1.599340e-08   \n",
       "9            -4.884405e-03 -5.227151e-04  2.318769e-08   \n",
       "10            7.729956e-04  1.871200e-03 -2.225583e-09   \n",
       "11            6.516237e-05 -9.357805e-03 -2.690479e-11   \n",
       "12           -1.901127e-04  6.592589e-03  1.239297e-10   \n",
       "13            6.298768e-05  1.867714e-02 -1.922524e-11   \n",
       "14           -8.964724e-07 -9.997581e-01 -2.379221e-11   \n",
       "15            0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "    cast_total_facebook_likes  director_facebook_likes      duration  \\\n",
       "0                    0.000001             2.453466e-07  3.245581e-09   \n",
       "1                    0.035679             1.075947e-02  4.567515e-05   \n",
       "2                    0.438417            -2.163424e-03  1.223984e-04   \n",
       "3                    0.667093             7.756734e-03 -7.160618e-07   \n",
       "4                   -0.365832            -5.231903e-02 -3.496958e-05   \n",
       "5                   -0.025629             9.985163e-01  7.409864e-04   \n",
       "6                    0.211427            -1.715132e-03 -5.116579e-04   \n",
       "7                    0.426327             4.937761e-03  9.645712e-04   \n",
       "8                   -0.023615            -4.231637e-03  1.165591e-02   \n",
       "9                    0.000817            -1.239935e-03  1.620013e-02   \n",
       "10                  -0.000082            -6.909812e-04  9.965626e-01   \n",
       "11                   0.000083            -1.335895e-04 -7.925557e-02   \n",
       "12                   0.000055             4.360332e-05 -5.468034e-03   \n",
       "13                  -0.000085             7.860101e-06  1.198988e-02   \n",
       "14                  -0.000001            -1.770611e-06  2.788234e-03   \n",
       "15                   0.000000             0.000000e+00  0.000000e+00   \n",
       "\n",
       "    facenumber_in_poster    imdb_score  movie_facebook_likes  \\\n",
       "0          -1.361421e-10  1.222433e-10          3.005359e-06   \n",
       "1          -3.856553e-07  3.524143e-06          8.748485e-02   \n",
       "2           9.407970e-06  2.998838e-06          8.354509e-01   \n",
       "3           9.142119e-06 -3.345472e-06         -5.420852e-01   \n",
       "4          -4.277794e-05  1.342436e-05          2.146841e-02   \n",
       "5          -4.539018e-05  1.674921e-05          6.205378e-03   \n",
       "6           3.766206e-05 -7.932303e-05          1.061799e-03   \n",
       "7          -2.592849e-04 -1.650957e-04         -2.791878e-05   \n",
       "8          -8.107269e-04 -4.626777e-04         -9.108129e-04   \n",
       "9           7.727561e-04 -1.456072e-03          3.226398e-03   \n",
       "10          4.683693e-03  1.287961e-02         -1.477591e-04   \n",
       "11         -1.297654e-02  1.114316e-02          5.464739e-05   \n",
       "12          9.996662e-01 -2.066455e-02         -3.557609e-06   \n",
       "13         -2.087347e-02 -9.994618e-01         -5.486233e-07   \n",
       "14          6.331734e-03 -1.888737e-02         -4.768378e-07   \n",
       "15          0.000000e+00  0.000000e+00          0.000000e+00   \n",
       "\n",
       "    num_critic_for_reviews  num_user_for_reviews  num_voted_users  \\\n",
       "0             4.102250e-08          5.791991e-08     2.372417e-05   \n",
       "1             5.344004e-04          1.836914e-03     9.952164e-01   \n",
       "2             3.008434e-03          1.591069e-04    -9.615447e-02   \n",
       "3            -1.402834e-03         -4.657174e-04     1.248983e-02   \n",
       "4            -3.230345e-04         -1.891060e-03     4.772002e-03   \n",
       "5            -7.913783e-04          3.996753e-03    -1.078638e-02   \n",
       "6            -3.272745e-03          1.025700e-02    -6.559790e-04   \n",
       "7             8.472064e-03          4.869104e-02    -2.760921e-04   \n",
       "8             1.122760e-01          9.923219e-01    -1.752186e-03   \n",
       "9            -9.925635e-01          1.122447e-01     3.469544e-05   \n",
       "10            1.815117e-02         -1.371200e-02    -6.863832e-06   \n",
       "11            4.221002e-02         -3.146010e-03    -1.343884e-05   \n",
       "12            1.365169e-03          7.352922e-04    -6.706338e-07   \n",
       "13            2.064998e-03         -8.601848e-04     3.696406e-06   \n",
       "14            2.219411e-04          7.950985e-05    -1.976209e-08   \n",
       "15            0.000000e+00          0.000000e+00     0.000000e+00   \n",
       "\n",
       "      title_year  \n",
       "0   1.172334e-09  \n",
       "1   5.563669e-06  \n",
       "2   1.936205e-04  \n",
       "3  -4.435336e-05  \n",
       "4  -1.150953e-05  \n",
       "5  -2.388906e-04  \n",
       "6  -1.928598e-05  \n",
       "7   1.559211e-04  \n",
       "8   6.985896e-04  \n",
       "9  -4.373986e-02  \n",
       "10 -7.844070e-02  \n",
       "11 -9.957644e-01  \n",
       "12 -1.282983e-02  \n",
       "13 -1.195211e-02  \n",
       "14  8.888696e-03  \n",
       "15  0.000000e+00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pca.components_,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic__C': 1.0, 'logistic__solver': 'newton-cg', 'pca__n_components': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
